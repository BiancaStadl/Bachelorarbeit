{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/03_huggingface_German_Bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "\n",
        "\n",
        "Following is a general pipeline for any transformer model:\n",
        "Tokenizer definition →Tokenization of Documents →Model Definition →Model Training →Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYXyFyKAQjmg"
      },
      "source": [
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/training.ipynb\n",
        "\n",
        "https://huggingface.co/transformers/ (get started)\n",
        "\n",
        "Sehr wichtig: https://huggingface.co/transformers/notebooks.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7ifvIz0X2QH"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEx0JLGOX2my",
        "outputId": "8c94ea04-d588-4f30-feb4-93bab63989cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj0g2dfIX4K-",
        "outputId": "7258e5ec-e43f-4d0f-cb6a-20434bc499ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.91314722300001\n",
            "GPU (s):\n",
            "0.037648257999990165\n",
            "GPU speedup over CPU: 77x\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "outputs": [],
      "source": [
        "max_length = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "Next step is now to perform tokenization on documents. It can be performed either by encode() or encode_plus() method.\n",
        "https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "\n",
        "https://huggingface.co/bert-base-german-cased\n",
        "\n",
        "mit folgenden Paramter (laut doku)\n",
        "\n",
        "batch_size = 1024\n",
        "n_steps = 810_000\n",
        "max_seq_len = 128 (and 512 later)\n",
        "learning_rate = 1e-4\n",
        "lr_schedule = LinearWarmup\n",
        "num_warmup_steps = 10_000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I3XyiQsHNl0z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def tokenize(sentences, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [],[],[]\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=60, pad_to_max_length=True, truncation=True,\n",
        "                                             return_attention_mask=True, return_token_type_ids=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])        \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mud88oeQ9UL",
        "outputId": "ada59bbe-ddb2-46f3-f4aa-8f362ad101ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NYaByZROqzU",
        "outputId": "3d35f6b5-337d-4095-a833-29b2cc897e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "#2.3.3 Fine-tuning a Pretrained transformer model, noch immer von https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelForMaskedLM, AutoConfig, AutoModelForMaskedLM\n",
        "german_bert='bert-base-german-cased'\n",
        "\n",
        "\n",
        "  # Defining German bert tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(german_bert, max_length=60, pad_to_max_length=True,add_special_tokens=True)\n",
        "#add_special_tokens: ->  <cls>, <sep>,<unk>, etc w.r.t Pretrained model in use. It should be always kept True\n",
        "\n",
        "#https://huggingface.co/transformers/model_doc/auto.html#autoconfig\n",
        "# Download configuration from huggingface.co and cache.\n",
        "config = AutoConfig.from_pretrained('bert-base-german-cased', dropout=0.2, attention_dropout=0.2, num_labels=2)\n",
        "\n",
        "config.output_hidden_states = False\n",
        "\n",
        "German_model = TFAutoModelForMaskedLM.from_pretrained(german_bert, config=config)\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_attmasks_in = tf.keras.layers.Input(shape=(60,), name='attention_token', dtype='int32') \n",
        "token_type_ids_in = tf.keras.layers.Input(shape=(60,),name=\"token_type_ids\", dtype='int32')\n",
        "\n",
        "embedding_layer = German_model(input_ids=input_ids_in, attention_mask=input_attmasks_in, token_type_ids=token_type_ids_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True, dropout=0.3))(embedding_layer)\n",
        "#X = tf.keras.layers.LSTM(100, return_sequences=True, dropout=0.3)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(130, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model180107 = tf.keras.Model(inputs=[input_ids_in, input_attmasks_in,token_type_ids_in], outputs = X)\n",
        "\n",
        "\n",
        "# embedding_layer = German_model(input_ids=input_ids_in, attention_mask=input_attmasks_in)[0]\n",
        "# #X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "# X = tf.keras.layers.LSTM(100, return_sequences=True, dropout=0.3)(embedding_layer)\n",
        "# X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "# X = tf.keras.layers.Dense(190, activation='relu')(X)\n",
        "# X = tf.keras.layers.Dropout(0.2)(X)\n",
        "# X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "# model180107 = tf.keras.Model(inputs=[input_ids_in, input_attmasks_in], outputs = X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC3Ce-wNQINj",
        "outputId": "840a557e-502a-4d75-c976-a8e4de4ac310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " attention_token (InputLayer)   [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " token_type_ids (InputLayer)    [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_for_masked_lm (TFBertF  TFMaskedLMOutput(lo  109112880  ['input_token[0][0]',            \n",
            " orMaskedLM)                    ss=None, logits=(No               'attention_token[0][0]',        \n",
            "                                ne, 60, 30000),                   'token_type_ids[0][0]']         \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 200)      24080800    ['tf_bert_for_masked_lm[0][0]']  \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 200)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 130)          26130       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 130)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            131         ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 133,219,941\n",
            "Trainable params: 133,219,941\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model180107.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "outputs": [],
      "source": [
        "#os.listdir(dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "outputs": [],
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "outputs": [],
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "outputs": [],
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "outputs": [],
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3XKTZ3dgItYe"
      },
      "outputs": [],
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn7B2064ujon"
      },
      "source": [
        "[Linktext](https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow)\n",
        "What is train steps?\n",
        "Step: A training step means using one batch size of training data to train the model. Number of training steps per epoch: total_number_of_training_examples / batch_size . Total number of training steps: number_of_epochs x Number of training steps per epoch ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P-JYfZWQVh_",
        "outputId": "89d1f5bc-7daf-44a3-9b56-0855824bbb4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        }
      ],
      "source": [
        "training_epochs = 3\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "#init_lr=2e-5\n",
        "#laut German bert docu:\n",
        "init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "B-tWBjvUI_9x"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "D10-ZRr_QV6W"
      },
      "outputs": [],
      "source": [
        "model180107.compile(loss=loss, optimizer=optimizer ,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZWF9QIpOJga",
        "outputId": "21993138-8702-4087-98ef-4404f0f6eac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention, X_segments = tokenize(training_sentences,tokenizer)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention, Y_segments = tokenize(testing_sentences,tokenizer)\n",
        "#encding von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQhkO9_5SGs4"
      },
      "source": [
        "steps-per-epoch-Erklärung...\n",
        "https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
        "Traditionally, the steps per epoch is calculated as train_length // batch_size, since this will use all of the data points, one batch size worth at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RzowLK8xgHzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIOKqYkGerVJ",
        "outputId": "b1ff3f8c-9c84-4940-adbf-cfd045804b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "157/157 [==============================] - 116s 615ms/step - loss: 0.5559 - binary_accuracy: 0.7175 - metrics_recall: 0.4063 - metrics_precision: 0.6201 - metrics_f1: 0.4462\n",
            "Epoch 2/3\n",
            "157/157 [==============================] - 96s 611ms/step - loss: 0.4655 - binary_accuracy: 0.8063 - metrics_recall: 0.6562 - metrics_precision: 0.7663 - metrics_f1: 0.6723\n",
            "Epoch 3/3\n",
            "157/157 [==============================] - 96s 612ms/step - loss: 0.3646 - binary_accuracy: 0.8571 - metrics_recall: 0.7384 - metrics_precision: 0.8283 - metrics_f1: 0.7648\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f866b086ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model180107.fit(\n",
        "    x = [X_train_ids, X_train_attention, X_segments],\n",
        "    y=np.array(training_labels),\n",
        "    epochs = 3,\n",
        "    batch_size = 32\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6KWbgX5NLk9r"
      },
      "outputs": [],
      "source": [
        "BERTGermanPredict = model180107.predict([Y_test_ids, Y_test_attention, Y_segments])\n",
        "BERT_pred_thresh = np.where(BERTGermanPredict >= 0.5, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rk4KctG_MJ20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbc8c25-f2a9-43cc-ccd9-3bb15aea6795"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [0],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "BERT_pred_thresh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kOwlJFGrMKeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35878f88-55a2-480f-8d5f-25044b11a7ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.06697626],\n",
              "       [0.931447  ],\n",
              "       [0.05849774],\n",
              "       ...,\n",
              "       [0.8784087 ],\n",
              "       [0.06430776],\n",
              "       [0.18357813]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "BERTGermanPredict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "outputs": [],
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "a6035b9d-4322-4ee5-9c01-af21aedae510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2068  262]\n",
            " [ 501  701]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='German BERT, unfreezed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gE5mZvfmUyJF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-3NPPARgU0Kt"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EUt-18pmU0pA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaa97669-c8ee-4994-8bed-72ba7fd8492a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7839750849377124"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "accuracy"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "03 huggingface German Bert ohne Freeze.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}