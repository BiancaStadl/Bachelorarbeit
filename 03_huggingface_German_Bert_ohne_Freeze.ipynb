{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/03_huggingface_German_Bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "\n",
        "\n",
        "Following is a general pipeline for any transformer model:\n",
        "Tokenizer definition →Tokenization of Documents →Model Definition →Model Training →Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYXyFyKAQjmg"
      },
      "source": [
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/training.ipynb\n",
        "\n",
        "https://huggingface.co/transformers/ (get started)\n",
        "\n",
        "Sehr wichtig: https://huggingface.co/transformers/notebooks.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7ifvIz0X2QH"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEx0JLGOX2my",
        "outputId": "04a78de2-f978-4a21-d630-f5abd2965e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj0g2dfIX4K-",
        "outputId": "bd26b813-2f38-4ec8-bb46-e5b15d8379bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.8898560899999666\n",
            "GPU (s):\n",
            "0.04633991199989396\n",
            "GPU speedup over CPU: 62x\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "outputs": [],
      "source": [
        "max_length = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "Next step is now to perform tokenization on documents. It can be performed either by encode() or encode_plus() method.\n",
        "https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "\n",
        "https://huggingface.co/bert-base-german-cased\n",
        "\n",
        "mit folgenden Paramter (laut doku)\n",
        "\n",
        "batch_size = 1024\n",
        "n_steps = 810_000\n",
        "max_seq_len = 128 (and 512 later)\n",
        "learning_rate = 1e-4\n",
        "lr_schedule = LinearWarmup\n",
        "num_warmup_steps = 10_000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I3XyiQsHNl0z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def tokenize(sentences, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [],[],[]\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=60, pad_to_max_length=True, truncation=True,\n",
        "                                             return_attention_mask=True, return_token_type_ids=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])        \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mud88oeQ9UL",
        "outputId": "eac9927d-a6b0-4786-a229-c4d7a1de8b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NYaByZROqzU",
        "outputId": "caf889de-3015-4d05-804f-dfa97835480f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "#2.3.3 Fine-tuning a Pretrained transformer model, noch immer von https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelForMaskedLM, AutoConfig, AutoModelForMaskedLM\n",
        "german_bert='bert-base-german-cased'\n",
        "\n",
        "\n",
        "  # Defining German bert tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(german_bert, max_length=60, pad_to_max_length=True,add_special_tokens=True)\n",
        "#add_special_tokens: ->  <cls>, <sep>,<unk>, etc w.r.t Pretrained model in use. It should be always kept True\n",
        "\n",
        "#https://huggingface.co/transformers/model_doc/auto.html#autoconfig\n",
        "# Download configuration from huggingface.co and cache.\n",
        "config = AutoConfig.from_pretrained('bert-base-german-cased', dropout=0.2, attention_dropout=0.2, num_labels=2)\n",
        "\n",
        "config.output_hidden_states = False\n",
        "\n",
        "German_model = TFAutoModelForMaskedLM.from_pretrained(german_bert, config=config)\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_attmasks_in = tf.keras.layers.Input(shape=(60,), name='attention_token', dtype='int32') \n",
        "token_type_ids_in = tf.keras.layers.Input(shape=(60,),name=\"token_type_ids\", dtype='int32')\n",
        "\n",
        "embedding_layer = German_model(input_ids=input_ids_in, attention_mask=input_attmasks_in, token_type_ids=token_type_ids_in)[0]\n",
        "#X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X = tf.keras.layers.LSTM(100, return_sequences=True, dropout=0.3)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(190, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model180102 = tf.keras.Model(inputs=[input_ids_in, input_attmasks_in,token_type_ids_in], outputs = X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC3Ce-wNQINj",
        "outputId": "cc46d528-10ad-47f3-e0f2-64cdedab92cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " attention_token (InputLayer)   [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " token_type_ids (InputLayer)    [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_for_masked_lm (TFBertF  TFMaskedLMOutput(lo  109112880  ['input_token[0][0]',            \n",
            " orMaskedLM)                    ss=None, logits=(No               'attention_token[0][0]',        \n",
            "                                ne, 60, 30000),                   'token_type_ids[0][0]']         \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 60, 100)      12040400    ['tf_bert_for_masked_lm[0][0]']  \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 100)         0           ['lstm[0][0]']                   \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 190)          19190       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 190)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            191         ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 121,172,661\n",
            "Trainable params: 121,172,661\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model180102.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "outputs": [],
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "outputs": [],
      "source": [
        "#os.listdir(dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "outputs": [],
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "outputs": [],
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "outputs": [],
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "outputs": [],
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3XKTZ3dgItYe"
      },
      "outputs": [],
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn7B2064ujon"
      },
      "source": [
        "[Linktext](https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow)\n",
        "What is train steps?\n",
        "Step: A training step means using one batch size of training data to train the model. Number of training steps per epoch: total_number_of_training_examples / batch_size . Total number of training steps: number_of_epochs x Number of training steps per epoch ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P-JYfZWQVh_",
        "outputId": "902c983b-3da3-4e70-d2d2-4b56908ae1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        }
      ],
      "source": [
        "training_epochs = 4\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "#init_lr=2e-5\n",
        "#laut German bert docu:\n",
        "init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "B-tWBjvUI_9x"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "D10-ZRr_QV6W"
      },
      "outputs": [],
      "source": [
        "model180102.compile(loss=loss, optimizer=optimizer ,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZWF9QIpOJga",
        "outputId": "c15dc078-d7c9-41b3-dbb0-9ecd126ab87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention, X_segments = tokenize(training_sentences,tokenizer)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention, Y_segments = tokenize(testing_sentences,tokenizer)\n",
        "#encding von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQhkO9_5SGs4"
      },
      "source": [
        "steps-per-epoch-Erklärung...\n",
        "https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
        "Traditionally, the steps per epoch is calculated as train_length // batch_size, since this will use all of the data points, one batch size worth at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RzowLK8xgHzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIOKqYkGerVJ",
        "outputId": "e66480b1-da4f-45e7-9f19-fbe185f258d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "157/157 [==============================] - 101s 524ms/step - loss: 0.6076 - binary_accuracy: 0.6924 - metrics_recall: 0.3619 - metrics_precision: 0.4888 - metrics_f1: 0.3681\n",
            "Epoch 2/4\n",
            "157/157 [==============================] - 85s 540ms/step - loss: 0.5427 - binary_accuracy: 0.7383 - metrics_recall: 0.5974 - metrics_precision: 0.6511 - metrics_f1: 0.5847\n",
            "Epoch 3/4\n",
            "157/157 [==============================] - 86s 548ms/step - loss: 0.5153 - binary_accuracy: 0.7754 - metrics_recall: 0.5295 - metrics_precision: 0.7435 - metrics_f1: 0.5616\n",
            "Epoch 4/4\n",
            "157/157 [==============================] - 86s 547ms/step - loss: 0.5052 - binary_accuracy: 0.7936 - metrics_recall: 0.6722 - metrics_precision: 0.7108 - metrics_f1: 0.6573\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f96db9f2e10>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model180102.fit(\n",
        "    x = [X_train_ids, X_train_attention, X_segments],\n",
        "    y=np.array(training_labels),\n",
        "    epochs = 4,\n",
        "    batch_size = 32\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6KWbgX5NLk9r"
      },
      "outputs": [],
      "source": [
        "BERTGermanPredict = model180102.predict([Y_test_ids, Y_test_attention, Y_segments])\n",
        "BERT_pred_thresh = np.where(BERTGermanPredict >= 0.5, 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk4KctG_MJ20",
        "outputId": "e322b15a-7f55-4c46-e848-0642d42e1829"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [0],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "BERT_pred_thresh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOwlJFGrMKeM",
        "outputId": "2867b97d-b202-43b2-b57d-470c6a2dd759"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22648868],\n",
              "       [0.85351115],\n",
              "       [0.22648868],\n",
              "       ...,\n",
              "       [0.84004855],\n",
              "       [0.22648878],\n",
              "       [0.22648868]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "BERTGermanPredict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "outputs": [],
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "X-K7cFJfWsGV",
        "outputId": "26e9bd20-edcf-476d-f7a1-6728d0a3d1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[1870  460]\n",
            " [ 434  768]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='German BERT, unfreezed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gE5mZvfmUyJF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-3NPPARgU0Kt"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUt-18pmU0pA",
        "outputId": "fd34111d-850f-4713-ab13-07f26a742e2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.746885617214043"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "accuracy"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "03 huggingface German Bert ohne Freeze.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}