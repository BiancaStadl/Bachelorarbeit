{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03 huggingface German Bert API drei Inputs ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyS3onGZ3d9yFK2iYVGx7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/03_huggingface_German_Bert_API_drei_Inputs_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "\n",
        "\n",
        "Following is a general pipeline for any transformer model:\n",
        "Tokenizer definition →Tokenization of Documents →Model Definition →Model Training →Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYXyFyKAQjmg"
      },
      "source": [
        "https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/training.ipynb\n",
        "\n",
        "https://huggingface.co/transformers/ (get started)\n",
        "\n",
        "Sehr wichtig: https://huggingface.co/transformers/notebooks.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ],
      "metadata": {
        "id": "L7ifvIz0X2QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEx0JLGOX2my",
        "outputId": "d17f1662-87ae-4e65-92bb-c9eeb58fba64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "metadata": {
        "id": "vj0g2dfIX4K-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4fa206-abc0-4746-b1a2-d0b769612e9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.823963238000033\n",
            "GPU (s):\n",
            "0.04651446499974554\n",
            "GPU speedup over CPU: 82x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "https://huggingface.co/bert-base-german-cased\n",
        "\n",
        "mit folgenden Paramter (laut doku)\n",
        "\n",
        "batch_size = 1024\n",
        "n_steps = 810_000\n",
        "max_seq_len = 128 (and 512 later)\n",
        "learning_rate = 1e-4\n",
        "lr_schedule = LinearWarmup\n",
        "num_warmup_steps = 10_000\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3XyiQsHNl0z"
      },
      "source": [
        "#Next step is now to perform tokenization on documents. It can be performed either by encode() or encode_plus() method.\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "#alles original von ihm.\n",
        "def tokenize(sentences, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [],[],[]\n",
        "    #for sentence in tqdm(sentences):\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=60, pad_to_max_length=True, truncation=True,\n",
        "                                             return_attention_mask=True, return_token_type_ids=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])        \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mud88oeQ9UL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e3e4d2-fc42-4c2b-ee62-f592ef181167"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NYaByZROqzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba9abdf-7faf-489c-aa98-b1b0490b359d"
      },
      "source": [
        "#2.3.3 Fine-tuning a Pretrained transformer model, noch immer von https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModelForMaskedLM, AutoConfig, AutoModelForMaskedLM\n",
        "#Ich hab mi AutoModelForMaskedLM einen Fehler bekommen; Lösung war hier: https://stackoverflow.com/questions/67274470/attributeerror-kerastensor-object-has-no-attribute-size-with-hugging-face-be\n",
        "german_bert='bert-base-german-cased'\n",
        "#german_bert='distilbert-base-german-cased'\n",
        "\n",
        "  # Defining German bert tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(german_bert, max_length=60, pad_to_max_length=True,add_special_tokens=True)\n",
        "\n",
        "#do_lower_case=True, add_special_tokens=True?\n",
        "#add_special_tokens: Is used to add special character like <cls>, <sep>,<unk>, etc w.r.t Pretrained model in use. It should be always kept True\n",
        "\n",
        "#https://huggingface.co/transformers/model_doc/auto.html#autoconfig\n",
        "# Download configuration from huggingface.co and cache.\n",
        "# Change some config attributes when loading a pretrained config.\n",
        "config = AutoConfig.from_pretrained('bert-base-german-cased', dropout=0.2, attention_dropout=0.2, num_labels=2)\n",
        "\n",
        "#config = GermanBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "config.output_hidden_states = False\n",
        "\n",
        "German_model = TFAutoModelForMaskedLM.from_pretrained(german_bert, config=config)\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_attmasks_in = tf.keras.layers.Input(shape=(60,), name='attention_token', dtype='int32') \n",
        "token_type_ids_in = tf.keras.layers.Input(shape=(60,),name=\"token_type_ids\", dtype='int32')\n",
        "\n",
        "embedding_layer = German_model(input_ids=input_ids_in, attention_mask=input_attmasks_in, token_type_ids=token_type_ids_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(80, return_sequences=True, dropout=0.3))(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(90, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model01 = tf.keras.Model(inputs=[input_ids_in, input_attmasks_in,token_type_ids_in], outputs = X)\n",
        "\n",
        "##original -> \n",
        "# embedding_layer = German_model(input_ids_in, attention_mask=input_attmasks_in)[0]\n",
        "# X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "# X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "# X = tf.keras.layers.Dense(120, activation='relu')(X)\n",
        "# X = tf.keras.layers.Dropout(0.2)(X)\n",
        "# X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "# model = tf.keras.Model(inputs=[input_ids_in, input_attmasks_in], outputs = X)\n",
        "\n",
        "# for layer in model.layers[:4]:\n",
        "#   layer.trainable = True\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC3Ce-wNQINj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c465c6b2-96d4-4f86-9810-a95776d12779"
      },
      "source": [
        "model01.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " attention_token (InputLayer)   [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " token_type_ids (InputLayer)    [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_for_masked_lm (TFBertF  TFMaskedLMOutput(lo  109112880  ['input_token[0][0]',            \n",
            " orMaskedLM)                    ss=None, logits=(No               'attention_token[0][0]',        \n",
            "                                ne, 60, 30000),                   'token_type_ids[0][0]']         \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 160)      19251840    ['tf_bert_for_masked_lm[0][0]']  \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 160)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 90)           14490       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 90)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            91          ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 128,379,301\n",
            "Trainable params: 128,379,301\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XKTZ3dgItYe"
      },
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Linktext](https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow)\n",
        "What is train steps?\n",
        "Step: A training step means using one batch size of training data to train the model. Number of training steps per epoch: total_number_of_training_examples / batch_size . Total number of training steps: number_of_epochs x Number of training steps per epoch ."
      ],
      "metadata": {
        "id": "Vn7B2064ujon"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P-JYfZWQVh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec9429f-18f5-4449-b150-5c534ad23065"
      },
      "source": [
        "training_epochs = 3\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5,\n",
        "#init_lr=2e-5\n",
        "#laut German bert docu:\n",
        "init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-tWBjvUI_9x"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D10-ZRr_QV6W"
      },
      "source": [
        "model01.compile(loss=loss, optimizer=optimizer ,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZWF9QIpOJga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a43159-7d27-46c3-8086-85e5a8217976"
      },
      "source": [
        "# Encode training set X\n",
        "#input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments,\n",
        "X_train_ids, X_train_attention, X_segments = tokenize(training_sentences,tokenizer)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention, Y_segments = tokenize(testing_sentences,tokenizer)\n",
        "#encding von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "steps-per-epoch-Erklärung...\n",
        "https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
        "Traditionally, the steps per epoch is calculated as train_length // batch_size, since this will use all of the data points, one batch size worth at a time."
      ],
      "metadata": {
        "id": "qQhkO9_5SGs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model01.fit(\n",
        "    x = [X_train_ids, X_train_attention, X_segments],\n",
        "    y=np.array(training_labels),\n",
        "    epochs = 3,\n",
        "    batch_size = 32\n",
        "    #validation_data = ([Y_test_ids, Y_test_attention],testing_labels.to_numpy())\n",
        ")"
      ],
      "metadata": {
        "id": "pIOKqYkGerVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67a0d46-06a1-4273-c8e8-43d42918bb22"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "157/157 [==============================] - 221s 1s/step - loss: 0.5457 - binary_accuracy: 0.7301 - metrics_recall: 0.4249 - metrics_precision: 0.5630 - metrics_f1: 0.4488\n",
            "Epoch 2/3\n",
            "157/157 [==============================] - 194s 1s/step - loss: 0.4109 - binary_accuracy: 0.8305 - metrics_recall: 0.7104 - metrics_precision: 0.7841 - metrics_f1: 0.7268\n",
            "Epoch 3/3\n",
            "157/157 [==============================] - 194s 1s/step - loss: 0.3044 - binary_accuracy: 0.8840 - metrics_recall: 0.8290 - metrics_precision: 0.8288 - metrics_f1: 0.8188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc21fb12b90>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "prediction und so kann dann eigentlich auch von hier https://colab.research.google.com/drive/18GKh0XbdDS8YBlIur1E2yfIFZGcaL1dC#scrollTo=YjfBDQO4y7vV\n",
        " verwendet werden, sollte funktionieren"
      ],
      "metadata": {
        "id": "Ytv10802g8HD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BERTGermanPredict = model01.predict([Y_test_ids, Y_test_attention, Y_segments])\n",
        "BERT_pred_thresh = np.where(BERTGermanPredict >= 0.5, 1, 0)"
      ],
      "metadata": {
        "id": "6KWbgX5NLk9r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_pred_thresh"
      ],
      "metadata": {
        "id": "rk4KctG_MJ20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb18472-2f7c-43a3-e7af-881c3b7d4b3f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [0],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BERTGermanPredict"
      ],
      "metadata": {
        "id": "kOwlJFGrMKeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fe6491-27f4-40ea-849b-e7b04c9321ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0383181 ],\n",
              "       [0.66679996],\n",
              "       [0.09641343],\n",
              "       ...,\n",
              "       [0.9476108 ],\n",
              "       [0.03309439],\n",
              "       [0.24440286]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "4259ddb4-2858-4353-fea6-dab4971675a5"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='German BERT, unfreezed')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2041  289]\n",
            " [ 469  733]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEmCAYAAACzoiEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5gUVdbH8e9vQAEliRgwYkARE2JAxDUvJhRdc1gTiq4JX9esK2ZdxEXMijnngAlEFDNBFFGMKCAIBiSIggJ63j/ubSyHmZ6eWFMz57NPP9N9q7r6duOevn3q1j0yM5xzzqWjKO0OOOdcfeZB2DnnUuRB2DnnUuRB2DnnUuRB2DnnUuRB2DnnUuRB2LlaQNJlkmZI+jbtvpRG0nBJx6bdj7rGg3A9IOlgSSMl/SLp+3j/RElKu2/lFQPBr5J+ljRH0uuSNk5sv0jSwrg9d5ud2G7xc/hZ0jeS/iepgaQXE/svlLQg8fiWan5PawD/BjqY2crV+Vqu9vEgXMdJ+jcwALgaWBlYCTgB6AosXYHjNazSDlbMyWbWFGgFDAfuK7b9ETNrmri1LLZ90/j87YGDgGPMbPfc/sADQN/E80+o5vezBvCjmX1f0sZa8pm7auJBuA6T1AK4BDjRzB43s7kWvG9mh5nZb3G/RpL6Sfpa0neSbpHUJG7bQdJUSWfHn8p3xdHmY5LulzRX0oeS1pN0bhxpT5HULdGPoyV9Evf9StLxiW254/87Pne6pKMLeX9m9jvwMNChIp+PmU0A3gI6VuT5ScV/qks6StKbiccm6QRJX0iaLelGBbsAQ4FV4qj7bklt4/49JX0NvBKPcUz8HGdJGiJpzcTx20saKmmmpM8kHRjbc8fN3eZJssTz8h3z75I+jb84bgAy98spCzwI121dgEbAM2XsdxWwHiEYrQusClyY2L4yYdS5JtArtu1FGIEuB7wPDCH897QqIfDfmnj+90B3oDlwNNBfUqdix28Rn9sTuFHScmW9OUlLA4cBI8rat5Tntwf+BkyoyPMroDuwJbAJcCCwq5m9DOwOTIuj7qMS+28PbADsKqkHcB7wD2AF4A3gofg+liUE8geBFYGDgZskdTCzaclfBcBThC8uyjhma+BJ4AKgNfAl4deTq2pm5rc6egMOB74t1vY2MBuYD2xHGN38AqyT2KcLMDHe3wFYADRObL8IGJp4vBfwM9AgPm4GGNCylH49DfROHH8+0DCx/Xtg61KeOxyYF9/Db8AcYOdifVsQt+durya2G/BTfM9GCDqNir3G3cBl5fyshwPHJh4fBbxZ7HW3TTx+FDgn8RlMTWxrG/dfO9H2ItAz8bgofg5rElIqbxTrz61An2JtZwNjgCYFHPMIYERim4Cpyffot6q5+Ui4bvsRaJ3MKZrZNhZypD8S/k+3ArAMMCb+TJ4NDI7tOT+Y2a/Fjv1d4v58YIaF9EDuMUBTAEm7SxoRfyrPBvYgjK4W99PMFiUez8s9txSnxvfQhDC6fFzSJontj5pZy8Rtx2LP7xSPfxDQGVg2z2tVpeTMh7LeI8CUxP01gQGJf6OZhMC4atzWObctbj+M8AsDCP8GQG9gHzObX8AxV0m+voVInOyPqyIehOu2dwijxR559plBCJobJoJWCws/XXMqvNSepEbAE0A/YKUYPF+gCvKLZvaHmb1BSCd0K2v/Ys81M3uU8BldWNb+BfiF8GWWUxWzHJKf+xTg+GJfLk3M7O247bVi25qa2b8AJK0P3AMcaGZTCjzmdGD13I6SlHzsqo4H4TrMzGYDFxPyg/tLaiapSFJH4ujPzP4ABhLytCsCSFpV0q5V1I2lCXnpH4BFcURWroCZj6QuhBNz4yt4iKuA4ySVGTTjybIdStk8FviHpGUkrUvIbVelW4BzJW0Y+9JC0gFx23PAepL+KWmpeNtS0gaSmhPOCZxvZm+W45jPAxtK+kf8JXUqVfPF4orxIFzHmVlf4HTgLEIK4TtCvvBsQn6YeH8CMELST8DLwPpV9PpzCf8HfhSYBRwKDKrkYW/Ine0nnBy8wMxeTGw/qNiMgJ9zXzAl9O9D4HXgzHwvKGl1YC7wYSm79Cfkor8jjDofKN9bys/MngL+Czwc/40+IpzQy33G3Qgn5KYR0h7/JXz5dSL8W/ZPfh4FHHMGcADhS+pHoB1hJomrYgqpHudcPpIOJ6Rszk27L65u8SDsnHMp8nSEc86lyIOwc86lyIOwc86lyBcGqaXUsIlp6WZpd6Pe2GyDNdLuQr0yefIkZsyYUSVrUTRovqbZovl597H5Pwwxs92q4vWqmgfhWkpLN6PR+gem3Y16462RN6TdhXqla+ctquxYtmh+mf9f+XXsja3z7pAiT0c457JNgqIG+W9lHkKrS3pV0seSxkvqHdtbxdXpvoh/l4vtknSdpAmSxiUXpJJ0ZNz/C0lHlvXaHoSdc9mnovy3si0C/m1mHYCtgZMkdQDOAYaZWTtgWHwM4aKWdvHWC7gZQtAG+hDWJNkK6FPWioAehJ1zGVf5kbCZTTez9+L9ucAnhIWMehCugCT+3Sfe7wHcG9cgGQG0lNQG2JWwwuBMM5tFWGI0by7ac8LOuewru1JXa0nvJh7fZma3lXwotQU2A0YSFp2aHjd9S6hMAyFAJxdDmhrbSmsvlQdh51y2iUJSDjPMrMyzgZKaElb9O83MflIiuJuZJauSVBVPRzjnMq7y6QgASUsRAvADZvZkbP4uphmIf3N1AL/hr0t7rhbbSmsvlQdh51z2SflvZT5dAu4APjGz/yU2DQJyMxyO5M9SYYOAI+Isia2BOTFtMQToJmm5eEKuW2wrlacjnHMZp0JnQOTTFfgn8KGksbHtPMJSno9K6glMJtQGhFCYYA/CErDzCLUTMbOZki4FRsf9LjGzmfle2IOwcy7bRMEph9LEBe9LGzLvXML+BpxUyrHuBO4s9LU9CDvnMq5KRsKp8SDsnMs2AQ0qNxJOkwdh51z2FXDyrbbyIOycyzhPRzjnXLoqeWIuTR6EnXPZVuBc4NrKg7BzLvt8JOycc2nxnLBzzqXL0xHOOZcSCYqyG8qy23PnnMvxkbBzzqUowyfmspvNds45iFPUKldjTtKdkr6X9FGiraOkEZLGSnpX0laxvcqKfIIHYedcXVDJ9YSBu1myFlxf4GIz6whcGB9DFRb5BA/CzrmME1BUVJT3VhYzex0ovu6vAc3j/RbAtHi/yop8gueEnXNZJ0pfCfhPBRf6TDgNGCKpH2HAuk1sr7Iin+BB2DmXeUJlpxwKKvRZzL+A/zOzJyQdSCh/tEtFepiPpyOcc5lX2XREKY4EcgU/HyPkeaEKi3yCB2HnXB0gKe+tgqYB28f7OwFfxPtVVuQTPB3hnMs4SaiochdrSHoI2IGQO55KmOVwHDBAUkPgV8JMCKjCIp/gQdg5VwdUYrQLgJkdUsqmzUvYt8qKfIIHYedcHVDZIJwmD8LOuWwTlU5HpMmDsHMu83wk7JxzKRGqzDS01HkQds5lX3YHwh6EnXMZJ09HOOdcqrKcjshuz12NWG2llgy+7VTee+J8xjx+PicdsgMAyzVfhuduPpkPn7mQ524+mZbNmvzleZt3WIO5owew7y4dF7c9c8OJTH+9L08MOKEm30JmTZkyhV132ZHNNulAp0035IbrBgDwwdixbNd1azpv3pGunbdg9KhRAMyaNYsD99+XLTfbhG27bMX4jz7Kd/g6Q+S/Wq62j5I9CLu8Fv3+B+f870k67Xc52x/Rj+MP2o72a6/MGUf/neGjPmPjHpcwfNRnnHF0t8XPKSoSl/XuwcsjPv3Lsfrf+zI9L7i3pt9CZjVs2JCr+l7D++M+5rU3R3DrLTfyyccfc/65Z3H+f/owcsxY/nPRJZx/7lkA9L3qCjbdtCOj3x/HHXfdyxmn9075HdSQOEUt36028yDs8vp2xk+M/XQqAD/P+41PJ37LKiu0pPsOm3D/syMBuP/Zkey14yaLn3Piwdvz9LAP+GHm3L8ca/ioz5n7y2811/mMa9OmDZt1CkUbmjVrRvv2GzBt2jdI4qeffgJgzpw5tFllFQA+/eRjtt9xJwDWb9+eyZMn8d1336XT+RrmI2FXL6zRphUd11+N0R9NYsXlm/HtjBAIvp3xEysu3wyAVVZowd47bcptj72RZlfrnMmTJjF27PtsuVVnrr7mWs4750zWXWt1zj37DC657EoANt5kU555Kiz6NXrUKL6ePJlvpk5Ns9s1xoNwFZN0t6T9y7F/S0knVmefKkPSJEmt0+5HZSzbZGke6ncsZ/Z7grm//LrEdrPw9+oz9+OCAc9guQZXaT///DOHHLgfV19zLc2bN+e2W2+mb7/+TJg4hb79+vOvXj0BOOOsc5gzezadN+/IzTdez6YdN6NBg+wWwCyPLKcj6srsiJbAicBNaXekLmrYsIiH+h3HIy++yzOvfADA9z/OZeXWzfl2xk+s3Lr54tRDpw5rcO9VRwOwfMum7Lrthixa9AfPDh+XWv+zbOHChRxy4H4cdMhh7LPvPwB44L57uKZ/OEm33/4HcOLxxwKEAH3HXQCYGe3brcVaa6+dTsdrUFWMdiXdCXQHvjezjRLtpxAW6/kdeN7Mzort5wI9Y/upZjYktu8GDAAaALeb2VVlvXa1jIQltZX0iaSBksZLeklSk7gtV8F0nKSn8hTC207S25K+yo2KJTWVNEzSe5I+lNQj7nsVsE6sinp13PdMSaPj61wc25aV9LykDyR9JOmg2D5JUt94zFGS1o3tK0h6Ih5ntKSuiePcGfd9P9cPSQ0k9YvHHhf/AXNOSfS7fdV+4tXrlj6H8dnEb7nu/lcWtz3/2occvldnAA7fqzPPxSC7QfeLaL9nH9rv2YenXn6f0658xANwBZkZJxzXk/Xbb0Dv/zt9cXubVVbhjddfA2D4q6+w7rrtAJg9ezYLFiwA4K47bmfbbbejefPmSx64DqqCdMTdFKsHJ2lHQj25Tc1sQ6BfbO8AHAxsGJ9zU/z/fgPgRkIh0A7AIXHfvKpzJNwOOMTMjpP0KLAfcD9wL3CKmb0m6RLCup2nlfD8NsC2QHvCIsqPE9b03NfMfoo/70dIGgScA2wUq6IiqVt8/a0I19IMkrQdsAIwzcz2jPu1SLzeHDPbWNIRwLWEb8UBQH8ze1PSGoQFmjcAzgdeMbNjJLUERkl6GTgCaAt0NLNFCtVXc2aYWaeYNjkDOLb4G5bUi9yapUs1LeQzrnbbdFybw7p35sPPv2HEw+cA0OeGQfS7ayj3//cYjtynC19Pn8nhZ5W9et/Ld5zGemutRNMmjZgw+FJOuPhBXn7nk+p+C5n19ltv8eAD97HRRhvTefMw1e/iy67gxpsHcubpvVm0aBGNGjfmhptDqbRPP/mE43oeiSQ26LAht9x2R5rdr1GVTTmY2euS2hZr/hdwlZn9Fvf5Prb3AB6O7RMlTeDPqhsTzOwrAEkPx30/zvfa1RmEJ5rZ2Hh/DNA2Br2WZvZabL+HUDakJE+b2R/Ax5JWim0CrogB9Q9CEb2VSnhut3h7Pz5uSgjKbwDXSPov8JyZJc8ePZT42z/e3wXokPgmbS6paTz23pLOiO2NgTXi/reY2SIIizwnjp8rkzIG+EdJbzgWHrwNoGiZFWtFUvXtsV/RZLOTS9y2xwnX531urz73/+XxLj2vrbJ+1Qddt92W+QtL/s/g7VFjlmjbuksXPvz48+ruVq1UwGi3IoU+1wP+JulywgDwDDMbTYg7IxL7JQt6Fi/02bmsjlVnEE7ORfodaFLajgU8P/cJH0YYzW5uZgslTSIEwOIEXGlmty6xQepEWBX/MknDzOySuCn5X3vufhGwtZn9WuwYAvYzs8+KtRfyfn6n7uTinUudFOaml6EihT4bAq2ArYEtgUclVXmSvUZnR5jZHGCWpL/Fpn8Cr+V5SnEtCInzhTFfs2Zsnws0S+w3BDgmjlqRtKqkFSWtAswzs/uBq4FOiecclPj7Trz/ErA4ryspd/nXEEKOV7F9s9g+FDheoRwKxdIRzrlqUW1XzE0FnrRgFOHXd2uquNBnGiOyI4FbJC0DfEWsz1SgB4BnJX0IvAt8CmBmP0p6S9JHwItmdqakDYB34j/Az8DhwLrA1ZL+ABYScj45y0kaRxix5kqdnArcGNsbAq8DJwCXEvLG4yQVARMJOeTbCT9hxklaCAwEbijH+3POVUA1TQV+GtgReFXSesDSwAzCOaoHJf0PWIWQ6hxF+AXeTtJahOB7MHBomX33+ZxhdgSwhZnNSLsvOUXLrGiN1j8w7W7UG7NG+3dlTeraeQvGjHm3SkJn4zbrWdsj85+f+Oy/u43Jl45QotAn8B1hwsB9hHpxHYEFhJzwK3H/84FjgEXAaWb2YmzfgzBAawDcaWaXl9V/z0065zJNFJQTzitPoc/DS9n/cmCJAGtmLxCqMRfMgzBgZm3T7oNzruIqG4TT5EHYOZdtqraccI3wIOycyzThlTWccy5F8nSEc86lyUfCzjmXkgKvmKu1PAg75zIvwwNhD8LOuezzdIRzzqXF0xHOOZeeMEUt7V5UnAdh51zG1f5invl4EHbOZV6W0xG1stqyc84VLF62nO9W5iFCzcjv43K4xbf9W5LFkmoouE7ShFhLslNi3yMlfRFvRxbSfQ/CzrlMC6uoFeW9FeBuihX6BJC0OqGc2deJ5t0Jawi3I9SEvDnu24qwBGZnQs25Piq9kPFiHoSdc5lX2ZGwmb0OzCxhU3/gLP5a/qwHcG+suDECaCmpDbArMNTMZprZLEKlnSUCe3GeE3bOZV51FPqU1AP4xsw+KHb8VVmyoOeqedrz8iDsnMs0qaAFfMpV6DOWXzuPkIqoVp6OcM5lXmXTESVYB1gL+CCWP1sNeE/SytRUoU9J1/PXPMhfmNmpZR3cOedqQoMqnqJmZh8CK+YeJ+tQShoEnCzpYcJJuDlmNl3SEOCKxMm4bsC5Zb1WvnTEu3m2OedcrRBGu5ULwslCn5KmAn3M7I5Sdn8B2AOYAMwjVow3s5mSLgVGx/0uMbOSTvb9RalB2MzuKdbJZcxsXlkHdM65mlbZgXCeQp+57W0T9w04qZT97iRUaC5YmTlhSV0kfQx8Gh9vKumm8ryIc85Vp6Ii5b3VZoWcmLuWMP/tRwAz+wDYrjo75ZxzhRKgMv5XmxU0Rc3MphTLufxePd1xzrlykqr8xFxNKiQIT5G0DWCSlgJ6A59Ub7ecc65wGV5EraAgfAIwgHDlxzRgCKUkpZ1zrqYJKMpwFC4zCJvZDOCwGuiLc85VSG0/+ZZPIbMj1pb0rKQf4lJvz0hauyY655xzZSnrarnaPkguZHbEg8CjQBtgFeAx4KHq7JRzzpVHkZT3VpsVEoSXMbP7zGxRvN0PNK7ujjnnXKGyHITzrR3RKt59UdI5wMOEtSQOIly255xzqQsn5tLuRcXlOzE3hhB0c2/v+MQ2o4CFKZxzrtoVtpRlrZVv7Yi1arIjzjlXUVmutlzQesKSNpJ0oKQjcrfq7phzzhUil47IdyvzGCUU+pR0taRPYzHPpyS1TGw7Nxb6/EzSron23WLbhJjGLVMhU9T6ANfH245AX2DvQg7unHM1oQpOzN3NkvXghgIbmdkmwOfEFKykDsDBwIbxOTdJaiCpAXAjoRBoB+CQuG/+vhfQuf2BnYFvzexoYFOgRQHPc865aidVPgiXVOjTzF4ys0Xx4QhCpQwIhT4fNrPfzGwiYV3hreJtgpl9ZWYLCJMZepT12oUE4flm9gewSFJz4Hv+WsLDOedSVcBSlq0lvZu49SrnSxwDvBjv13ihz3djLmQgYcbEz8A7BTzPOedqRAGD3XIV+vzrsXU+sAh4oCLPL0sha0ecGO/eImkw0NzMxlVHZ5xzrrxE9V2QIekooDuwc6yoAfkLelZpoc9O+baZ2XtlHdxV3Ebrrc7zw65Juxv1xnsTZ6XdhXrllwVVuCS5qmcBH0m7AWcB2xcr7TYIeFDS/whLObQDRoWe0E7SWoTgezBwaFmvk28knC8CGLBTWQd3zrmaUNBc2zxKKvRJmA3RCBga5yGPMLMTzGy8pEeBjwlpipPM7Pd4nJMJy/02AO40s/FlvXa+izV2rNS7cs65GiAqX/K+lEKfpVVbxswuBy4vof0FyrmsQ0HljZxzrjbL8FXLHoSdc9kW1gzObhT2IOycy7wGlU0Kp6iQy5Yl6XBJF8bHa0jaqvq75pxzZcvVmMvqesKFfH/cBHQBconruYTro51zrlYoKuNWmxWSjuhsZp0kvQ9gZrMkLV3N/XLOuYJIqvTsiDQVEoQXxtWBDEDSCsAf1dor55wrh1qeccirkCB8HfAUsKKkywmrql1Qrb1yzrkCCWhYl0fCZvaApDGE5SwF7GNmn1R7z5xzrkB1eiQsaQ1gHvBsss3Mvq7OjjnnXEEKrJ5RWxWSjniePwt+NgbWAj4jrCrvnHOpEtAgw0PhQtIRGycfx9XVTixld+ecq3FZHgmXewpdXMKyczX0xTnnyi23gE++W5nHKLnQZytJQyV9Ef8uF9sl6bpYzHNcctlfSUfG/b+QdGQh/S8kJ3x64mER0AmYVsjBnXOu2qlKTszdDdwA3JtoOwcYZmZXxcrJ5wBnEwp5tou3zsDNQGdJrQhLYG5BSOGOkTTIzPIuVl3ISLhZ4taIkCMus3idc87VlOoo9EmIc/fE+/cA+yTa77VgBNBSUhtgV2Comc2MgXcoS1ZwXkLekXC8SKOZmZ1R5rtwzrkUhHREmbu1lvRu4vFtZnZbGc9Zycymx/vfAivF+zVT6FNSQzNbJKlrWQdxzrn0iCLKHO1WuNAngJmZJCt7z/LLNxIeRcj/jpU0CHgM+CXRqSero0POOVceUrUtZfmdpDZmNj2mG76P7aUV+vyGUCIp2T68rBcppOuNgR8JNeW6A3vFv845VytU01KWg4DcDIcjgWcS7UfEWRJbA3Ni2mII0E3ScnEmRbfYlle+kfCKcWbER/x5sUZOtQzLnXOuvETlZ0eUUujzKuBRST2BycCBcfcXgD2ACYSriY8GMLOZki4FRsf9LjGz4if7lpAvCDcAmkKJyRYPws65WqOaCn1CWDOn+L4GnFTKce4E7izPa+cLwtPN7JLyHMw552qaqP0Lt+eTLwhn+EJA51y9UYcLfS4xDHfOudqmzi7gU0hC2TnnaoPshmAvee+cyzxRlOFl1DwIO+cyrS6fmHPOuUyoqyfmnHOu9hOVuSoudR6EnXOZ5ukI55xLmY+EnXMuRRmOwR6EnXPZFtIR2Y3CWU6lOOcckH8Zy0JTFZL+T9J4SR9JekhSY0lrSRoZi3o+ImnpuG+j+HhC3N62or33IOycyzwp/63s52tV4FRgCzPbiLCK5MHAf4H+ZrYuMAvoGZ/SE5gV2/vH/SrEg7BzLtOksHZEvluBGgJNJDUElgGmE4pZPB63Fy/2mSsC+jiwsyo4Wdlzwq7cfv/9d7rvvA0rtVmFux96CjPj6sv78PygJ2lQ1IDDj+7FMcefxOzZszjzlOOZPOkrGjVqTL/rb2X9DTZMu/uZMfmrL7jwtJ6LH0+bMolje5/LnNkzeXPYi0hFLLd8a86/6kZWWKkNb7z8AgMHXIFURIOGDel93hVsusXWKb6DmlNA+Mtb6NPMvpHUD/gamA+8BIwBZpvZorhbsnDn4qKesRbnHGB5YEZ5++5B2JXbnbfewLrrrc/cuXMBeOzBe5n2zVReHTGOoqIiZvwQSnHd2L8vHTbehIH3PcqEzz/jgrN68/DTg9PseqasuXY77hn0OhC++Pb524Zs//fuNGvRgl6nnQ/AY/feyl03Xs1Zl/yPzbtsx7Y7744kJnw6nv/0PoaHhoxM8y3UGFWy0GcsR9QDWAuYTaipWWa5+qrg6QhXLtO/mcqwl17k4MOPXtx2310DOe3M8ykqCv85tV5hRQC++OwTtvnbDgCsu976TJ0ymR++/67G+1wXvPvOa6y6RltWXnV1lm3afHH7/HnzFl+yu8yyTRff/3X+L5metlUeuaUsK5mO2AWYaGY/mNlC4EmgK9Aypifgz4KekCj2Gbe3INTiLDcPwq5cLjr/TM676IrFARdg8qSvePapx9hzp2044sC9mfjlBAA22HBjBj8XaiOOHTOab6Z8zfRp35R4XJffsOefZJc991v8+Nb/Xca+223ES88+xrG9z13c/tpLz3HIrp05o9fBnHfl9Wl0NRWVPTFHSENsLWmZmNvdGfgYeBXYP+5TvNhnrgjo/sArsexRudXaICypraSPyrH/PpI6VGefKkrSUZJuSLsflfXykBdo3XoFNunY6S/tCxb8RqPGjXn+lbc55J/HcMapvQA4sfeZ/DRnNrttvxV3DbyJDTfuSIMGDdLoeqYtXLCAN4cNZqfdeyxuO/70C3jq9Y/ottcBPHHfwMXt23frzkNDRnLVTfcz8Nor0+huKlTG/8piZiMJJ9jeAz4kxMbbgLOB0yVNIOR874hPuQNYPrafDpxT0b7XpZzwPsBzhG8vVw3eHfk2Qwc/z6svD+a3335j7tyf6H38UbRpsyq7dQ8BYrfuPTjjlBCEmzVvzjU3hABhZnTdbH3WWHOt1PqfVSNef5n1NtyEVq1XXGJbt70P4IzjDvzLaBig45bbMG3KJGbP/JGWrZavqa6mQpRrBkSpzKwPocpy0lfAViXs+ytwQKVflFo8Eo4aSBoYJ1C/JKmJpOMkjZb0gaQn4s+HbYC9gasljZW0TrwNljRG0huS2gNIOiBOxv5A0uux7ShJz0gaLukLSYv/ISQdLmlUPO6tkhrE9m6S3pH0nqTHJDWN7VtKejsef5SkZvFQq8T+fCGpb41+ilXknAsvY9RHX/L22M+5YeC9bPO3HRhw691022Nv3nnzNQBGvPU6a63TDoA5c2azYMECAB6670626rItzZo3L/X4rmRDn3uCv3f/MxUxZdKXi++/8fILrLl2+LynTv6K3C/iz8Z/wIIFC2ixXKua7WwaykhF1PbceG0fCbcDDjGz4yQ9CuwHPGlmAwEkXQb0NLPrJQ0CnjOzx+O2YcAJZvaFpM7ATYQ5fxcCu8YpKS0Tr7UVsBEwD2uu9ggAABT5SURBVBgt6XngF+AgoKuZLZR0E3CYpBeAC4BdzOwXSbmfLFcBjwAHmdloSc0J010AOgKbAb8Bn0m63symVM/HVrNOPO0Meh9/FLfffD3LLtuUvgNuBmDC559y+knHIsR67TvQ97pbUu5p9syf9wuj3x7OWZf2X9x2c7+L+XriBIqKilh5ldU58+JrABg+5FlefPphGjZcikaNG3PJtXdkep3dQtXZGnO1xEQzGxvvjwHaAhvF4NsSaAoMKf6kOCrdBngs8R9ho/j3LeDuGNSfTDxtqJn9GJ//JLAtsAjYnBCUAZoA3wNbAx2At2L70sA7wPrAdDMbDWBmP8XjAQwzsznx8cfAmsR5hol+9wJ6Aay62uoFf0hp6LLt9nTZdnsAWrRoyd0PP73EPptvuTWvjSo4re9K0GSZZXlx1Jd/abvihntL3PfwXr05vFfvmuhWrZPdEFz7g/Bvifu/E4Lg3cA+ZvaBpKOAHUp4XhFhknXH4hvM7IQ4Mt4TGCNp89ym4rsS/m3vMbO/JNwk7UUI2ocUa9+4HO9lic8+Th6/DWCTjptX6Eyrc/VShqNwbc8Jl6QZMF3SUsBhifa5cVtuBDpR0gEACjaN99cxs5FmdiHwA3GuH/B3Sa0kNSGc5HsLGAbsL2nF+NxWktYERgBdJa0b25eVtB7wGdBG0paxvVlijqFzrppUxQI+acliEP4PMJIQJD9NtD8MnCnpfUnrEAJ0T0kfAOMJV8NAOHn3YZz+9jbwQWwfBTwBjAOeMLN3zexjQu73JUnjgKFAGzP7ATgKeCi2vwO0N7MFhBzy9fF1hwKNq+VTcM4tpjJutVmtHaWZ2STCibLc436JzTeXsP9bhDxt0hKXHZrZP4q3xZztVDPbp4T9HyGcbCve/gqwZQntowk546S74y23T/fiz3POVYzwQp/OOZeeDExDy8eDMGBmd5MYqTrnsiXDMdiDsHMu6+TpCOecS1OGY7AHYedctoUTc2n3ouKyOEXNOef+orKrqAFIainpcUmfSvpEUpd4bcDQuObL0Lj4e+7ag+sUCn2Ok9SprOOXxoOwcy7zqmgBnwHAYDNrD2wKfEJYonKYmbUjXLyVW7Jyd8LaNu0ISw0sMW22UB6EnXPZVgWrqElqAWxHXC/YzBaY2Wz+WtCzeKHPey0YQajA0aYi3fcg7JzLvALSEa0lvZu49Sp2iLUIyxjcFa+6vV3SssBKZjY97vMtsFK8v7jQZ5QsAloufmLOOZdpAorKHu3mLfRJiIWdgFPMbKSkARSrlmFmJqnKF9bykbBzLvsqv3jEVMLSBbny1I8TgvJ3uTRD/Pt93L640GeULAJaLh6EnXOZVwU15r4FpkhaPzblCn0mC3oWL/R5RJwlsTUwJ5G2KBdPRzjnMq+AdEQhTgEekLQ0obbc0YSB6qOSegKTgQPjvi8AewATCNV4jq7oi3oQds5lXxUE4VjFp6S88c4l7GvASZV/VQ/CzrmMC2nf7F4y50HYOZdtqrJ0RCo8CDvnss+DsHPOpaX215HLx4Owcy7TslBHLh8Pws657MtwFPYg7JzLPE9HOOdcirIbgj0IO+eyTl7y3jnnUpP18kYehJ1zmZfhGOxB2DmXfVk+MedLWTrnsq/y6wmHw0gNYmWN5+LjtSSNjAU9H4krrCGpUXw8IW5vW9GuexB2zmWa4toR+W7l0JtQ4DPnv0B/M1sXmAX0jO09gVmxvX/cr0I8CDvnMq+KSt6vBuwJ3B4fC9iJUGUDliz0mSsA+jiwsyo4RcODsHMu+8pOR5RV6BPgWuAs4I/4eHlgtpktio+TxTwXF/qM2+fE/cvNT8w55zKvsoU+JXUHvjezMZJ2qMKulcmDsHMu4wpPOeTRFdhb0h5AY6A5MABoKalhHO0mi3nmCn1OldQQaAH8WJEX9nSEcy7Tchdr5LuVxczONbPVzKwtcDDwipkdBrwK7B93K17oM1cAdP+4v1Wk/x6EnXOZV9kgnMfZwOmSJhByvnfE9juA5WP76cA5FX0BT0c45zKvKmvMmdlwYHi8/xWwVQn7/AocUBWv50HYOZdpuXnCWeVB2DmXfR6EnXMuPV7y3jnnUuTpCOecS5MHYeecS4fI9lKWquD8YlfNJP0ATE67HxXQGpiRdifqkax+3mua2QpVcSBJgwmfQz4zzGy3qni9quZB2FUpSe/mu0bfVS3/vLPPr5hzzrkUeRB2zrkUeRB2Ve22tDtQz/jnnXGeE3bOuRT5SNg551LkQdg551LkQdg551LkQdg551LkQdjVWpIaxL8rS2qSdn/qGklFxR5n99rfDPMg7GodSWtJ6mpmv0vaC3gDuE7S5Wn3rS6QtAyAmf0haXNJ+0lqXNEaaa5yfIqaq3UkHQLcCPQCdiIUV5wNnAL8aGa9U+xepklqCfQBngYWAPcA04D5wH+AsbGysKshPhJ2tY6ZPQScDPQHmpjZEGAMcBnQStKtafYv45YFpgMHAecBPcxsB+B94FSgYyzh7mqIB2FXa+RykpLamdmDwGnATpJ2iKOzz4GrgJaSOqTY1UySJDP7Brgf+ARYF+gMYGbnAV8TqgZ3Sq2T9ZAHYVdrmJlJ2hsYKKmjmT0BXATcLml7M/uDEDyOMbOP0+xr1sQAbJJ2AVYDHgYGAl0l7Q5gZhcAXwK/pdfT+sdzwq7WiKPb+4BeZjYm0X4EcDVwiJm9klb/si4G2/5AbzMbIml1oAewIfCCmT2bagfrKc/9uNqkBfB1LgBLWsrMFprZvZIWAT5iqKA4I+I04F9m9mocGU+R9CzQCNhX0gjC4uf+OdcgD8IuNYmfyEUx1TAN+FXSBsAXZrZQ0nbAZmY2IPmcNPudUQ2ApQmfMYTA+yswC7gLaG5mP6TUt3rNc8IuFYkA3B24XNI1hClT3wMnASdI6kEIEONzz/MAXJjESc41JTUys7nAEOAqScuZ2a/xC24wgJlNSq+39ZuPhF0qYgDeEbgEOBh4kZBuOAs4BlgH2BI42cxeTq2jGRU/3z2A84HXJK0IXAc0B96SdBdwJHCemc1Msav1np+Yc6mRdBHwJiH4XgYcamYTE9ubmNn8lLqXafEk54PA3oRfFp2A/czsJ0kHEX51zDCzNzzFky4fCbs0TSdcFdcGONzMJko6GljDzC7Gp0qVWyKgNiYE4XWBHYDDYgDeAnjSzBbmnuMBOF2eE3Y1IpGj3FrSzpI2B14CNgFuBybHttOBkRDWNkirv1mTWHwnN7D6GjiUcFnybmY2Ic4RPhdYLoUuulJ4OsLVGEm7EuapXg3cAWwBrAH0JIx6VwKuNrNB/hO5cImTnH8HDgTeAyYAKxDSEcOBSYSrDfuY2TMpddWVwNMRrtrFUVoroDewD7A6YcbDt2b2nqRXCVOompnZZA/A5RMD8E7AtYS5wOcT1oLoR5iSdhphZHyBmT3nn2/t4iNhV2MkXQj8DOwPHGVmn0s6FPjQzD5Mt3fZFdddPhkYBSwCbgX2NrOpkpYxs3mJfT0A1zI+EnbVIvETeSVgbgwErQijtBXiSaJOwJnAcWn2NeviusuzCGtB/AbsYWbfxrWYV5V0e255Sg/AtY8HYVctEhdi9AXel7TIzI6UtA5wj6RJhLP2F5nZuyl2NXMSX3CbAWsRTmSOA0YDk2IA3oqQA/63rw9cu3k6wlULSRsScpEPEQLELcAyZrZHvBKuCJhuZiP8J3L5xZNwNxFWlTPgNcLc37WBrsBCoK+ZDUqtk64gHoRdlZO0PPAB8CHhAoF5sf054DEzuyfN/mVdXFtjAHC2mb0fv9Q2B0ab2bOS1gTmm9n3/gVX+/k8YVclEvOA25rZj8AJQDvg74ndRgJNU+he5iXmAQPsSFh+cjuAOOVsHnBEfDzZzL6P9z0A13KeE3aVlshR7g38W9LJcSpUY+BaSVsC7xLWKjgp1c5mUOLz3Rn4kbDmMsBWkvaLi9+/BnSR1NzMfkqts67cPAi7SosBogtwMWH9h08ktTCzxyVNBx4hzA3eK27zn8jlkPiCuxI408zGSnqCkAv+T9y2DvBfD8DZ40HYVZXWhNHuKvHKuD0k/U6YftaLcCHBmoQTSa4cJLUGzgb2jXOrNwGWB54kXOTSFXjEK2NkkwdhVyGJn8itCT+RPwe+IyyX2JewROUOQDsze0FSK+BKSW+a2c9p9TujGhAWYN9N0jmEvPp2wBmEtSEWADtK+sLMBqfXTVcRPjvCVVj8GXw0MJUwR/U5YKGZzY0XYtwPHGdmb8X9m8XFxV0eiS+4TQnB9wfC7Ie9gOct1Ic7ENjJzE6QtAawMzDYzKan13NXER6EXYXEJREHArsDNwMirNplwKaEihhnxSlTRWb2h+eCC6dQlLMvcDdhofsuZvZV3LYjcAPhQozBsa2Bmf2eUnddJXg6whWkhAC6EmEJyg6E9YAPMbN5cVT2A3CAmX0Un/cH+HSpQsSpaKsSLu/em7DS3HTg57itDXABYY7w4Ny/iwfg7PKRsCtTnGq2h5k9GX8irwt8SbhgYLm4baqkfYHuwCnJRWNcfpKWAhqa2fz4WS9NWHHuK8LCPEfGE3I9CGswNzGzmf7Lom7wkbArxEJgDUmfxft7E07GfQjMATpIakuYona+B+DCSWoI7AT8Eq9025aQfuhGKEm0nJktkNQZOAf4zMw+Bf9lUVf4SNgVJC4W8wzwg5ltnmj7G+EKroXA/eYLspdbXAv4cmBl4Awze0LSyoTqyO8QZp78k7DYkS/IXsd4EHalSgbT+JN5NcLlyJ0JOd8fJK1uZlNy69Z6AC5csc/3bsLn2x9438ymSWpGKPc0A/jEzF7xz7fu8SDsSpSYJrUn0AX43cz6SCoC/kc4YXQF4TLk481saordzZzE57sa8A3QiJCKOAZ4wczul7QCsJSZTUuzr656+QI+rkQxQOxBCLRPAEdKehxoYWanEdYqOBu4yQNw+SW+4B4jfMYnA68T1oXYXdLVwKeEy71dHeYjYVciSU0I84D7AasA5xFKEzUiXD47W1LL+Nd/IpeTpG0J6wHvS0g5bA28Qfhi6wBsBkw2s2GpddLVCA/CbrHcRRWJxy2AFQmjsx3jFKrZwPOEaVNesaEckhdUxOlmnwNtgcuAPoQ1Nr4GLjazHxLP8y+5OsynqLncqHeRmS2U1JVwQcBEMxsjqSXhYoHVJS1LWDTmTg/Ahctdrm2hFtyOhMA7nvC5Hg8cY2YfSNofaEn44lschD0A120ehOs5hSoYZwKDYjC+h5CnvF3S4XFd4AnApYTVuo4xszd9dFYYScsAz0u6jlBt5EbgY8JJuPGEk57fSFoa2ADoaWbj0+qvq3mejqjn4tSzvoSVuoqAp8xsWLz67R6gu5m9LqkDoUacF+Usp/hZngPMBM6Jo95DCSPiVQhzrb8EHjKzx1LrqEuFB+F6LLGwzlKE9Qh2JMyEuC3mf/8BPA7sY14wslIUCnM+ClxhZlfHK+UOAtYnrJR2i1+KXD/5FLV6LAbgIjNbSDg5NJSwLsSWkpY2syeBA4Hf0uxnXWBmQwnLfh4l6ZCYU38Y+Izw62Nm3M8DcD3jI+F6qtjVWg3NbFHMS14INAMGAW+Y2YLi+7uKi3OvLwWuM6867fCRcL0Tl0OExL99DMBLxYB7CaFSw34kKiN7AK4aZvYCYaGjsyWtEq9AdPWYj4TrkcSlsrsQFoT5CvjSzO6P25eK09SWBtqa2edp9rcuk7RCci6wq7/8W7geiQF4e+B6YDhhzYKTJP07bl8Yc8QLPABXLw/ALsfnCdc/qwEDzewuAEkjgaslDTaz8ckr5pxz1c9HwnVcIgec0wQ4PPF4PKFKsuelnEuBB+E6LpeCkHSipA5mdjswUtIwhTL0WwCbAEul21Pn6ic/MVdHJU7CdQbuJFwqOw94E3iAcJVcW2B54Eq/GMO5dHgQrsMkbUWYcnaWmY2TdAhhycRxZnZHnB7V0q/Uci49no6o21oCuwB/j48fA94CtpbUGxAwC3wesHNp8dkRdZiZvRTXf7hS0jQzeyhWx2gAfJBb29Y5lx4PwnWcherHi4BL43oQ9wAPpd0v51zgOeF6QtLewFWE9MS3Ph/YudrBg3A94pfKOlf7eBB2zrkU+ewI55xLkQdh55xLkQdh55xLkQdh55xLkQdhlwpJv0saK+kjSY/F0vAVPdbdkvaP92+PlaFL23cHSdtU4DUmSWpdaHuxfX4u52tdJOmM8vbRZZMHYZeW+WbW0cw2IpRTOiG5MVYjLjczO9bMPs6zyw5AuYOwc9XFg7CrDd4A1o2j1DckDQI+ltRA0tWSRksaJ+l4CCvESbpB0meSXgZWzB1I0nBJW8T7u0l6T9IHcenOtoRg/39xFP43SStIeiK+xmhJXeNzl5f0kqTxkm4nrLORl6SnJY2Jz+lVbFv/2D5M0gqxbR1Jg+Nz3pDUvio+TJctftmyS1Uc8e4ODI5NnYCNzGxiDGRzzGxLSY2AtyS9BGwGrA90AFYiLNN5Z7HjrgAMBLaLx2oVV4u7BfjZzPrF/R4E+pvZm5LWAIYAGwB9gDfN7BJJewI9C3g7x8TXaAKMlvSEmf0ILAu8a2b/J+nCeOyTgduAE8zsi7jk6E3AThX4GF2GeRB2aWkiaWy8/wZwByFNMMrMJsb2bsAmuXwv0AJoB2wHPBQXIJom6ZUSjr818HruWGY2s5R+7AJ0SBQgaS6paXyNf8TnPi9pVgHv6VRJ+8b7q8e+/gj8ATwS2+8HnoyvsQ3wWOK1GxXwGq6O8SDs0jLfzDomG2Iw+iXZBJxiZkOK7bdHFfajCNjazH4toS8Fk7QDIaB3MbN5koYDjUvZ3eLrzi7+Gbj6x3PCrjYbAvxL0lIAktaTtCzwOnBQzBm3AXYs4bkjgO0krRWf2yq2zwWaJfZ7CTgl90BSLii+Dhwa23YHliujry2AWTEAtyeMxHOKgNxo/lBCmuMnYKKkA+JrSNKmZbyGq4M8CLva7HZCvvc9SR8BtxJ+vT0FfBG33Qu8U/yJcaGiXoSf/h/wZzrgWWDf3Ik54FRgi3ji72P+nKVxMSGIjyekJb4uo6+DgYaSPiGsVjcise0XYKv4HnYiVDsBOAzoGfs3HuhRwGfi6hhfwMc551LkI2HnnEuRB2HnnEuRB2HnnEuRB2HnnEuRB2HnnEuRB2HnnEuRB2HnnEvR/wN/v3hs2kfmcAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "gE5mZvfmUyJF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "metadata": {
        "id": "-3NPPARgU0Kt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "EUt-18pmU0pA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bca5736-ce09-48a0-b7d3-a27d3cadfed0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7853907134767837"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}