{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "5dafe010-40d3-4e45-a4b1-1b95c42e564c"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6049c9b-31a7-4628-b5a2-abee65597d13"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.799528628000189\n",
            "GPU (s):\n",
            "0.03726148900022963\n",
            "GPU speedup over CPU: 75x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de00964b-c974-4b0e-cde0-31ba1d91c2f6"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"ğŸ˜œ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ«\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ğŸ–\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜‡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¬\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜ƒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’™\", \" \",string)  \n",
        "   string = re.sub(\"ğŸ˜›\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ–•\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜‰\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’©\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¢\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¨\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤£\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜ˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’ƒğŸ½\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘¹\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤˜\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜±\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤”\", \" \",string) \n",
        "   string = re.sub(\"ğŸŒˆ\", \" \",string) \n",
        "   string = re.sub(\"ğŸ’•\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘©â€â¤ï¸â€ğŸ‘©\", \" \",string) \n",
        "   string = re.sub(\"ğŸ˜\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘†\", \" \",string) \n",
        "   string = re.sub(\"ğŸ˜–\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘‡\", \" \",string) \n",
        "   string = re.sub(\"ğŸ”¥\", \" \",string) \n",
        "   string = re.sub(\"ğŸ˜˜\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‰\", \" \",string) \n",
        "   string = re.sub(\"ğŸ¤¬\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‡©ğŸ‡ª\", \" \",string)  \n",
        "   string = re.sub(\"ğŸ’”\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™ˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¯\", \" \",string)\n",
        "   string = re.sub(\"ğŸŸ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ›¶\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜“\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜³\", \" \",string)\n",
        "   string = re.sub(\"ğŸš€\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¸\", \" \",string)\n",
        "   string = re.sub(\"ğŸ“ˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜…\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜†\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™ğŸ¿\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘ğŸ½\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤­\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¤\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜²\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤®\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™„\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ…\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘‹\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’ª\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜„\", \" \",string)\n",
        "   string = re.sub(\"ğŸ§\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜ \", \" \",string)\n",
        "   string = re.sub(\"ğŸˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸš‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸš‡\", \" \",string)\n",
        "   string = re.sub(\"ğŸšŠ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤·\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¥\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™ƒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ”©\", \" \",string)\n",
        "   string = re.sub(\"ğŸ”§\", \" \",string)\n",
        "   string = re.sub(\"ğŸ”¨\", \" \",string)\n",
        "   string = re.sub(\"ğŸ› \", \" \",string)\n",
        "   string = re.sub(\"ğŸ’“\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’¡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¸\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¥ƒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¥‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜·\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤\", \" \",string)\n",
        "   string = re.sub(\"ğŸŒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤›\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜€\", \" \",string)\n",
        "   string = re.sub(\"ğŸ›¤\", \" \",string)\n",
        "   string = re.sub(\"ğŸ„\", \" \",string)\n",
        "   string = re.sub(\"ğŸ“´\", \" \",string)\n",
        "   string = re.sub(\"ğŸŒ­\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤•\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜­\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¾\", \" \",string)\n",
        "   string = re.sub(\"ğŸ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¦\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¯\", \" \",string)\n",
        "   string = re.sub(\"ğŸ•¯ï¸\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae8abdc-b41b-4cf5-e405-a1dcca82e7e7"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150, return_sequences=True))(embedding_layer)\n",
        "#X= tf.keras.layers.LSTM(150, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(160, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1416 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1416.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_layer_norm', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1416.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc47251-914c-4c42-826e-4ece50c29a61"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 300)      1102800     ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 300)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 160)          48160       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 160)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            161         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,885,201\n",
            "Trainable params: 135,885,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 8\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5e4618-ddac-40fe-8897-cbb9a6f7eaf6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1416.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e649e52-f639-40a9-c15e-a0071505736f"
      },
      "source": [
        "model1416.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =8,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "157/157 [==============================] - 53s 264ms/step - loss: 0.6101 - binary_accuracy: 0.6740 - metrics_recall: 0.1843 - metrics_precision: 0.4104 - metrics_f1: 0.2251\n",
            "Epoch 2/8\n",
            "157/157 [==============================] - 41s 262ms/step - loss: 0.4999 - binary_accuracy: 0.7568 - metrics_recall: 0.5413 - metrics_precision: 0.6995 - metrics_f1: 0.5777\n",
            "Epoch 3/8\n",
            "157/157 [==============================] - 41s 262ms/step - loss: 0.4164 - binary_accuracy: 0.8135 - metrics_recall: 0.6901 - metrics_precision: 0.7490 - metrics_f1: 0.7039\n",
            "Epoch 4/8\n",
            "157/157 [==============================] - 41s 262ms/step - loss: 0.3450 - binary_accuracy: 0.8507 - metrics_recall: 0.7587 - metrics_precision: 0.7886 - metrics_f1: 0.7626\n",
            "Epoch 5/8\n",
            "157/157 [==============================] - 41s 262ms/step - loss: 0.2842 - binary_accuracy: 0.8846 - metrics_recall: 0.8065 - metrics_precision: 0.8499 - metrics_f1: 0.8171\n",
            "Epoch 6/8\n",
            "157/157 [==============================] - 41s 263ms/step - loss: 0.2294 - binary_accuracy: 0.9096 - metrics_recall: 0.8598 - metrics_precision: 0.8801 - metrics_f1: 0.8616\n",
            "Epoch 7/8\n",
            "157/157 [==============================] - 41s 262ms/step - loss: 0.1978 - binary_accuracy: 0.9281 - metrics_recall: 0.8799 - metrics_precision: 0.9063 - metrics_f1: 0.8868\n",
            "Epoch 8/8\n",
            "157/157 [==============================] - 41s 262ms/step - loss: 0.1780 - binary_accuracy: 0.9339 - metrics_recall: 0.8982 - metrics_precision: 0.9058 - metrics_f1: 0.8968\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4281e1d450>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1416.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790a8a6d-06cb-417e-eefc-44bad3299ac7"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaed2263-f33a-434c-e90b-1cd5d77e1dde"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0170269 ],\n",
              "       [0.38291138],\n",
              "       [0.953093  ],\n",
              "       ...,\n",
              "       [0.95512885],\n",
              "       [0.03619198],\n",
              "       [0.01808408]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106248fe-b026-49dd-ad59-c67ef02c93fd"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7661381653454133"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "040eadee-0148-4967-b198-de4aae5720d1"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2098  232]\n",
            " [ 594  608]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEmCAYAAACzoiEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5xU1fnH8c93KYqKFEFEELAgtigqAopR7ErsRpRgNyqJGk1iiybBGo2anwa7WNCgiIoFFVGCsWEDlKqiKChVmkqzUJ7fH+cMDuvu7Oyyu3fu7vPO6772zrntzGCeOfPcc8+RmeGccy4ZRUlXwDnnajMPws45lyAPws45lyAPws45lyAPws45lyAPws45lyAPwq7aSRog6dq4/ktJU/I45nJJ98X1dpJMUt34+lVJv63aWledsuov6W5Jf6vOOrnqUzfpCrjazczeADrksd8/qqE6QAiKwEAzu6+6rpl17dOA35rZ3pkyM+tT3fVw1ceDsHORJAFKuh6udvF0hKtyknaV9L6kJZIGA+tnbesuaWbW60slzYr7TpF0QCy/UtLAHJfZWtJ7khZLelZS06xzdpX0lqRvJI2X1D1r26uSrpM0ClgO/Af4JXC7pKWSbi/h/WTSIadLmiHpa0l9JO0haUK8zu1Z+69V9+LplKzy7YG7gT3jtb+J5WvSN67m8SDsqpSk+sAzhODWFHgCOK6UfTsA5wF7mFlD4BBgep6XOgU4A2gJrAT6xXO2Al4Aro3XvwgYIql51rEnA2cDDYHTgDeA88xsIzM7L8c1uwDtgROAW4ErgAOBHYGekvbNs+4AmNlHQB/g7XjtxuU53qWTB2FX1boC9YBbzWyFmT0JjC5l31XAesAOkuqZ2XQz+yzP6/zHzCaZ2TLgb4QgWAc4CRhmZsPMbLWZjQDGAD2yjh1gZpPNbKWZrSjHe7vGzL43s5eBZcAgM5tnZrMIgXzXcpzL1VIehF1V2xyYZWuPFPVFSTua2VTgQuBKYJ6kxyRtnud1ZhQ7fz2gGdAWOD6mCL6JP/H3JrSYSzq2PL7KWv+uhNcbVfC8rhbxIOyq2hygVbzpldGmtJ3N7NHYM6AtYMA/87zOFsXOvwJYQAiw/zGzxlnLhmZ2Q/Zli1cjz2vmaxmwQdbrzXLs68Ma1jIehF1Ve5uQo/2DpHqSjgU6l7SjpA6S9pe0HvA9oTW5Os/rnCRpB0kbAFcDT5rZKmAgcISkQyTVkbR+vBnYOse5vgK2yvO6+RgH7COpjaRGwF/KuHbrmEt3tYAHYVelzOxH4FjCDa9FhJtYT5Wy+3rADYQW7FxgU3IHrGz/AQbE49YH/hCvPwM4CrgcmE9oGV9M7v/2/w38OvZ66Jfn9UsV89CDgQnAWOD5HLu/AkwG5kpasK7XdoVPPqi7c84lx1vCzjmXIA/CzjmXIA/CzjmXIA/CzjmXIB/Ap0CpbgNT/YZJV6PW2HX7UrsuuyrwxRfTWbBgQaUMllRn47ZmK7/LuY99N/8lMzu0Mq5X2TwIFyjVb8h6HXomXY1aY9S7Pxunx1Whbl06Vdq5bOV3Zf5/5ftxdzSrtAtWMg/Czrl0k6CoTtK1qDAPws659FN6b295EHbOpZy3hJ1zLllK74QoHoSdc+kmUp2OSG/NnXMOWJOOyLWUdQZpC0n/k/ShpMmSLojlTSWNkPRp/NsklktSP0lT45RWu2Wd69S4/6eSTi3r2h6EnXPpJ+VeyrYS+LOZ7UCYDeZcSTsAlwEjzaw9MDK+BjiMMLVVe8LUWHeFaqgp0Jcw9VVnoG8mcJfGg7BzLuUU0hG5ljKY2Rwzez+uLwE+AloRhkF9KO72EHB0XD8KeNiCd4DGkloS5kUcYWaLzOxrYASQ8yERzwk759JN5JNyaCZpTNbre83s3hJPJ7UjzA/4LtDCzObETXOBFnG9FWtPizUzlpVWXioPws65lFM+rd0FZlbmY3qSNgKGABea2eLsWbnMzCRV+gDsno5wzqWbgDp1ci/5nEaqRwjAj5hZZvaXr2Kagfh3XiyfxdrzGraOZaWVl8qDsHMu/dbxxlyciPZ+4CMz+7+sTUOBTA+HU4Fns8pPib0kugLfxrTFS8DBkprEG3IHx7JSeTrCOZdyeaUjytINOBmYKGlcLLucMOfh45LOBL4AMiMFDQN6AFOB5cDpAGa2SNI1wOi439VmtijXhT0IO+fSbx0fWzazNwmJjZIcUML+BpxbyrkeAB7I99oehJ1z6ZZ/X+CC5EHYOZd+PoCPc84lpVJywonxIOycSz9PRzjnXEIkKEpvKEtvzZ1zLsNbws45lyC/MeeccwmR35hzzrlkeTrCOeeSIaCoyFvCzjmXDFH6A8cp4EHYOZdyQp6OcM655Hg6wjnnEuQtYeecS4gkVORB2DnnEpPmlnB6EynOORdJyrnkcfwDkuZJmpRVNljSuLhMz8y4IamdpO+ytt2ddczukiZKmiqpn/K4uLeEnXPpJiojHTEAuB14OFNgZiesuYT0L+DbrP0/M7OOJZznLuAs4F3CFEiHAi/murC3hJ1zqbeuLWEzex0ocS642JrtCQwqow4tgY3N7J04/dHDwNFlXduDsHMu1YQoKirKuQDNJI3JWs4uxyV+CXxlZp9mlW0p6QNJr0n6ZSxrBczM2mdmLMvJ0xHOufQru7G7wMw6VfDsvVi7FTwHaGNmCyXtDjwjaccKntuDsHMu5VR1vSMk1QWOBXbPlJnZD8APcX2spM+AbYFZQOusw1vHspw8HeGcS7080hEVdSDwsZmtSTNIai6pTlzfCmgPfG5mc4DFkrrGPPIpwLNl1n1daudqvtYtGjP83j/w/pArGPvkFZzbqzsATTbegOfvOo+Jz/6d5+86j8YNGwDQuGEDBv/rLN4b/Bfe+M9F7LB1yzXnOr/3fox98grGPHE5D11/GuvV9x9iucyYMYNDDtyPXXfegd122ZHb+/0bgKv6/o09dt2ZLrt35PDDDmb27NkADHr0EfbYdWc6dfwF3X+5FxPGj0+y+tVG5L4pl2cXtUHA20AHSTMlnRk3ncjPb8jtA0yIXdaeBPqYWeam3u+B+4CpwGeU0TMCQOEmnis0RRtsaut16Jl0Ndis2cZs1mxjxn08k402WI+3Hr2Unn+6l5OP6MLXi5dz84MjuOj0g2jccAP+2u9Z/nHh0Sxd/gP/uPdFtm3Xglsv60mPPrexefNGjHzwj+x63HV8/8MKBv7zDIa/OZmBz72b9FsE4OvRtyddhZ+ZM2cOc+fMYdfddmPJkiXs1WV3Hn/yGVq1bs3GG28MwB239ePjjz7ktjvv5u233mK77benSZMmvDT8Ra69+kreeKswPt/iunXpxNixYyolh1B/022s+XE35dxn9t3Hjl2HnHCV8pawy2nugsWM+zj8Elu6/Ac+njaXzZs35vDuO68JoAOfe5cj9tsZgO222ozXRn8CwCfTv6Lt5k3ZtGlDAOrWqUOD9epRp04RDdavz5z535ZwRZfRsmVLdt1tNwAaNmzIdtttz+zZs9YEYIDly5etaentuddeNGnSBIDOXboya9bMn5+0hlrXlnCS/Pegy1ublk3p2KE1oydNZ9NNGjJ3wWIgBOpNNwmBduInszhq/10Y9cFndNqxLW1aNqVVi8Z88NEMbn14JJ+8eA3f/fAjI9/+mJHvfJzk20mVL6ZPZ9y4D9ijcxcA+v7tCh4Z+DCNGjVi+Ij//Wz/AQ/ezyGHHFbd1UxMoQfaXAqyJSxpgKRfl2P/xpJ+X5V1WhfxkcdmSddjXWzYoD6Dbv4tF988hCXLvv/Z9kxW6+YHR9Co4Qa889hl/O7EfRk/ZSarVq2mccMGHN79F2x/eF+2OvgKNmxQnxN77FHN7yKdli5dSq+ex3HTv25d0wq+6prrmDptBif26s3dd66dSnnt1f/x0IP3c+31/0yiuolQkXIuhawgg3AFNCYkxF0VqFu3iEE3n8XgF8fw7CvhZs+8hUvYrFkICJs125j5i5YAsGTZ95xz5UC6nngDZ/7tYZo12Yhpsxayf5ftmD57IQu+XsrKlat55pXxdN1ly8TeU1qsWLGCXj2P44RevTn6mGN/tv2EXr155ukha15PnDCB353zW54Y8iybbLJJdVY1MWWlIgq9lVwlQTgOcPGRpP6SJkt6WVKDuK2jpHckTZD0tKQmpZxmH0lvSfo80yqWtJGkkZLej4NkHBX3vQHYOg6mcVPc92JJo+N1roplG0p6QdJ4SZMknRDLp0u6MZ7zPUnbxPLmkobE84yW1C3rPA/EfT/I1ENSHUk3x3NPkHR+1vs5P6ve21XuJ1617u7bmynT5tJv4Ctryl54bSInHRF+Gp90RBeef3UCAI02akC9umH68dOP2Ys335/KkmXfM2PuIjr/YksarF8PgP06d2DKtK+q+Z2ki5nR56wz6bDd9lzwxz+tKZ/66U8Pbj0/9Fm27RD+c/ryyy85seex3P/gf2i/7bbVXt8kpTkIV2VOuD3Qy8zOkvQ4cBwwkPA89flm9pqkq4G+wIUlHN8S2BvYDhhK6AryPXCMmS2OP+/fkTQUuAzYKTOghqSD4/U7E56lGSppH6A5MNvMfhX3a5R1vW/N7BeSTgFuBQ4H/g3cYmZvSmoDvARsD1wBvGJmZ0hqDLwn6b+EfoHtgI5mtlJS06zzLzCz3WLa5CLgt8XfsMKjlOFxynob5fMZV7m9Om5F78O7MPGTWbzz2GUA9L19KDc/OIKB/zyDU4/eky/nLOKkSx4Awo25/lefjJnx0Wdz6HPVIwCMnvQFT//3A95+9FJWrlrN+I9ncv+QUYm9rzR4a9QoHn3kP+y00y/osnsYK+aqa//BgAfv59NPplCkItq0bUu/O8IgXtdfezWLFi7kwvPDj8K6desy6t0xidW/OhV6yiGXKumiJqkdMMLM2sfXlwL1gNuAiWbWJpZvDTxhZrsVO35APP6R+HqJmTWUVA+4hdBPbzXQAdgSWB943sx2ivvfDPwa+CaeciPgeuAN4GVgcNz/jbj/dGB/M/s8XmOumW0iaR4wO6tqzeM1X43XXBnLmwKHANcCd5vZiGLvZzrQzcxmSeoCXGdmB+b6DAuli1ptUYhd1Gqyyuyitl6L9taq979z7jPtll8VbBe1qmwJ/5C1vgposA7HZ/6xehMC4e5mtiIGt/VLOFbA9WZ2z882SLsBPYBrJY00s6vjpuxvo8x6EdDVzL4vdg4Bx5nZlGLl+byfVXivFOcqjQRFKW4JV+uNOTP7FvhaP406dDLwWjlO0QiYFwPwfkDbWL4EaJi130vAGZI2ApDUStKmkjYHlpvZQOAmILsFfkLW37fj+svAmryupMz4oS8RcryK5bvG8hHAOQrPm1MsHeGcqxLpvjGXRIvsVOBuSRsAnwOnl+PYR4DnJE0ExgAfA8TRjEYpjIr/opldLGl74O34D7AUOAnYBrhJ0mpgBfC7rHM3kTSB0GLtFcv+ANwRy+sCrwN9gGsIeeMJkoqAaYQc8n2EgTwmSFoB9CcMFO2cq0IFHmdz8seWWZOz7WRmC5KuS4bnhKuX54SrV2XmhNdvua21O/W2nPtM+eehtTIn7JxzVU6kOyfsQRgws3ZJ18E5V3EehJ1zLilKd07Yg7BzLtWED+DjnHMJEkVFuZcyzxCGIZgXe1hlyq6UNCsOhzBOUo+sbX+RNFXSFEmHZJUfGsumSrosn9p7EHbOpV4l9BMeABxaQvktZtYxLsPitXYgzLixYzzmzjhuTB3gDuAwYAegV9w3J09HOOdSrTKemDOz1+NwC/k4CngsTvg5TdJUwjg1AFPN7PNQLz0W9/0w18m8JeycSz0p9wI0kzQmazk7z1OfF0dEfEA/jfjYCpiRtc/MWFZaeU4ehJ1zqZdHOmKBmXXKWu7N47R3AVsDHYE5wL+qou6ejnDOpVsVDeBjZmsGvJbUH3g+vpwFbJG1a+tYRo7yUnlL2DmXaqGLWpnpiPKfV2qZ9fIYINNzYihwoqT1JG1JGLv8PWA00F7SlpLqE27eDS3rOt4Sds6l3LqPlCZpENCdkDueSZhsonscOdGA6cA5AGY2OU5U8SFhTPFzzWxVPM95hFEW6wAPmNnksq7tQdg5l3qV0DuiVwnF9+fY/zrguhLKhwHDynNtD8LOuXTzx5adcy45YRS19N7e8iDsnEs9bwk751yC0jyAjwdh51yqSfkN0lOoPAg751IvxQ3h0oOwpNtYexr4tZjZH6qkRs45V051amhLeEy11cI55yooPBVXA4OwmT2U/VrSBma2vOqr5Jxz5ZPihnDZY0dI2lPSh8DH8fUuku6s8po551ye1nVmjSTl08P5VuAQYCGAmY0H9qnKSjnnXL4EqIz/FbK8ekeY2YxiOZdVVVMd55wrJ6nG3pjLmCFpL8Ak1QMuAD6q2mo551z+UnxfLq8g3Af4N2GajtmEYdrOrcpKOedcvgQUpTgKlxmEzWwB0Lsa6uKccxVS6Dffcsmnd8RWkp6TNF/SPEnPStqqOirnnHNlKWtWjUJvJOfTO+JR4HGgJbA58AQwqCor5Zxz5VEk5VzKEmdTnidpUlbZTZI+jrMtPy2pcSxvJ+k7SePicnfWMbtLmihpqqR+yuMpknyC8AZm9h8zWxmXgcD6eRznnHPVYl2DMDAAOLRY2QhgJzPbGfgE+EvWts/MrGNc+mSV3wWcRZh3rn0J5/x53UvbIKmppKbAi5Iui9G/raRLKOf0Hc45V1XCjbncS1nM7HVgUbGyl81sZXz5DmH25NLrESYG3djM3jEzAx4Gji7r2rluzI0lDOCTeQvnZNePtb8VnHMuGfkNZdlMUvZ4OPea2b3luMoZwOCs11tK+gBYDPzVzN4g9CCbmbXPzFiWU66xI7YsRwWdcy4xeaReF5hZpwqe+wrCrMqPxKI5QBszWyhpd+AZSTtW5NyQ5xNzknYCdiArF2xmD1f0os45V1ky6YgqObd0GnA4cEBMMWBmPwA/xPWxkj4DtgVmsXbKonUsy6nMICypL9CdEISHAYcBbxLyHc45l7iqeFhD0qHAJcC+2SNISmoOLDKzVbG7bnvgczNbJGmxpK7Au8ApwG1l1j2PuvwaOACYa2anA7sAjcr9jpxzrgpIldJFbRDwNtBB0kxJZwK3Aw2BEcW6ou0DTJA0DngS6GNmmZt6vwfuA6YCnwEvlnXtfNIR35nZakkrJW0MzAO2yOM455yrFuv6xJyZ9Sqh+P5S9h0CDCll2xhgp/JcO58gPCZ2Uu5P6DGxlPCN4ZxzBaHQn4rLJZ+xI34fV++WNJzQD25C1VbLOefyI/J+IKMg5Zroc7dc28zs/aqpkgPYfpvWDBp6fdLVqDU+nr0k6SrUKt+tWF15J1O6B/DJ1RL+V45tBuxfyXVxzrkKyaeHQaHK9bDGftVZEeecqwhRc6e8d865VEhxDPYg7JxLtzBmcHqjsAdh51zq1UlxUjifmTUk6SRJf4+v20jqXPVVc865smXmmFvH8YQTk8/3x53AnkDmiZIlwB1VViPnnCunojKWQpZPOqKLme0Wx87EzL6WVL+K6+Wcc3mRVON7R6yQVIfQNzgzglAl9rR2zrl1U+AZh5zyCcL9gKeBTSVdRxhV7a9VWivnnMuTgLo1uSVsZo9IGksYzlLA0Wb2UZXXzDnn8lSjW8KS2gDLgeeyy8zsy6qsmHPO5SXPyTwLVT7piBf4acLP9YEtgSlAhedUcs65yiKgToqbwmX23jCzX5jZzvFve6AzPp6wc66ArOuU95IekDRP0qSssqaSRkj6NP5tEsslqZ+kqZImZI84KenUuP+nkk7Nq+7lfbNxCMsu5T3OOeeqQmYAn1xLHgYAhxYruwwYGRufI+NrCPNsto/L2cBdEII20JcQHzsDfTOBO5d8csJ/ynpZBOwGzC7rOOecqxZa9xtzZva6pHbFio8iTHIM8BDwKnBpLH84zr78jqTGklrGfUdk5puTNIIQ2AflunY+OeGGWesrCTniEudXcs65JOTxaHIzSWOyXt9rZveWcUwLM5sT1+cCLeJ6K2BG1n4zY1lp5TnlDMLxIY2GZnZRWSdyzrkkhHREmbstMLNOFb2GmZkkq+jxuZRadUl1zWwV0K0qLuycc5VDFJWxVNBXMc1A/Dsvls9i7RnnW8ey0spzyvX98V78O07SUEknSzo2s+T5JpxzrkpJoSWca6mgoUCmh8OpwLNZ5afEXhJdgW9j2uIl4GBJTeINuYNjWU755ITXBxYS5pTL9Bc24KlyvBnnnKsy6zpcpaRBhBtrzSTNJPRyuAF4XNKZwBdAz7j7MKAHMJXwINvpAGa2SNI1wOi439WZm3S55ArCm8aeEZP4KfhmVEluxDnnyktUSu+IXqVsOqCEfQ04t5TzPAA8UJ5r5wrCdYCNoMSEigdh51zBqKlDWc4xs6urrSbOOVcBovAHbs8lVxBO71eLc672qMETff4sF+Kcc4Um7QP4lBqE87mr55xzhSC9IdinvHfOpZ4oqqE35pxzruDV5BtzzjmXCjX1xpxzzhU+rfsTc0nyIOycSzVPRzjnXMK8JeyccwlKcQz2IOycS7eQjkhvFPYg7JxLOXk6wjnnkpTiGJzqm4rOORdm1pByLmWfQx0kjctaFku6UNKVkmZllffIOuYvkqZKmiLpkIrW34OwK5fD9tqJ4w7qSs9Du9HrV/sCMOXDiZx89AEcd1BXzj+9J0uXLF7rmDmzZtB1u5Y8dE+/JKqcaku+/YaL+pzMMfvvzrH7d2L82Hf59ptF9Ol9FEfu25E+vY9i8bdfh30Xf8sFZ/Sk56F7cdyBnXn28YEJ1776SLmXspjZFDPraGYdgd0JM2Y8HTffktlmZsPC9bQDcCKwI2Fa+zvjxMjl5kHYldt9g1/g8eGjGPTCawBcdcl5XHDZVQwZ8Q77H3oEA+7591r733z15ezd/aAkqpp6N151KXvteyBPvzKWwcPfYqttOvDgnbfQudu+DH1tHJ277cuDd94CwOMP92er9tvx+PC36D94GP937eWs+PHHhN9B9VAZ/yunA4DPzOyLHPscBTxmZj+Y2TTCVEedK1J3D8JunX0x7TN27xIm5d7zl/sxctjQNdteeel5WrVpy9bbbpdU9VJryeJvef/dtzjmxFMAqFe/Pg0bNebVES9wxHG/AeCI437D/15+PhwgsWzpEsyM75YtpVHjJtSpW/Nv+2SGslyXdEQxJwKDsl6fJ2mCpAfiBJ4ArYAZWfvMjGXl5kHYlY9En5OO5sQe+/DkIw8CsPW22/G/l18A4OUXnmHunDDL9/JlS3nwrlvoc+FliVU3zWbP+IImm2xC34t+x4mH7c1Vl5zHd8uXsXDBfJq32AyAZpu2YOGC+QCceOrZTJv6CQfvsS3HH7InF/f9J0VFteP/4nmkI5pJGpO1nF3yeVQfOBJ4IhbdBWwNdATmAP+q7LoX7L+QpHaSJpVj/6NjnqbgSDpN0u1J16MyDBjyEoOHvcEdDw9h8MP9GfvuKK666U4GP9yfE3vsw/KlS6hXrx4Ad91yPSedeS4bbLhRwrVOp5WrVvLxpPEcf9KZPPbimzTYYAMeuPP/1tpH+unn9luvjaTDjr/g5dGf8NiLb3LD3y/+WX6+psojHbHAzDplLfeWcqrDgPfN7CsAM/vKzFaZ2WqgPz+lHGYBW2Qd1zqWlVvBBuEKOBooyCBck7TYbHMANmnWnP0POZxJ48ay5Tbbcs8jz/LYsNc59Khf07rtlgBM/GAMt17/dw7bayceeeAu7rv9ZgYNuCfJ6qdKi81asWnLVvxi1z0AOLDH0Xw8aTybNGvO/K/mAjD/q7k0bdYMgKFPDGT/Q49EEm3abU2rLdoy/bNPEqt/dRG5UxHlTEf0IisVIall1rZjCLPPAwwFTpS0nqQtgfbAexWpf6EH4TqS+kuaLOllSQ0knSVptKTxkoZI2kDSXoSfEDfFbiRbx2W4pLGS3pC0HYCk4yVNise/HstOk/SspFclfSqpb6YCkk6S9F487z2ZO6CSDpb0tqT3JT0haaNYvoekt+L535PUMJ5q81ifTyXdWK2fYiVZvnwZy5YuWbP+9huvsE2H7df8HF69ejX9+93E8SedCYRW84tvTeLFtybR+4zf8dvzLqLXaeckVv+0abZpCzZr2Yrpn30KwHujXmWr9tux74E9eG7IowA8N+RRuh/0KwA2a7UF7416FYCF8+cx/fNPadVmy0TqXq3KSEXkG4MlbQgcBDyVVXyjpImSJgD7AX8EMLPJwOPAh8Bw4FwzW1WR6hd61r490MvMzpL0OHAc8JSZ9QeQdC1wppndJmko8LyZPRm3jQT6mNmnkroAdwL7A38HDjGzWZIaZ12rM7AToWvKaEkvAMuAE4BuZrZC0p1Ab0nDgL8CB5rZMkmXAn+SdAMwGDjBzEZL2hj4Lp6/I7Ar8AMwRdJtZpad2C94i+bP449n9wZg5cqV9Dj6eLp1P4hH7r+Txx7uD8ABhx7J0T1PSrKaNcqlV93E5Rf8lpUrfqRVm3ZcdfOdrF69mkt/fxrPDH6Ylq3acOOdAwA46w+X0PfPfTj+4K6YGRdcdhVNmm6S7BuoBpU1x5yZLQM2KVZ2co79rwOuW9frFnoQnmZm4+L6WKAdsFMMvo2BjYCXih8UW6V7AU9kDfa8Xvw7ChgQg3r2N94IM1sYj38K2BtYSegzODqepwEwD+hKSH2MiuX1gbeBDsAcMxsNYGaL4/kARprZt/H1h0Bb1r67SrxZcDZAy1bZ6abC0Lrtljzx0ls/K+995u/pfebvcx77uz9dXlXVqtE67Lgzjz7/2s/K7xn03M/KNm3RkrsGPlsd1So4KX5gruCD8A9Z66sIQXAAcLSZjZd0GtC9hOOKgG9ix+u1mFmf2DL+FTBW0u6ZTcV3JfzbPmRmf8neIOkIQtDuVaz8F+V4Lz/77OPNgnsBdtx5t+L1cc6VJsVRuNBzwiVpCMyRVA/onVW+JG7LtECnSToeQMEucX1rM3vXzP4OzOenO5wHSWoqqQHhJt8oYCTwa0mbxmObSmoLvAN0k7RNLN9Q0rbAFKClpD1ieUNJhf5F51zqFUk5l0KWxiD8N+BdQpD8OKv8MeBiSR9I2vnTPFAAABL6SURBVJoQoM+UNB6YTHjCBcLNu4mx+9tbwPhY/h4wBJgADDGzMWb2ISH3+3JMzI8AWprZfOA0YFAsfxvYzsx+JOSQb4vXHQGsXyWfgnNuDZWxFLKCbaWZ2XTCjbLM65uzNt9Vwv6j+HkXtUNL2O/Y4mUxZzvTzI4uYf/BhJttxctfAfYooXw0IWecbUBcMvscXvw451zFCJ/o0znnklOObmiFyIMwYGYDyGqpOufSJcUx2IOwcy7t5OkI55xLUopjsAdh51y6hRtzSdei4jwIO+dSrwIDtxcMD8LOudTzlrBzziXFu6g551yyPB3hnHMJEVCU3hjsQdg5VwOkOAincQAf55xbS2VMeS9pehzca5ykMbGsqaQRcUacEYqzLceRGftJmqowE/NuFa27B2HnXOoVKfdSDvuZWUcz6xRfX0aYkKE9YWjbzNThhxFm/mlPmIjhZ4OK5V33ih7onHMFo+rGsjwKeCiuP0QYazxT/rAF7wCNi00KmjcPws65VAtxtsx0RDNJY7KWs0s4lRHGDh+btb2Fmc2J63OBFnG9FWtPTzYzlpWb35hzzqVbfimHBVkphtLsHScA3hQYISl70gjMzCRV+rRj3hJ2zqVfJaQjzGxW/DsPeJowA/tXmTRD/Dsv7j6Ln6ZGA2gdy8rNg7BzLuVyzy+XzxxzcZ7Ihpl14GBgEjAUODXudiqQmc56KHBK7CXRFfg2K21RLp6OcM6lWiXNI9cCeDqOS1wXeNTMhksaDTwu6UzgC6Bn3H8Y0AOYCiwHTq/ohT0IO+fSbx2jsJl9DuxSQvlC4IASyg04d92uGngQds6lXqFPa5+LB2HnXOqlNwR7EHbOpZ18ynvnnEuMT2/knHMJS3EM9iDsnEs/vzHnnHNJSm8M9iDsnEs3lX+4yoLiQdg5l3o+x5xzziUpvTHYg7BzLv08HeGcc4nJfx65QuRB2DmXav6whnPOJcyDsHPOJcjTEc45l5C09xP26Y2cc+m3jnPMSdpC0v8kfShpsqQLYvmVkmZJGheXHlnH/EXSVElTJB1S0ap7S9g5l3qVkI5YCfzZzN6Pc82NlTQibrvFzG5e63rSDsCJwI7A5sB/JW1rZqvKe2FvCTvnUq9IuZeymNkcM3s/ri8BPgJa5TjkKOAxM/vBzKYR5prrXKG6V+Qg55wrKGWnI5pJGpO1nF3qqaR2wK7Au7HoPEkTJD0gqUksawXMyDpsJrmDdqk8CDvnUk2Qz5T3C8ysU9Zyb4nnkjYChgAXmtli4C5ga6AjMAf4V2XX33PCBerDiR8s2KXNxl8kXY8KaAYsSLoStUhaP++2lXWi998f+1KDempWxm5lfkaS6hEC8CNm9hSAmX2Vtb0/8Hx8OQvYIuvw1rGs3BRmbnauckgaY2adkq5HbeGfd+VQmKTuIWCRmV2YVd7SzObE9T8CXczsREk7Ao8S8sCbAyOB9hW5MectYeecg27AycBESeNi2eVAL0kdAQOmA+cAmNlkSY8DHxJ6VpxbkQAM3hJ2lcxbZtXLP+/08xtzrrKVeMPDVRn/vFPOW8LOOZcgbwk751yCPAg751yCPAg751yCPAg751yCPAi7giWpTvy7maQGSdenppFUVOx1ikflTS8Pwq7gSNpSUjczWyXpCOANoJ+k65KuW00gaQMAM1staXdJx0la37yrVCK8i5orOJJ6AXcAZwP7A88C3wDnAwvN7IIEq5dqkhoDfYFngB8Jj+rOBr4D/gaMM7OVydWw9vGWsCs4ZjYIOA+4BWhgZi8BY4FrgaaS7kmyfim3IWE0sBMIj+UeZWbdgQ+APwAdJflwBtXIg7ArGJmcpKT2ZvYocCGwv6TusXX2CXAD0DjObODKQZLMbBYwkDBo+TZAFwAzuxz4ErgM2C2xStZCHoRdwTAzk3Qk0F9SRzMbAlwJ3CdpXzNbTQgeZ5jZh0nWNW1iADZJBxKGXXwM6A90k3QYgJn9FfgM+CG5mtY+nhN2BSO2bv8DnG1mY7PKTwFuAnqZ2StJ1S/tYrC9BbjAzF6StAVhmp4dgWFm9lyiFaylPPfjCkkj4MtMAJZUz8xWmNnDklYShhN0FRB7RFwI/M7M/hdbxjMkPQesBxwj6R3CDBT+OVcjD8IuMVk/kYtiqmE28L2k7YFPzWyFpH2AXc3s39nHJFnvlKoD1Cd8xhAC7/fA18CDwMZmNj+hutVqnhN2icgKwIcD10n6F6HL1DzgXKCPpKMIAWJy5jgPwPnJusnZVtJ6cQbhl4AbJDUxs+/jF9xwADObnlxtazdvCbtExAC8H3A1cCLwIiHdcAlwBmFyxT2A88zsv4lVNKXi59sDuAJ4TdKmQD9gY2CUpAeBU4HLzWxRglWt9fzGnEuMpCuBNwnB91rgN2Y2LWt7AzP7LqHqpVq8yfkocCThl8VuwHFmtljSCYRfHQvM7A1P8STLW8IuSXMIT8W1BE4ys2mSTgfamNlVeFepcssKqOsTgvA2QHegdwzAnYCnzGxF5hgPwMnynLCrFlk5yq6SDpC0O/AysDNwH/BFLPsT8C6EsQ2Sqm/aZA2+k2lYfQn8hvBY8qFmNjX2Ef4L0CSBKrpSeDrCVRtJhxD6qd4E3A90AtoAZxJavS2Am8xsqP9Ezl/WTc6DgJ7A+8BUoDkhHfEqYabgG4C+ZvZsQlV1JfB0hKtysZXWFLgAOBrYgtDjYa6ZvS/pf4QuVA3N7AsPwOUTA/D+wK2EvsBXEMaCuJnQJe1CQsv4r2b2vH++hcVbwq7aSPo7sBT4NXCamX0i6TfARDObmGzt0iuOu3we8B6wErgHONLMZkrawMyWZ+3rAbjAeEvYVYmsn8gtgCUxEDQltNKax5tEuwEXA2clWde0i+Muf00YC+IHoIeZzY1jMbeSdF9meEoPwIXHg7CrElkPYtwIfCBppZmdKmlr4CFJ0wl37a80szEJVjV1sr7gdgW2JNzInACMBqbHANyZkAP+s48PXNg8HeGqhKQdCbnIQYQAcTewgZn1iE/CFQFzzOwd/4lcfvEm3J2EUeUMeI3Q93croBuwArjRzIYmVkmXFw/CrtJJ2gQYD0wkPCCwPJY/DzxhZg8lWb+0i2Nr/Bu41Mw+iF9quwOjzew5SW2B78xsnn/BFT7vJ+wqRVY/4HZmthDoA7QHDsra7V1gowSql3pZ/YAB9iMMP7kPQOxythw4Jb7+wszmxXUPwAXOc8JunWXlKI8E/izpvNgVan3gVkl7AGMIYxWcm2hlUyjr8z0AWEgYcxmgs6Tj4uD3rwF7StrYzBYnVllXbh6E3TqLAWJP4CrC+A8fSWpkZk9KmgMMJvQNPiJu85/I5ZD1BXc9cLGZjZM0hJAL/lvctjXwTw/A6eNB2FWWZoTW7ubxybgeklYRup+dTXiQoC3hRpIrB0nNgEuBY2Lf6p2BTYCnCA+5dAMG+8wY6eRB2FVI1k/kZoSfyJ8AXxGGS7yRMERld6C9mQ2T1BS4XtKbZrY0qXqnVB3CAOyHSrqMkFffB7iIMDbEj8B+kj41s+HJVdNVhPeOcBUWfwafDswk9FF9HlhhZkvigxgDgbPMbFTcv2EcXNzlkPUFtwsh+M4n9H44AnjBwvxwPYH9zayPpDbAAcBwM5uTXM1dRXgQdhUSh0TsDxwG3AWIMGqXAbsQZsS4JHaZKjKz1Z4Lzp/CpJw3AgMIA93vaWafx237AbcTHsQYHsvqmNmqhKrr1oGnI1xeSgigLQhDUO5AGA+4l5ktj62y+cDxZjYpHrcavLtUPmJXtFaEx7uPJIw0NwdYGre1BP5K6CM8PPPv4gE4vbwl7MoUu5r1MLOn4k/kbYDPCA8MNInbZko6BjgcOD970BiXm6R6QF0z+y5+1vUJI859ThiY59R4Q+4owhjMDcxskf+yqBm8JezysQJoI2lKXD+ScDNuIvAtsIOkdoQuald4AM6fpLrA/sCy+KTb3oT0w8GEKYmamNmPkroAlwFTzOxj8F8WNYW3hF1e4mAxzwLzzWz3rLJfEp7gWgEMNB+QvdziWMDXAZsBF5nZEEmbEWZHfpvQ8+RkwmBHPiB7DeNB2JUqO5jGn8ytCY8jdyHkfOdL2sLMZmTGrfUAnL9in+8Awud7C/CBmc2W1JAw3dMC4CMze8U/35rHg7ArUVY3qV8BewKrzKyvpCLg/wg3jP5BeAz5HDObmWB1Uyfr820NzALWI6QizgCGmdlASc2BemY2O8m6uqrlA/i4EsUA0YMQaIcAp0p6EmhkZhcSxiq4FLjTA3D5ZX3BPUH4jM8DXieMC3GYpJuAjwmPe7sazFvCrkSSGhD6Ad8MbA5cTpiaaD3C47PfSGoc//pP5HKStDdhPOBjCCmHrsAbhC+2HYBdgS/MbGRilXTVwoOwWyPzUEXW60bApoTW2X6xC9U3wAuEblM+Y0M5ZD9QEbubfQK0A64F+hLG2PgSuMrM5mcd519yNZh3UXOZVu9KM1shqRvhgYBpZjZWUmPCwwJbSNqQMGjMAx6A85d5XNvCXHD7EQLvZMLneg5whpmNl/RroDHhi29NEPYAXLN5EK7lFGbBuBgYGoPxQ4Q85X2STorjAk8FriGM1nWGmb3prbP8SNoAeEFSP8JsI3cAHxJuwk0m3PScJak+sD1wpplNTqq+rvp5OqKWi13PbiSM1FUEPG1mI+PTbw8Bh5vZ65J2IMwR55NyllP8LC8DFgGXxVbvbwgt4s0Jfa0/AwaZ2ROJVdQlwoNwLZY1sE49wngE+xF6Qtwb87/HAk8CR5tPGLlOFCbmfBz4h5ndFJ+UOwHoQBgp7W5/FLl28i5qtVgMwEVmtoJwc2gEYVyIPSTVN7OngJ7AD0nWsyYwsxGEYT9Pk9Qr5tQfA6YQfn0sivt5AK5lvCVcSxV7Wquuma2Mecm/Aw2BocAbZvZj8f1dxcW+19cA/cxnnXZ4S7jWicMhQta/fQzA9WLAvZowU8NxZM2M7AG4cpjZMMJAR5dK2jw+gehqMW8J1yJZj8oeSBgQ5nPgMzMbGLfXi93U6gPtzOyTJOtbk0lqnt0X2NVe/i1ci8QAvC9wG/AqYcyCcyX9OW5fEXPEP3oArloegF2G9xOufVoD/c3sQQBJ7wI3SRpuZpOzn5hzzlU9bwnXcFk54IwGwElZrycTZkn2vJRzCfAgXMNlUhCSfi9pBzO7D3hX0kiFaeg7ATsD9ZKtqXO1k9+Yq6GybsJ1AR4gPCq7HHgTeITwlFw7YBPgen8Yw7lkeBCuwSR1JnQ5u8TMJkjqRRgycYKZ3R+7RzX2J7WcS46nI2q2xsCBwEHx9RPAKKCrpAsAAV+D9wN2LineO6IGM7OX4/gP10uabWaD4uwYdYDxmbFtnXPJ8SBcw1mY/XglcE0cD+IhYFDS9XLOBZ4TriUkHQncQEhPzPX+wM4VBg/CtYg/Kutc4fEg7JxzCfLeEc45lyAPws45lyAPws45lyAPws45lyAPwi4RklZJGidpkqQn4tTwFT3XAEm/juv3xZmhS9u3u6S9KnCN6ZKa5VtebJ+l5bzWlZIuKm8dXTp5EHZJ+c7MOprZToTplPpkb4yzEZebmf3WzD7MsUt3oNxB2Lmq4kHYFYI3gG1iK/UNSUOBDyXVkXSTpNGSJkg6B8IIcZJulzRF0n+BTTMnkvSqpE5x/VBJ70saH4fubEcI9n+MrfBfSmouaUi8xmhJ3eKxm0h6WdJkSfcRxtnISdIzksbGY84utu2WWD5SUvNYtrWk4fGYNyRtVxkfpksXf2zZJSq2eA8Dhsei3YCdzGxaDGTfmtkektYDRkl6GdgV6ADsALQgDNP5QLHzNgf6A/vEczWNo8XdDSw1s5vjfo8Ct5jZm5LaAC8B2wN9gTfN7GpJvwLOzOPtnBGv0QAYLWmImS0ENgTGmNkfJf09nvs84F6gj5l9GoccvRPYvwIfo0sxD8IuKQ0kjYvrbwD3E9IE75nZtFh+MLBzJt8LNALaA/sAg+IARLMlvVLC+bsCr2fOZWaLSqnHgcAOWROQbCxpo3iNY+OxL0j6Oo/39AdJx8T1LWJdFwKrgcGxfCDwVLzGXsATWddeL49ruBrGg7BLyndm1jG7IAajZdlFwPlm9lKx/XpUYj2KgK5m9n0JdcmbpO6EgL6nmS2X9Cqwfim7W7zuN8U/A1f7eE7YFbKXgN9JqgcgaVtJGwKvAyfEnHFLYL8Sjn0H2EfSlvHYprF8CdAwa7+XgfMzLyRlguLrwG9i2WFAkzLq2gj4Ogbg7Qgt8YwiINOa/w0hzbEYmCbp+HgNSdqljGu4GsiDsCtk9xHyve9LmgTcQ/j19jTwadz2MPB28QPjQEVnE376j+endMBzwDGZG3PAH4BO8cbfh/zUS+MqQhCfTEhLfFlGXYcDdSV9RBit7p2sbcuAzvE97E+Y7QSgN3BmrN9k4Kg8PhNXw/gAPs45lyBvCTvnXII8CDvnXII8CDvnXII8CDvnXII8CDvnXII8CDvnXII8CDvnXIL+HwmCTgN72UeMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}