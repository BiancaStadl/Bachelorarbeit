{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "86df5507-360a-45d2-a793-735e37bf957b"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199a5df1-4285-4ed3-ca51-37f4357b04bb"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.825963585000409\n",
            "GPU (s):\n",
            "0.04902149999998073\n",
            "GPU speedup over CPU: 78x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57be3cc3-c1ca-4c4f-9a68-5914e9979525"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"ğŸ˜œ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ«\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ğŸ–\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜‡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¬\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜ƒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’™\", \" \",string)  \n",
        "   string = re.sub(\"ğŸ˜›\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ–•\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜‰\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’©\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¢\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¨\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤£\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜ˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’ƒğŸ½\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘¹\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤˜\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜±\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤”\", \" \",string) \n",
        "   string = re.sub(\"ğŸŒˆ\", \" \",string) \n",
        "   string = re.sub(\"ğŸ’•\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘©â€â¤ï¸â€ğŸ‘©\", \" \",string) \n",
        "   string = re.sub(\"ğŸ˜\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘†\", \" \",string) \n",
        "   string = re.sub(\"ğŸ˜–\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘‡\", \" \",string) \n",
        "   string = re.sub(\"ğŸ”¥\", \" \",string) \n",
        "   string = re.sub(\"ğŸ˜˜\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‰\", \" \",string) \n",
        "   string = re.sub(\"ğŸ¤¬\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‡©ğŸ‡ª\", \" \",string)  \n",
        "   string = re.sub(\"ğŸ’”\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™ˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¯\", \" \",string)\n",
        "   string = re.sub(\"ğŸŸ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ›¶\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜“\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜³\", \" \",string)\n",
        "   string = re.sub(\"ğŸš€\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¸\", \" \",string)\n",
        "   string = re.sub(\"ğŸ“ˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜…\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜†\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™ğŸ¿\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘ğŸ½\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤­\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¤\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜²\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤®\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™„\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ…\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘‹\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’ª\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜„\", \" \",string)\n",
        "   string = re.sub(\"ğŸ§\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜ \", \" \",string)\n",
        "   string = re.sub(\"ğŸˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸš‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸš‡\", \" \",string)\n",
        "   string = re.sub(\"ğŸšŠ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤·\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¥\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™ƒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ”©\", \" \",string)\n",
        "   string = re.sub(\"ğŸ”§\", \" \",string)\n",
        "   string = re.sub(\"ğŸ”¨\", \" \",string)\n",
        "   string = re.sub(\"ğŸ› \", \" \",string)\n",
        "   string = re.sub(\"ğŸ’“\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’¡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¸\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¥ƒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¥‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜·\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤\", \" \",string)\n",
        "   string = re.sub(\"ğŸŒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤›\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜€\", \" \",string)\n",
        "   string = re.sub(\"ğŸ›¤\", \" \",string)\n",
        "   string = re.sub(\"ğŸ„\", \" \",string)\n",
        "   string = re.sub(\"ğŸ“´\", \" \",string)\n",
        "   string = re.sub(\"ğŸŒ­\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤•\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜­\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¾\", \" \",string)\n",
        "   string = re.sub(\"ğŸ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¦\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¯\", \" \",string)\n",
        "   string = re.sub(\"ğŸ•¯ï¸\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19853ccd-e7fa-44d9-ff96-5eb8c83d1de3"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "#X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(80, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "#X= tf.keras.layers.LSTM(150, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(90, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1420 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1420.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1420.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07696e7e-37f7-4739-8d43-2d8481e0f72d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 160)      543360      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 160)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 90)           14490       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 90)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            91          ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,292,021\n",
            "Trainable params: 135,292,021\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 7\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f88f0f1b-7dda-4eab-c38b-66ad96f7ddd5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1420.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7aeb61-c78a-432d-ee7e-4303e54c94ba"
      },
      "source": [
        "model1420.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =7,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "157/157 [==============================] - 200s 1s/step - loss: 0.6046 - binary_accuracy: 0.6776 - metrics_recall: 0.1686 - metrics_precision: 0.4137 - metrics_f1: 0.2111\n",
            "Epoch 2/7\n",
            "157/157 [==============================] - 182s 1s/step - loss: 0.4933 - binary_accuracy: 0.7570 - metrics_recall: 0.5659 - metrics_precision: 0.6889 - metrics_f1: 0.5907\n",
            "Epoch 3/7\n",
            "157/157 [==============================] - 181s 1s/step - loss: 0.4104 - binary_accuracy: 0.8137 - metrics_recall: 0.6912 - metrics_precision: 0.7494 - metrics_f1: 0.7001\n",
            "Epoch 4/7\n",
            "157/157 [==============================] - 182s 1s/step - loss: 0.3407 - binary_accuracy: 0.8509 - metrics_recall: 0.7612 - metrics_precision: 0.7951 - metrics_f1: 0.7640\n",
            "Epoch 5/7\n",
            "157/157 [==============================] - 181s 1s/step - loss: 0.2835 - binary_accuracy: 0.8862 - metrics_recall: 0.8181 - metrics_precision: 0.8443 - metrics_f1: 0.8199\n",
            "Epoch 6/7\n",
            "157/157 [==============================] - 180s 1s/step - loss: 0.2500 - binary_accuracy: 0.9020 - metrics_recall: 0.8434 - metrics_precision: 0.8629 - metrics_f1: 0.8440\n",
            "Epoch 7/7\n",
            "157/157 [==============================] - 179s 1s/step - loss: 0.2155 - binary_accuracy: 0.9173 - metrics_recall: 0.8722 - metrics_precision: 0.8823 - metrics_f1: 0.8712\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f47874b70d0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1420.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570c7292-fce1-4951-a158-832649960f16"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc65ac2-c4c2-49b3-ee8a-5235e835173b"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03230936],\n",
              "       [0.3065918 ],\n",
              "       [0.8901024 ],\n",
              "       ...,\n",
              "       [0.93575704],\n",
              "       [0.07602257],\n",
              "       [0.07358016]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db77cdf1-c8bc-428e-f7d9-224e9e31ea9f"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7593431483578709"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7e3d536b-fd9f-4369-aff8-f6cb7796e78a"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2056  274]\n",
            " [ 576  626]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}