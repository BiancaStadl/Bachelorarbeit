{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOA8Tmp2Nh/3kemmIQygaPu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow"
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "40fd7124-4dcf-43d5-c4a3-64199f7d1197"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1a650a-d89a-4aca-a9a5-87b402448672"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.7584264879997136\n",
            "GPU (s):\n",
            "0.044662187000085396\n",
            "GPU speedup over CPU: 84x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c8bdea-6ef4-4d7a-d18a-cece12dc3354"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb67f23d-d53b-4614-914f-c733a3582268"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(90, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model11 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model11.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_layer_norm', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model11.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d83ee3f7-c469-4ad6-fe3c-616ff5262230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 320)      1189120     ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 320)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 90)           28890       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 90)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            91          ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,952,181\n",
            "Trainable params: 135,952,181\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 6\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "# init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a11ef03-c03a-4ef1-d04a-5ae4208bddbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model11.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d8a8d6b-5dd4-4161-e934-c28ea6df77b1"
      },
      "source": [
        "model11.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs = 6,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "157/157 [==============================] - 193s 1s/step - loss: 0.6114 - binary_accuracy: 0.6550 - metrics_recall: 0.3176 - metrics_precision: 0.4442 - metrics_f1: 0.3073\n",
            "Epoch 2/6\n",
            "157/157 [==============================] - 177s 1s/step - loss: 0.4804 - binary_accuracy: 0.7676 - metrics_recall: 0.5877 - metrics_precision: 0.6953 - metrics_f1: 0.6145\n",
            "Epoch 3/6\n",
            "157/157 [==============================] - 177s 1s/step - loss: 0.3959 - binary_accuracy: 0.8195 - metrics_recall: 0.6959 - metrics_precision: 0.7580 - metrics_f1: 0.7083\n",
            "Epoch 4/6\n",
            "157/157 [==============================] - 177s 1s/step - loss: 0.3333 - binary_accuracy: 0.8563 - metrics_recall: 0.7725 - metrics_precision: 0.8003 - metrics_f1: 0.7735\n",
            "Epoch 5/6\n",
            "157/157 [==============================] - 177s 1s/step - loss: 0.2802 - binary_accuracy: 0.8844 - metrics_recall: 0.8180 - metrics_precision: 0.8384 - metrics_f1: 0.8187\n",
            "Epoch 6/6\n",
            "157/157 [==============================] - 177s 1s/step - loss: 0.2476 - binary_accuracy: 0.9058 - metrics_recall: 0.8417 - metrics_precision: 0.8783 - metrics_f1: 0.8523\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fab58cffb50>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model11.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dab5152-28c7-4c06-dddf-092a67b80432"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20b2753-d8ec-46fa-b847-c6121cca832d"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03052118],\n",
              "       [0.68034583],\n",
              "       [0.9204246 ],\n",
              "       ...,\n",
              "       [0.9290142 ],\n",
              "       [0.19463202],\n",
              "       [0.02362558]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3495c1c-7c25-420f-86cc-5f10776ecada"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7664212910532276"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "edf28786-cd4a-4334-a222-8b37f7a761b0"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2117  213]\n",
            " [ 612  590]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEmCAYAAACzoiEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5xVxfnH8c93aRZQQBARVCxYEBUBBYNRsYu9oaixxpJY4i/RaDSJJZoY0ahYI2rs2LCgIkgwKmKhiQgqigIKgohYkCbl+f0xc/G47t69W8+e3eed133tvXPnnDN7ic+dnTPzjMwM55xz6ShKuwHOOVefeRB2zrkUeRB2zrkUeRB2zrkUeRB2zrkUeRB2zrkUeRB2NU7SvZKuis9/KWlqAcdcIumu+LyDJJPUML5+WdKvq7fV1aes9ku6Q9JfarJNruY0TLsBrn4zs1HAVgXU+3sNNAcIQRF40MzuqqlrJq59MvBrM9s1V2ZmZ9V0O1zN8SDsXCRJgNJuh6tffDjCVTtJO0qaIGmhpEeBNRLv7SFpVuL1RZJmx7pTJe0Vyy+X9GCey2wuaYyk7yQ9I6ll4pw9Jb0u6RtJ70jaI/Hey5KuljQaWAw8APwSuEXS95JuKeH3yQ2HnCLpM0lfSzpL0k6SJsXr3JKo/5O2Fx9OSZRvA9wB7BKv/U0sXz184+oeD8KuWklqDDxNCG4tgceBI0upuxVwDrCTmTUD9gNmFHipE4FTgbbACmBAPGc74Hngqnj9C4DBklonjv0VcAbQDDgZGAWcY2ZNzeycPNfsAXQEjgFuBC4F9ga2BfpK2r3AtgNgZu8DZwFvxGs3L8/xLps8CLvq1hNoBNxoZsvN7AlgbCl1VwJNgE6SGpnZDDP7uMDrPGBmk81sEfAXQhBsAJwADDWzoWa2ysxGAOOAPolj7zWzKWa2wsyWl+N3+5uZLTWzF4FFwCAzm2dmswmBfMdynMvVUx6EXXXbEJhtP80UNbOkimY2DTgfuByYJ+kRSRsWeJ3Pip2/EdAK2AQ4Og4RfBP/xN+V0GMu6djy+CLxfEkJr5tW8LyuHvEg7KrbHKBdvOmVs3Fplc3s4TgzYBPAgH8WeJ2Nip1/OTCfEGAfMLPmicfaZnZN8rLFm1HgNQu1CFgr8XqDPHU9rWE940HYVbc3CGO050lqJOkIYOeSKkraStKekpoASwm9yVUFXucESZ0krQVcCTxhZiuBB4GDJe0nqYGkNeLNwPZ5zvUFsFmB1y3ERGA3SRtLWhf4UxnXbh/H0l094EHYVSsz+wE4gnDDawHhJtaTpVRvAlxD6MHOBdYnf8BKegC4Nx63BnBevP5nwKHAJcCXhJ7xheT///5NwFFx1sOAAq9fqjgO/SgwCRgPPJen+kvAFGCupPmVvbar/eRJ3Z1zLj3eE3bOuRR5EHbOuRR5EHbOuRR5EHbOuRR5Ap9aSg3XNDVulnYz6o0dtyl16rKrBjNnzmD+/PlVkiypwTqbmK1YkreOLflyuJntXxXXq2oehGspNW5Gk636pt2MemP0Wz/L0+OqUa8e3avsXLZiSZn/rSydeGurKrtgFfMg7JzLNgmKGqTdigrzIOycyz5l9/aWB2HnXMZ5T9g559Kl7G6I4kHYOZdtwocjnHMuPT4c4Zxz6crwcER2+/DOOQeAwnBEvkdZZ5A2kvQ/Se9JmiLpd7G8paQRkj6KP1vEckkaIGla3Ny1a+JcJ8X6H0k6qaxrexB2zmWbCMMR+R5lWwH8wcw6EfZFPFtSJ+BiYKSZdQRGxtcABxA2ee1I2CT2dghBG7iMsAnszsBlucBdGg/CzrmMq3xP2MzmmNmE+Hwh8D7QjrAhwH2x2n3AYfH5ocD9FrwJNJfUlrBD+AgzW2BmXwMjgLzLpX1M2DmXbQIalNnbbSVpXOL1nWZ2Z4mnkzoQdsp+C2hjZnPiW3OBNvF5O366QeysWFZaeak8CDvnsq/sG3PzzazMhBWSmgKDgfPN7Lvk/rRmZpKqfCsiH45wzmVc5YcjACQ1IgTgh8wstw/iF3GYgfhzXiyfzU93+G4fy0orL5UHYedc9lXyxpxCl/du4H0z+1firSFAbobDScAzifIT4yyJnsC3cdhiOLCvpBbxhty+saxUPhzhnMs2qSrmCfcCfgW8K2liLLuEsPv3Y5JOA2YCuZyZQ4E+wDRgMXAKgJktkPQ3YGysd6WZLch3YQ/Czrnsq+SKOTN7jXCLryR7lVDfgLNLOdc9wD2FXtuDsHMu4+S5I5xzLlUZXrbsQdg5l20SFGU3lGW35c45l+M9YeecS5GnsnTOuZTIb8w551y6fDjCOefSIaCoyHvCzjmXDlH6MosM8CDsnMs4IR+OcM659PhwhHPOpch7ws45lxJJqMiDsHPOpcZ7ws45l6IsB+HsjmY75xzETJbK+yjzFNI9kuZJmpwoe1TSxPiYkUv2LqmDpCWJ9+5IHNNN0ruSpkkaoAK+Hbwn7JzLvCroCd8L3ALcnysws2MS578e+DZR/2Mz61LCeW4HTifs1DyUsN39C/ku7D1h51ymCVFUVJT3URYzexUocRui2JvtCwzK246wEeg6ZvZm3HnjfuCwsq7tQdg5l30q4wGtJI1LPM4ox9l/CXxhZh8lyjaV9LakVyT9Mpa1A2Yl6syKZXn5cIRzLttU0HDEfDPrXsEr9OOnveA5wMZm9pWkbsDTkrat4Lk9CDvnsq+6VsxJaggcAXTLlZnZMmBZfD5e0sfAlsBsoH3i8PaxLC8fjnB5tW/TnGF3nseEwZcy/olLObvfHgAcsfeOjH/iUhaNH0DXThuvrt9y3bUZdud5fDn6em646OjV5U3XasKbj1y8+vHZS9fQ/4Ija/rXyZTPPvuM/fbuzY7bd6LrDttyy4CbABj8xON03WFb1mpcxPhx41bXHztmDD26daFHty7s3HUHnnn6qbSaXqMUc0fke1TC3sAHZrZ6mEFSa0kN4vPNgI7AJ2Y2B/hOUs84jnwi8ExZF/CesMtrxcpVXPyvJ5n4wSyartWE1x++iJFvfcCUjz/n2D8M5JY/9/tJ/aXLlnPlbc/RaYsN2XbztqvLv1+8jJ7HXrP69eiH/sjTL02ssd8jixo2bMg1117Pjl27snDhQn7Roxt77b0P227bmUcee5JzfnvmT+pv27kzo98aR8OGDZkzZw49uu3AgQcdTMOGdfw/8zhFrVKnkAYBexDGjmcBl5nZ3cCx/PyG3G7AlZKWA6uAs8wsd1Pvt4SZFmsSZkXknRkBHoRdGebO/465878DQiD9YPpcNmzdnJfe+qDE+ouX/sDrEz9hs41al3rOLTZen/VbNmP0hI+rpc11Rdu2bWnbNnyRNWvWjK233obPP5/NXnvvU2L9tdZaa/XzZUuXZnoBQ3lV9nc1s36llJ9cQtlgYHAp9ccBnctzbR+OcAXbuG1LumzVnrGTZ1TqPEfv35UnXpxQNY2qJ2bOmMHEiW+z08498tYb89ZbdN1hW7rvuB0Dbr2j7veCo2ocjqh2tTIIS7pX0lHlqN9c0m+rs02VEVfbtEq7HZWx9pqNGXTdr7nwusEsXLS0Uuc6er9uPDZsXNkVHQDff/89/foeSf/rb2SdddbJW3fnHj2Y8M4UXntjLP3/+Q+WLq3cv1VWVHbFXJpqZRCugOaEsRhXDRo2LGLQdafz6AvjeOaldyp1ru22bEfDBg14+/3Pqqh1ddvy5cvp1/dIjul3PIcdfkTBx229zTY0bdqUKZMnl10548rqBdfLnnBcW/2+pIGSpkh6UdKa8b0ukt6UNEnSU5JalHKa3SS9LumTXK9YUlNJIyVNiOuzD411rwE2j+u4+8e6F0oaG69zRSxbW9Lzkt6RNFnSMbF8hqRr4znHSNoilreWNDieZ6ykXonz3BPrvp1rh6QGkq6L554k6dzE73Nuot1bV+0nXr3uuOx4pk6fy4AHX6r0ufru773gQpkZZ51+GlttvQ2/+7/fl1l/xvTprFixAoCZM2cydeoHbNKhQzW3snbIchCuzgGjjkA/Mztd0mPAkcCDhKV855rZK5KuBC4Dzi/h+LbArsDWwBDgCWApcLiZfRf/vH9T0hDgYqBzbi23pH3j9XcmrJcZImk3oDXwuZkdGOutm7jet2a2naQTgRuBg4CbgBvM7DVJGwPDgW2AS4GXzOxUSc2BMZL+S5iS0gHoYmYrJLVMnH++mXWNwyYXAL8u/gsrrOIJK3kaNS3kM652v+iyGccf1IN3P5zNm49cDMBltwyhSaOG/Ouio2nVoilPDjiLSVNnc8jZtwLwwfNX0GztNWjcqCEH996eg357Kx98MheAI/fpymHn3p7a75Mlr48ezcMPPUDnztvRo1tIU3DFVX9n2bJl/P78c5n/5ZccceiBbL9DF54dOpzXR7/Gdf2voVHDRhQVFXHTzbfRqlWmR8EKVtuHHPJRWOJcxSeVOgAjzKxjfH0R0Ai4GXjXzDaO5ZsDj5tZ12LH3xuPfyi+XmhmzSQ1Am4gTBFZBWwFbAqsATxnZp1j/euAo4Bv4imbAv8ARgEvAo/G+qNi/RnAnmb2SbzGXDNbT9I84PNE01rHa74cr7kilrcE9gOuAu4wsxHFfp8ZQC8zmy2pB3C1me2d7zMsWmt9a7JV33xVXBX6euwtaTehXunVozvjx4+rksjZpE1Ha3f8TXnrTL/hwPGVWDFXraqzJ7ws8XwlYd5cRY/P/WMdTwiE3cxseQxua5RwrIB/mNm/f/aG1BXoA1wlaaSZXRnfSn4b5Z4XAT3NbGmxcwg40symFisv5PdZiU8NdK7KSFCU4Z5wjd6YM7Nvga/1Y8KLXwGvlOMU6wLzYgDuDWwSyxcCzRL1hgOnSmoKIKmdpPUlbQgsNrMHgf5Asgd+TOLnG/H5i8DqcV1JudR1wwljvIrlO8byEcCZCksdKTYc4ZyrFtm+MZdGj+wk4A5JawGfAKeU49iHgGclvQuMAz4AiIk0RiskZH7BzC6UtA3wRvwH+B44AdgC6C9pFbAc+E3i3C0kTSL0WHMTt88Dbo3lDYFXgbOAvxHGjSdJKgKmE8aQ7yKsIZ+ksJpmICFHqXOuGtXyOJtXtYwJZ00c1uhuZvPTbkuOjwnXLB8TrllVOSa8RtstrcNJN+etM/Wf+9fLMWHnnKt2Ittjwh6EATPrkHYbnHMV50HYOefSomyPCXsQds5lmsj2lvcehJ1zGadMD0fUlQQ+zrl6rLLzhGMumHlxmmuu7HJJs2NOmomS+iTe+5OkaZKmStovUb5/LJsm6eJC2u5B2DmXabkVc/keBbgX2L+E8hvMrEt8DA3XUyfCjhvbxmNui8m7GgC3AgcAnYB+sW5ePhzhnMu8yg4Jm9mrMedNIQ4FHokbfk6XNI2QLAxgmpl9EtqkR2Ld9/KdzHvCzrnMK2A4opWkcYnHGQWe+pyYlvYe/Zh2tx2QTIg9K5aVVp6X94Sdc9lWWAKf+RVYMXc7IUWBxZ/XA6eWv4H5eRB2zmVamKJW9ec1sy9WX0MaCDwXX84GNkpUbR/LyFNeKh+OcM5lXPVkUZPUNvHycCA3c2IIcKykJpI2JWwgMQYYC3SUtKmkxoSbd0PKuo73hJ1zmVfZecKSBgF7EMaOZxF2/Nkjpq81YAZwJoCZTVHYLeg9wsYOZ5vZyniecwipbhsA95jZlLKu7UHYOZdtVbBs2cz6lVB8d576VwNXl1A+FBhanmt7EHbOZVrIopbdkVUPws65zMtw6ggPws657PMEPs45lxIp2wl8PAg75zIvwx3h0oOwpJv56TbwP2Fm51VLi5xzrpwa1NGe8Lgaa4VzzlWQVEfHhM3svuRrSWuZ2eLqb5JzzpVPhjvCZS9blrSLpPeAD+LrHSTdVu0tc865AlVBPuHUFDLD+UZgP+ArADN7B9itOhvlnHOFEqAy/lebFTQ7wsw+KzbmsrJ6muOcc+Uk1dkbczmfSfoFYJIaAb8D3q/eZjnnXOEyfF+uoCB8FnATIUP854QMQWdXZ6Occ65QAooyHIXLDMJmNh84vgba4pxzFVLbb77lU8jsiM0kPSvpy7gl9DOSNquJxjnnXFmksh+1WSGzIx4GHgPaAhsCjwODqrNRzjlXHkVS3kdZ4kae8yRNTpT1l/RB3OjzKUnNY3kHSUskTYyPOxLHdJP0rqRpkgaogFUkhQThtczsATNbER8PAmsUcJxzztWIygZh4F5g/2JlI4DOZrY98CHwp8R7H5tZl/g4K1F+O3A6YcujjiWc8+dtL+0NSS0ltQRekHRxjP6bSPoj5cwc75xz1SXcmMv/KIuZvQosKFb2opmtiC/fJGzcWXo7wp5065jZm2ZmwP3AYWVdO9+NufGEBD65X+HMZPv46beCc86lo7BUlq0kJfPh3Glmd5bjKqcCjyZebyrpbeA74M9mNoowg2xWos6sWJZXvtwRm5ajgc45l5oChl7nm1n3Cp77UsKGng/FojnAxmb2laRuwNOStq3IuaHAFXOSOgOdSIwFm9n9Fb2oc85VldxwRLWcWzoZOAjYKw4xYGbLgGXx+XhJHwNbArP56ZBF+1iWV5lBWNJlhK2gOxHGgg8AXiOMdzjnXOqqY7GGpP2BPwK7JzNISmoNLDCzlXG6bkfgEzNbIOk7ST2Bt4ATgZvLbHsBbTkK2AuYa2anADsA65b7N3LOuWogVckUtUHAG8BWkmZJOg24BWgGjCg2FW03YJKkicATwFlmlrup91vgLmAa8DHwQlnXLmQ4YomZrZK0QtI6wDxgowKOc865GlHZFXNm1q+E4rtLqTsYGFzKe+OAzuW5diFBeFycpDyQMGPie8I3hnPO1Qq1fVVcPoXkjvhtfHqHpGGEeXCTqrdZzjlXGFHwgoxaKd9Gn13zvWdmE6qnSQ5g6y3a8dDTf0+7GfXG1M8Xpt2EemXp8lVVdzJlO4FPvp7w9XneM2DPKm6Lc85VSCEzDGqrfIs1etdkQ5xzriJE3d3y3jnnMiHDMdiDsHMu20LO4OxGYQ/CzrnMa5DhQeFCdtaQpBMk/TW+3ljSztXfNOecK1tuj7lK5hNOTSHfH7cBuwC5FSULgVurrUXOOVdORWU8arNChiN6mFnXmDsTM/taUuNqbpdzzhVEUp2fHbFcUgPC3OBcBqEqnGntnHOVU8tHHPIqJAgPAJ4C1pd0NSGr2p+rtVXOOVcgAQ3rck/YzB6SNJ6QzlLAYWb2frW3zDnnClSne8KSNgYWA88my8zs0+psmHPOFaTAzTxrq0JuHD4PPBd/jgQ+oYBExc45VxMENJDyPso8h3SPpHmSJifKWkoaIemj+LNFLJekAZKmSZqUTHYm6aRY/yNJJxXS/jKDsJltZ2bbx58dgZ3xfMLOuVqkslveA/cC+xcruxgYGePeyPgawhZvHePjDOB2CEEbuAzoQYiTl+UCd962F9S8hJjCskd5j3POueqQS+CT71EWM3sVWFCs+FDgvvj8PuCwRPn9FrwJNJfUFtgPGGFmC8zsa2AEPw/sP1PImPDvEy+LgK7A52Ud55xzNULVdmOujZnNic/nAm3i83bAZ4l6s2JZaeV5FTJFrVni+QrC2HCJ+ys551waClia3ErSuMTrO83szkLPb2YmySrUuDLkDcJxkUYzM7ugOi7unHOVFYYjyqw238y6l/PUX0hqa2Zz4nDDvFg+m59udtw+ls0G9ihW/nJZFym16ZIamtlKoFf52u2cczVJFJXxqKAhQG6Gw0nAM4nyE+MsiZ7At3HYYjiwr6QW8YbcvrEsr3w94TGE8d+JkoYAjwOLcm+a2ZPl/IWcc67KSZVPZSlpEKEX20rSLMIsh2uAxySdBswE+sbqQ4E+wDTCGopTAMxsgaS/AWNjvSvNrPjNvp8pZEx4DeArwp5yRuj9G+BB2DlXK1Q2XaWZ9Svlrb1KqGvA2aWc5x7gnvJcO18QXj/OjJjMj8F39bXKcxHnnKsuou4uW24ANIUSB1Q8CDvnao26mspyjpldWWMtcc65ChC1P3F7PvmCcHa/Wpxz9Ucd3ujzZwPSzjlX2+QS+GRVqUG4kKkVzjlXG2Q3BPuW9865zBNFdfTGnHPO1Xp1+cacc85lQl29Meecc7WfKr9iLk0ehJ1zmebDEc45lzLvCTvnXIoyHIM9CDvnsi0MR2Q3CnsQds5lnDI9HJHl8WznnAPCcES+R9nHaytJExOP7ySdL+lySbMT5X0Sx/xJ0jRJUyXtV9G2e0/YOZdpUuVzR5jZVKBLOJ8aEPaLe4qwa8YNZnbdT6+pTsCxwLbAhsB/JW0Zt4QrF+8Ju3JZ+O03XPibX3HEnt05Yq+deGf8GEY8/xRH7dODbps2571JE1bXfXPUSxx30G703W8XjjtoN8a8/kqKLc+mA3p15qh9e9L3gF4cd9DuAEx9711OPGwvjtq3J+ed2pfvF363uv7dt17PwbvtwKG9u/L6K/9Nq9k1rrI94WL2Aj42s5l56hwKPGJmy8xsOmGro50r0nbvCbty6X/Fxfxi973pf/sDLP/hB5YuWUyzddflujse5OpLzv9J3eYt1uOmux+ldZu2TJv6HmefeATD3/ogpZZn18BHnqdFy/VWv77ionP4/aVX073nrjz96APc9++bOPuCv/Dxhx8w/NnBDB4xhi+/mMOZxx/CMy+/TYMGDVJsfc1Q1d6YOxYYlHh9jqQTgXHAH8zsa6Ad8GaizqxYVm7eE3YFW/jdt0wYM5rDjjkRgEaNG9Ns3eZstsVWdNi848/qb915B1q3aQvA5ltuw7KlS/hh2bIabXNd9On0j+nWI2yC3vOXvRn5whAAXh7xPPsdfCSNmzSh3cYd2KjDZkyeOC7NptaIXCrLfA/CBp7jEo8zSjyX1Bg4hLCxMcDtwOaEoYo5wPVV3X4Pwq5gn382kxbrteLyC35Lvz67cuVF57Bk8aKyDwRGvvAMW3fegcZNmlRzK+sWIX5zwmH0O3A3nnj4PwBs1nFr/vfi8wCMeP5p5s6ZDcC8uZ+zQdsfO2NtNmjHvLlzar7RKShgOGK+mXVPPO4s5VQHABPM7AsAM/vCzFaa2SpgID8OOcwGNkoc1z6WlVutDcKSOkiaXI76h8XB8lpH0smSbkm7HZW1cuUKPpj8DkedcBqDhr7GmmuuzX9uv6HM4z7+8H0GXHMZl/79xhpoZd3yn8HDeWToKG69bzCP3T+Q8W+N5or+t/HYAwPpd+BuLFq0kEaNGqXdzNSpjP+VQz8SQxGS2ibeO5yw8THAEOBYSU0kbQp0BMZUpO21NghXwGFArQzCdcX6G7Rj/Q3asd2O3QHYq8+hfDD5nbzHfDFnNn8483iu/Ne/2WiTzWqimXVKmw02BKBlq9b03u8gJk8cz6ZbbMkdDz7DoOdf5YBDjqL9JpsCsP4GG67uFQN8MXc262/QtsTz1iUi/1BEoTMnJK0N7AM8mSi+VtK7kiYBvYH/AzCzKcBjwHvAMODsisyMgNofhBtIGihpiqQXJa0p6XRJYyW9I2mwpLUk/YIwjtM/zuXbPD6GSRovaZSkrQEkHS1pcjz+1Vh2sqRnJL0s6SNJl+UaIOkESWPief8dp68gaV9Jb0iaIOlxSU1j+U6SXo/nHyOpWTzVhrE9H0m6tkY/xSrSav02tNmwHTM+/giAMaNfYdOOW5Vaf+G333DeKX0596LL6dK9Z001s85YsngRi75fuPr5G6++xBZbbcOC+V8CsGrVKgbe3J+jjz8NgN336cPwZwfzw7JlzP50Bp9O/4TOXbqn1v4aU8ZQRKGzI8xskZmtZ2bfJsp+ZWbbmdn2ZnaImc1JvHe1mW1uZluZ2QsVbX5tnx3REehnZqdLegw4EnjSzAYCSLoKOM3MbpY0BHjOzJ6I740EzjKzjyT1AG4D9gT+CuxnZrMlNU9ca2egM7AYGCvpeWARcAzQy8yWS7oNOF7SUODPwN5mtkjSRcDvJV0DPAocY2ZjJa0DLInn7wLsCCwDpkq62cw+q56PrfpcdPm1XHr+r1m+fDntN+rA5dfdykvDnuXay//I1wvmc96pfdlym+247YGnePT+gXw28xMG3nQtA28K3zu3PfAULVu1Tvm3yIav5s/j92ccD8CKFSs44NCj6bXHPjx0z208ev9AAPba/xAO7XsCAFtsuQ37HHg4R+y9Ew0aNuRPf7uunsyMyPYeczKztNtQIkkdgBFm1jG+vghoBIwCrgKaA02B4WZ2lqR7iUE49kq/BKYmTtnEzLaRdAfhbudjhID+laSTgT3N7MR4rSuBBcAK4BJgXjzHmoTxonHAvYRpKQCNgTeAG4E7zKxXsd/lZEIgPz2+fgG42sxeK1bvDOAMgA3abdRt6OiCh8RdJWV52WsWHXfQ7kyZNKFKPvRtttvR/vPU//LW2aVji/FmViv/LKjtPeHkfKaVhCB4L3CYmb0Tg9seJRxXBHxjZl2KvxEDdg/gQGC8pG65t4pXJXzJ3mdmf0q+IelgwhdEv2Ll25Xjd/nZZx/v2N4J0Gn7HWvnt6NztVGGv0Nr+5hwSZoBcyQ1Ao5PlC+M72Fm3wHTJR0NoGCH+HxzM3vLzP5K6C3nppnsI6mlpDUJN/lGAyOBoyStH49tKWkTwiTtXpK2iOVrS9qS0PNuK2mnWN5MUm3/onMu84qkvI/aLItB+C/AW4QgmVx+9QhwoaS3JW1OCNCnSXoHmEJYZgjh5t27cfrb60Du9v4YYDAwCRhsZuPM7D3C2O+L8e7oCKCtmX0JnAwMiuVvAFub2Q+EMeSb43VHAGtUy6fgnFtNZTxqs1rbSzOzGYQbZbnXyQQat5dQfzQ/n6K2fwn1jihepvBNOcvMDiuh/qOEm23Fy18CdiqhfCxQfCrAvfGRq3NQ8eOccxUjfKNP55xLT8WS9NQaHoQBM7uXRE/VOZctGY7BHoSdc1knH45wzrk0ZTgGexB2zmVbuDGXdisqzoOwcy7zqjipe43yIOycyzzvCTvnXFp8ippzzqXLhyOccy4lAoqyG4MzmTvCOed+qgqSR0iaEfPKTJQ0Lpa1lDQibsYwQlKLWC5JAyRNkzRJUteKNt2DsHMu86pwj7neZtYlkXv4YmBkzGs+Mr6GsCFox/g4gxLy2RTKg7BzLvOKlP9RCY2rvwkAABGdSURBVIcC98Xn9xHS3ObK77fgTaB5sU1BC297pZrnnHO1QdnDEa0kjUs8zijhLEZIWzs+8X6bxL5yc4E28Xk7ILk92axYVm5+Y845l2khzpbZ3Z1fwPZGu8a9J9cHRkhK5ivHzExSle944z1h51y2lTEUUehwhJnNjj/nAU8RNv/9IjfMEH/m9puczY+78gC0j2Xl5kHYOZd9lZwdEbcoa5Z7DuwLTAaGACfFaicBz8TnQ4AT4yyJnsC3iWGLcvHhCOdcxlXJPnJtgKdiSsyGwMNmNkzSWOAxSacBM4G+sf5QoA8wDVgMnFLRC3sQds5lWlXsI2dmnwA7lFD+FbBXCeUGnF3JywIehJ1zdUGGV8x5EHbOZV5t39Y+Hw/CzrnMy24I9iDsnMs6+Zb3zjmXGt/eyDnnUpbhGOxB2DmXfX5jzjnn0pTdGOxB2DmXbap8uspUeRB2zmWe7zHnnHNpym4M9iDsnMs+H45wzrnUlHsfuVrFg7BzLtN8sYZzzqUsy0HYd9ZwzmVeZbe8l7SRpP9Jek/SFEm/i+WXS5otaWJ89Ekc8ydJ0yRNlbRfRdvuPWHnXKZV0TzhFcAfzGxC3OZovKQR8b0bzOy6n15TnYBjgW2BDYH/StrSzFaW98LeE3bOZV8l95gzszlmNiE+Xwi8T/4t7A8FHjGzZWY2nbDN0c4VaboHYedc5hUwHNFK0rjE44xSzyV1AHYE3opF50iaJOkeSS1iWTvgs8Rhs8gftEvlQdg5l3kFbHk/38y6Jx53lnQeSU2BwcD5ZvYdcDuwOdAFmANcX+Vtr+oTOudcjavkcASApEaEAPyQmT0JYGZfmNlKM1sFDOTHIYfZwEaJw9vHsnLzIOycyzQRUlnme5R5jrA1x93A+2b2r0R520S1w4HJ8fkQ4FhJTSRtCnQExlSo/WHnZlfbSPoSmJl2OyqgFTA/7UbUI1n9vDcxs9ZVcSJJwwifQz7zzWz/POfYFRgFvAusisWXAP0IQxEGzADONLM58ZhLgVMJMyvON7MXKtR+D8KuKkkaZ2bd025HfeGfd/b5cIRzzqXIg7BzzqXIg7CraiVO/XHVxj/vjPMxYeecS5H3hJ1zLkUehJ1zLkUehJ1zLkUehJ1zLkUehF2tJalB/LmBpDXTbk9dI6mo2OsM70+RXR6EXa0jaVNJvcxspaSDCctJB0i6Ou221QWS1gIws1WSukk6UtIa5lOlUuFT1FytI6kfcCtwBrAn8AzwDXAu8JWZ/S7F5mWapObAZcDTwA/AfcDnwBLgL8BEM1uRXgvrH+8Ju1rHzAYB5wA3AGua2XBgPHAV0FLSv9NsX8atTciLewwhQc2hZrYH8DZwHtBFkm97VoM8CLtaIzcmKamjmT0MnA/sKWmP2Dv7ELgGaB73+HLlIElmNht4kLB9zxZADwAzuwT4FLgY6JpaI+shD8Ku1jAzk3QIMFBSFzMbDFwO3CVp95hY+33gVDN7L822Zk0MwCZpb0IC8kcIScp7SToAwMz+DHwMLEuvpfWPjwm7WiP2bh8AzjCz8YnyE4H+QD8zeymt9mVdDLY3AL8zs+GSNiJsWLktMNTMnk21gfWUj/242mRd4NNcAJbUyMyWm9n9klYQEmu7CogzIs4HfmNm/4s9488kPQs0AQ6X9CYh+bl/zjXIg7BLTeJP5KI41PA5sFTSNsBHZrZc0m7AjmZ2U/KYNNudUQ2AxoTPGELgXQp8DfwHWMfMvkypbfWajwm7VCQC8EHA1ZKuJ0yZmgecDZwl6VBCgJiSO84DcGESNzk3kdTEzBYCw4FrJLUws6XxC24YgJnNSK+19Zv3hF0qYgDuDVwJHAu8QBhu+CNh367NgZ2Ac8zsv6k1NKPi59sHuBR4RdL6wABgHWC0pP8AJwGXmNmCFJta7/mNOZcaSZcDrxGC71XAcWY2PfH+mma2JKXmZVq8yfkwcAjhL4uuwJFm9p2kYwh/dcw3s1E+xJMu7wm7NM0hrIprC5xgZtMlnQJsbGZX4FOlyi0RUNcgBOEtgD2A42MA7g48aWbLc8d4AE6Xjwm7GpEYo+wpaS9J3YAXge2Bu4CZsez3wFsQchuk1d6sSSTfyXWsPgWOIyxL3t/MpsU5wn8CWqTQRFcKH45wNUbSfoR5qv2Bu4HuwMbAaYRebxugv5kN8T+RC5e4ybkP0BeYAEwDWhOGI14GZhBWG15mZs+k1FRXAh+OcNUu9tJaAr8DDgM2Isx4mGtmEyT9jzCFqpmZzfQAXD4xAO8J3EiYC3wpIRfEdYQpaecTesZ/NrPn/POtXbwn7GqMpL8C3wNHASeb2YeSjgPeNbN3021ddsW8y+cAY4AVwL+BQ8xslqS1zGxxoq4H4FrGe8KuWiT+RG4DLIyBoCWhl9Y63iTqClwInJ5mW7Mu5l3+mpALYhnQx8zmxlzM7STdlUtP6QG49vEg7KpFYiHGtcDbklaY2UmSNgfukzSDcNf+cjMbl2JTMyfxBbcjsCnhRuYkYCwwIwbgnQljwH/w/MC1mw9HuGohaVvCWOQgQoC4A1jLzPrElXBFwBwze9P/RC6/eBPuNkJWOQNeIcz93QzoBSwHrjWzIak10hXEg7CrcpLWA94B3iUsEFgcy58DHjez+9JsX9bF3Bo3AReZ2dvxS60bMNbMnpW0CbDEzOb5F1zt5/OEXZVIzAPuYGZfAWcBHYF9EtXeApqm0LzMS8wDBuhNSD+5G0CccrYYODG+nmlm8+JzD8C1nI8Ju0pLjFEeAvxB0jlxKtQawI2SdgLGEXIVnJ1qYzMo8fnuBXxFyLkMsLOkI2Py+1eAXSStY2bfpdZYV24ehF2lxQCxC3AFIf/D+5LWNbMnJM0BHiXMDT44vud/IpdD4gvuH8CFZjZR0mDCWPBf4nubA//0AJw9HoRdVWlF6O1uGFfG9ZG0kjD97AzCQoJNCDeSXDlIagVcBBwe51ZvD6wHPElY5NILeNR3xsgmD8KuQhJ/Irci/In8IfAFIV3itYQUlXsAHc1sqKSWwD8kvWZm36fV7oxqQEjAvr+kiwnj6rsBFxByQ/wA9Jb0kZkNS6+ZriJ8doSrsPhn8CnALMIc1eeA5Wa2MC7EeBA43cxGx/rNYnJxl0fiC24HQvD9kjD74WDgeQv7w/UF9jSzsyRtDOwFDDOzOem13FWEB2FXITEl4kDgAOB2QISsXQbsQNgR449xylSRma3yseDCKWzKeS1wLyHR/S5m9kl8rzdwC2EhxrBY1sDMVqbUXFcJPhzhClJCAG1DSEHZiZAPuJ+ZLY69si+Bo81scjxuFfh0qULEqWjtCMu7DyFkmpsDfB/fawv8mTBHeFju38UDcHZ5T9iVKU4162NmT8Y/kbcAPiYsGGgR35sl6XDgIODcZNIYl5+kRkBDM1sSP+vGhIxznxAS85wUb8gdSsjBvKaZLfC/LOoG7wm7QiwHNpY0NT4/hHAz7l3gW6CTpA6EKWqXegAunKSGwJ7AorjSbVfC8MO+hC2JWpjZD5J6ABcDU83sA/C/LOoK7wm7gsRkMc8AX5pZt0TZLwkruJYDD5onZC+3mAv4amAD4AIzGyxpA8LuyG8QZp78ipDsyBOy1zEehF2pksE0/sncnrAcuQdhzPdLSRuZ2We5vLUegAtX7PO9l/D53gC8bWafS2pG2O5pPvC+mb3kn2/d40HYlSgxTepAYBdgpZldJqkI+BfhhtHfCcuQzzSzWSk2N3MSn297YDbQhDAUcSow1MwelNQaaGRmn6fZVle9PIGPK1EMEH0IgXYwcJKkJ4B1zex8Qq6Ci4DbPACXX+IL7nHCZ3wO8CohL8QBkvoDHxCWe7s6zHvCrkSS1iTMA74O2BC4hLA1URPC8tlvJDWPP/1P5HKStCshH/DhhCGHnsAowhdbJ2BHYKaZjUytka5GeBB2q+UWVSRerwusT+id9Y5TqL4BnidMm/IdG8ohuaAiTjf7EOgAXAVcRsix8SlwhZl9mTjOv+TqMJ+i5nK93hVmtlxSL8KCgOlmNl5Sc8JigY0krU1IGnOPB+DC5ZZrW9gLrjch8E4hfK5nAqea2TuSjgKaE774VgdhD8B1mwfhek5hF4wLgSExGN9HGKe8S9IJMS/wNOBvhGxdp5rZa947K4yktYDnJQ0g7DZyK/Ae4SbcFMJNz9mSGgPbAKeZ2ZS02utqng9H1HNx6tm1hExdRcBTZjYyrn67DzjIzF6V1ImwR5xvyllO8bO8GFgAXBx7vccResQbEuZafwwMMrPHU2uoS4UH4XoskVinESEfQW/CTIg74/jvEcATwGHmG0ZWisLGnI8Bfzez/nGl3DHAVoRMaXf4UuT6yaeo1WMxABeZ2XLCzaERhLwQO0lqbGZPAn2BZWm2sy4wsxGEtJ8nS+oXx9QfAaYS/vpYEOt5AK5nvCdcTxVbrdXQzFbEccm/As2AIcAoM/uheH1XcXHu9d+AAea7Tju8J1zvxHSIkPi3jwG4UQy4VxJ2ajiSxM7IHoCrhpkNJSQ6ukjShnEFoqvHvCdcjySWyu5NSAjzCfCxmT0Y328Up6k1BjqY2Ydptrcuk9Q6ORfY1V/+LVyPxAC8O3Az8DIhZ8HZkv4Q318ex4h/8ABcvTwAuxyfJ1z/tAcGmtl/ACS9BfSXNMzMpiRXzDnnqp/3hOu4xBhwzprACYnXUwi7JPu4lHMp8CBcx+WGICT9VlInM7sLeEvSSIVt6LsD2wON0m2pc/WT35iroxI34XoA9xCWyi4GXgMeIqyS6wCsB/zDF2M4lw4PwnWYpJ0JU87+aGaTJPUjpEycZGZ3x+lRzX2llnPp8eGIuq05sDewT3z9ODAa6Cnpd4CAr8HnATuXFp8dUYeZ2Ysx/8M/JH1uZoPi7hgNgHdyuW2dc+nxIFzHWdj9eAXwt5gP4j5gUNrtcs4FPiZcT0g6BLiGMDwx1+cDO1c7eBCuR3yprHO1jwdh55xLkc+OcM65FHkQds65FHkQds65FHkQds65FHkQdqmQtFLSREmTJT0et4av6LnulXRUfH5X3Bm6tLp7SPpFBa4xQ1KrQsuL1fm+nNe6XNIF5W2jyyYPwi4tS8ysi5l1JmyndFbyzbgbcbmZ2a/N7L08VfYAyh2EnasuHoRdbTAK2CL2UkdJGgK8J6mBpP6SxkqaJOlMCBniJN0iaaqk/wLr504k6WVJ3ePz/SVNkPROTN3ZgRDs/y/2wn8pqbWkwfEaYyX1iseuJ+lFSVMk3UXIs5GXpKcljY/HnFHsvRti+UhJrWPZ5pKGxWNGSdq6Kj5Mly2+bNmlKvZ4DwCGxaKuQGczmx4D2bdmtpOkJsBoSS8COwJbAZ2ANoQ0nfcUO29rYCCwWzxXy5gt7g7gezO7LtZ7GLjBzF6TtDEwHNgGuAx4zcyulHQgcFoBv86p8RprAmMlDTazr4C1gXFm9n+S/hrPfQ5wJ3CWmX0UU47eBuxZgY/RZZgHYZeWNSVNjM9HAXcThgnGmNn0WL4vsH1uvBdYF+gI7AYMigmIPpf0Ugnn7wm8mjuXmS0opR17A50SG5CsI6lpvMYR8djnJX1dwO90nqTD4/ONYlu/AlYBj8byB4En4zV+ATyeuHaTAq7h6hgPwi4tS8ysS7IgBqNFySLgXDMbXqxenypsRxHQ08yWltCWgknagxDQdzGzxZJeBtYopbrF635T/DNw9Y+PCbvabDjwG0mNACRtKWlt4FXgmDhm3BboXcKxbwK7Sdo0Htsyli8EmiXqvQicm3shKRcUXwWOi2UHAC3KaOu6wNcxAG9N6InnFAG53vxxhGGO74Dpko6O15CkHcq4hquDPAi72uwuwnjvBEmTgX8T/np7Cvgovnc/8EbxA2OiojMIf/q/w4/DAc8Ch+duzAHnAd3jjb/3+HGWxhWEID6FMCzxaRltHQY0lPQ+IVvdm4n3FgE7x99hT8JuJwDHA6fF9k0BDi3gM3F1jCfwcc65FHlP2DnnUuRB2DnnUuRB2DnnUuRB2DnnUuRB2DnnUuRB2DnnUuRB2DnnUvT/juDaNbTez3oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}