{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "9fc4d79f-0973-40fa-9096-6c9d6c87c5c0"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf817c6b-6bd9-4a54-86af-072bd7783532"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.879497279000134\n",
            "GPU (s):\n",
            "0.03722266499971738\n",
            "GPU speedup over CPU: 77x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1b28cb-1725-413b-f72c-e3b6bf19d5e5"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e823ce85-dfd8-43d2-dcd6-ad27247c4a4c"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "#X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X= tf.keras.layers.LSTM(90, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(100, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1407 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1407.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1407.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8721069-1432-442c-d9e6-c2ed300d3212"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 60, 90)       309240      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 90)          0           ['lstm[0][0]']                   \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100)          9100        ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 100)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            101         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,052,521\n",
            "Trainable params: 135,052,521\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 5\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "#init_lr=2e-5\n",
        "init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932abcb0-fb98-46ea-b5a7-7bc9ac9e4c78"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1407.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3215a32-39d6-4e46-dec0-54766314cab2"
      },
      "source": [
        "model1407.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =5,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 48s 244ms/step - loss: 0.5894 - binary_accuracy: 0.6902 - metrics_recall: 0.2875 - metrics_precision: 0.4867 - metrics_f1: 0.3160\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 39s 249ms/step - loss: 0.4416 - binary_accuracy: 0.7992 - metrics_recall: 0.6373 - metrics_precision: 0.7676 - metrics_f1: 0.6593\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 40s 252ms/step - loss: 0.2708 - binary_accuracy: 0.8960 - metrics_recall: 0.8371 - metrics_precision: 0.8612 - metrics_f1: 0.8392\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 40s 254ms/step - loss: 0.1654 - binary_accuracy: 0.9429 - metrics_recall: 0.9093 - metrics_precision: 0.9261 - metrics_f1: 0.9120\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 40s 255ms/step - loss: 0.0958 - binary_accuracy: 0.9742 - metrics_recall: 0.9561 - metrics_precision: 0.9675 - metrics_f1: 0.9597\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc58e23e90>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1407.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851934a1-5b0a-403d-a30b-bc882f71a791"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c3634b-d8be-4dd5-8b0d-7326a1daf391"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00967484],\n",
              "       [0.95737535],\n",
              "       [0.9698445 ],\n",
              "       ...,\n",
              "       [0.93126976],\n",
              "       [0.00947891],\n",
              "       [0.01022758]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ee1b5b-0258-40ec-b5bc-87401e6ac054"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7669875424688561"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "9e762982-f76d-45d4-93da-96de3fdbd44e"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2108  222]\n",
            " [ 601  601]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}