{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "0ce845b4-3145-46d4-df46-0aada7b99f30"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d441cd49-6e20-46c0-f3f5-d9a066228331"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.8488221560000966\n",
            "GPU (s):\n",
            "0.036810238999350986\n",
            "GPU speedup over CPU: 77x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea76e538-29f6-4ddc-c8d9-9801f082f8ed"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671de302-fd15-4626-ce5c-6f9c34627b62"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "#X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X= tf.keras.layers.LSTM(90, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(100, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1409 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1409.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'activation_13', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1409.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9135ef-9c84-4b70-e9ff-875b1adc6f4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 60, 90)       309240      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 90)          0           ['lstm[0][0]']                   \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100)          9100        ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 100)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            101         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,052,521\n",
            "Trainable params: 135,052,521\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 3\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3169114-cc01-46ac-9512-f7c23e6f81e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1409.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfb513c-a924-4c36-cd06-5caaeb885dab"
      },
      "source": [
        "model1409.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =3,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "157/157 [==============================] - 48s 245ms/step - loss: 0.5917 - binary_accuracy: 0.6912 - metrics_recall: 0.2050 - metrics_precision: 0.3771 - metrics_f1: 0.2395\n",
            "Epoch 2/3\n",
            "157/157 [==============================] - 39s 249ms/step - loss: 0.4896 - binary_accuracy: 0.7612 - metrics_recall: 0.5848 - metrics_precision: 0.6834 - metrics_f1: 0.6074\n",
            "Epoch 3/3\n",
            "157/157 [==============================] - 39s 251ms/step - loss: 0.4329 - binary_accuracy: 0.8036 - metrics_recall: 0.6564 - metrics_precision: 0.7302 - metrics_f1: 0.6789\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8459f9250>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1409.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfee2a5-9fad-4e86-cfa2-219ad1f50f40"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50cd594-d987-49b0-89d8-499e1202f782"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.10238376],\n",
              "       [0.4506375 ],\n",
              "       [0.7659522 ],\n",
              "       ...,\n",
              "       [0.7960245 ],\n",
              "       [0.59002805],\n",
              "       [0.10414886]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f93a1a-8c49-4828-ee32-608aa4097371"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7508493771234428"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "3aebf647-70c8-4fca-da88-b1d76d1426df"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2130  200]\n",
            " [ 680  522]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEmCAYAAACzoiEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7zVU/7H8df7dBMi6aJ7Lk2koXKLjMk99/sljOsw/YbBzDByGYxrEwa535qiERIKKWEQIyqSiqaQ0V3uRCqf3x9rnXw7ztlnn+v3fM/5POexH2fv9V3f73ft3fjstT/f9V1LZoZzzrl0FKTdAOecq8s8CDvnXIo8CDvnXIo8CDvnXIo8CDvnXIo8CDvnXIo8CLtqJ2mopKvi819Jmp3HPhdJujc+7yTJJNWPr1+U9NuqbXXVKa39ku6U9NfqbJOrPvXTboCr28xsItAlj3rXVENzgBAUgeFmdm91nTNx7pOB35rZroVlZta/utvhqo8HYeciSQKUdjtc3eLpCFflJPWQ9KakryU9DKyT2NZH0vzE6wskLYh1Z0vaM5ZfLml4jtNsLukNSV9JGi2pWeKYvST9R9IXkt6W1Cex7UVJV0t6FVgOPAD8CrhV0jeSbi3m/RSmQ06R9LGkzyX1l7SDpOnxPLcm6q/V9qLplET5VsCdwM7x3F/E8jXpG1f7eBB2VUpSQ+AJQnBrBowEjiihbhfgLGAHM2sC7AvMy/NUJwKnAq2BVcDgeMy2wNPAVfH85wGjJLVI7Psb4AygCXAyMBE4y8zWN7OzcpxzJ6AzcAxwE3AxsBewNXC0pF/n2XYAzOxdoD/wWjx307Ls77LJg7Crar2ABsBNZrbSzB4FJpdQdzXQCOgqqYGZzTOz9/M8zwNmNsPMvgX+SgiC9YATgLFmNtbMfjSzCcAUYP/EvkPNbKaZrTKzlWV4b1ea2fdm9izwLTDCzJaa2QJCIO9RhmO5OsqDsKtqbYAFtvZMUR8VV9HM5gLnApcDSyU9JKlNnuf5uMjxGwDNgY7AUTFF8EX8ib8rocdc3L5lsSTx/LtiXq9fzuO6OsSDsKtqi4C28aJXoQ4lVTazB+PIgI6AAX/P8zztixx/JbCMEGAfMLOmicd6ZjYwedqizcjznPn6Flg38XqTHHV9WsM6xoOwq2qvEXK0Z0tqIOlwYMfiKkrqImkPSY2A7wm9yR/zPM8JkrpKWhe4AnjUzFYDw4GDJO0rqZ6kdeLFwHY5jrUE2CzP8+ZjGrCbpA6SNgQuLOXc7WIu3dUBHoRdlTKzH4DDCRe8PiNcxHqshOqNgIGEHuxioCW5A1bSA8DQuN86wNnx/B8DhwAXAZ8Qesbnk/v/+zcDR8ZRD4PzPH+JYh76YWA6MBV4Kkf1F4CZwGJJyyp6blfzySd1d8659HhP2DnnUuRB2DnnUuRB2DnnUuRB2DnnUuQT+NRQqt/Y1LBJ2s2oM3psVeLQZVcFPvpoHsuWLauUyZLqbdDRbNV3OevYd5+MN7O+lXG+yuZBuIZSwyY06nJ02s2oM159/Wfz9Lgq1Hun7SvtWLbqu1L/W/l+2m3NK+2ElcyDsHMu2yQoqJd2K8rNg7BzLvuU3ctbHoSdcxnnPWHnnEuXsrsgigdh51y2CU9HOOdcejwd4Zxz6fJ0hHPOpUWZTkdkt+XOOQchJ1xQL/ejtENI7SX9W9IsSTMlnRPLm0maIGlO/LtRLJekwZLmxhW2eyaOdVKsP0fSSaWd24Owcy7jYk8416N0q4A/m1lXwuK0Z0rqCgwAnjezzsDz8TXAfoSVtjsTVuq+A0LQBi4jrMS9I3BZYeAuiQdh51y2CahXL/ejFGa2yMzejM+/Bt4F2hJWZRkWqw0DDo3PDwHut2AS0FRSa2BfYIKZfWZmnwMTgJxzVnhO2DmXfaVfmGsuaUri9d1mdnfxh1InoAfwOtDKzBbFTYuBVvF5W9ZepXt+LCupvEQehJ1zGZfXhbllZlbqrEGS1gdGAeea2VfJRcLNzCRV+npwno5wzmVfBS/MAUhqQAjA/zKzwsVol8Q0A/Hv0li+AGif2L1dLCupvOSm59U655yrqaTSH6UeQgLuA941s38kNo0BCkc4nASMTpSfGEdJ9AK+jGmL8cA+kjaKF+T2iWUl8nSEcy77Kn7HXG/gN8A7kqbFsouAgcAjkk4DPgIKJy4eC+wPzAWWA6cAmNlnkq4EJsd6V5jZZ7lO7EHYOZdxFb9Zw8xeCQcq1p7F1DfgzBKONQQYku+5PQg757LPb1t2zrmUSFCQ3VCW3ZY751wh7wk751yKfCpL55xLibI9i5oHYedc9nk6wjnn0iGgoMB7ws45lw5R8gjfDPAg7JzLOCFPRzjnXHo8HeGccynynrBzzqVEEirwIOycc6nxnrBzzqUoy0E4u9ls55yDOJOlcj5KPYQ0RNJSSTMSZQ9LmhYf8wrnGZbUSdJ3iW13JvbZTtI7kuZKGqw8vh28J+ycy7xK6AkPBW4F7i8sMLNjEse/AfgyUf99M+tezHHuAE4nLBI6lrDS8jO5Tuw9YedcpglRUFCQ81EaM3sZKHYFjNibPRoYkbMdYQ26DcxsUpz0/X7g0NLO7UHYOZd9KuVRMb8ClpjZnETZppLekvSSpF/FsraEJe4LlbrcPXg6wjmXdcorHdFc0pTE67vN7O48z9CPtXvBi4AOZvappO2AJyRtnX+D1+ZB2DmXeXmkHJaZ2fZlPa6k+sDhwHaFZWa2AlgRn0+V9D7wC8LS9u0Su5e63D14OsKVol2rpoy7+2zeHHUxUx+9mDP79QHg8L16MPXRi/l26mB6du2wpv72W3dk0kMDmPTQAF5/eAAH777Nmm1777IVbz/+V2aMvozzTtm7ut9K5nz88cfsu9fu9NimKz233ZpbB98MwGeffcYBffem21adOaDv3nz++ecAmBl/Ovdstt5yC3bosQ1vvflmms2vNopzR+R6VMBewHtmtibNIKmFpHrx+WZAZ+CDuOT9V5J6xTzyicDo0k7gQdjltGr1jwz4x2P0POJqfn3i9fzumN3YcrNNmPn+Qo798z288ub7a9Wf+f5Ceh8/iF7HDuSQM2/nlkv6Ua9eAQUF4qYBR3PIWbfT44irOKrvdmy52SYpvatsqF+/PgMH3cBb02fx0iuTuOvO23h31iyuHzSQPnvsyYx359Bnjz25ftBAAMaPe4b3585hxrtzuPWOuzn7rP9L+R1Uk8oZojYCeA3oIml+XOIe4Fh+fkFuN2B6HLL2KNA/saz974F7gbnA+5QyMgI8HeFKsXjZVyxe9hUA3yxfwXsfLqZNi6a88Pp7xdb/7vuVa543atiAcJEYdujWifc/Xsa8BZ8CMHL8mxzYZxve+2BxFb+D7GrdujWtW7cGoEmTJmy55VYsXLiAp54czfjnXgTghN+cxL579eHqa//OU2NGc9wJJyKJnXr14ssvv2DRokVrjlGbVXSImpn1K6H85GLKRgGjSqg/BehWlnN7EHZ569C6Gd27tGPyjHk56+3QrSN3Xn4CHVo347RLhrF69Y+0abkh85d8vqbOgiWfs2O3TlXb4Frko3nzmDbtLXbYcSeWLlmyJrBusskmLF2yBICFCxfQrl37Nfu0bduOhQsWeBCu4WpkOkLSUElHlqF+U0m/r8o2VUS826Z52u2oiPUaN2TE9b/l/OtH8fW33+esO3nGR2x35NXsesIgzj91Hxo19O/6ivjmm2/od/QRXHfDTWywwQZrbauEnGetUNF0RJpqZBAuh6aEXIyrAvXrFzDi+tN5+JkpjH7h7bz3m/3hEr5ZvoKtt2jDwqVf0q7VRmu2tW21EQs++TLH3g5g5cqV9Dv6CI7pdzyHHnY4AC1btWLRokUALFq0iBYtWwLQpk1b5s//eM2+CxbMp03bUoepZl5pF+Vq+pdUlQTheG/1u5LukTRT0rOSGsdt3SVNkjRd0uOSNirhMLtJ+o+kDwp7xZLWl/S8pDfj/dmHxLoDgc3jfdzXxbrnS5ocz/O3WLaepKclvS1phqRjYvk8SYPiMd+QtEUsbyFpVDzOZEm9E8cZEuu+VdgOSfUkXR+PPV3SHxLv5w+Jdm9ZuZ941brzsuOZ/eFiBg9/odS6HdtsTL164f9WHVpvRJdNN+GjhZ8yZeZHbNGhBR3bbEyD+vU4at+ePP3i9KpueqaZGf1PP40uW27FOX/805ryAw48mOEPDANg+APDOPCg8J/BAQcdzIPD78fMeH3SJDbYYMM6kYqA0gNxTVaVvxM7A/3M7HRJjwBHAMMJt/L9wcxeknQFcBlwbjH7twZ2BbYExhCuQn4PHGZmX8Wf95MkjQEGAN0K7+WWtE88/46E+2XGSNoNaAEsNLMDYr0NE+f70sx+KelE4CbgQOBm4EYze0VSB2A8sBVwMfCCmZ0qqSnwhqTnCENSOgHdzWyVpGaJ4y8zs54xbXIe8Nuib1jSGcAZADRYP5/PuMrt0n0zjj9wJ9757wImPTQAgMtuHUOjBvX5xwVH0Xyj9XlscH+mz17AwWfexi49NuO8U/Zh5arV/Pijcc41D/PpF98C8Me/P8KTt59JvQIxbPQk3vWLcjn959VXefBfD9Ct2y/ZabswTcHfrrqG8/4ygBP6Hc2wf95Hhw4dGT7iEQD67rc/458Zy9ZbbsG6jdflrnv/mWbzq1VNTznkosKr15V6UKkTMMHMOsfXFwANgFuAd8ysQyzfHBhpZj2L7D807v+v+PprM2siqQFwI2GIyI9AF2BTYB3gKTPrFutfDxwJfBEPuT5wLTAReBZ4ONafGOvPA/Ywsw/iORab2caSlgILE01rEc/5YjznqljeDNgXuAq408wmFHk/84DeZrZA0k7A1Wa2V67PsGDdltaoy9G5qrhK9PnkW9NuQp3Se6ftmTp1SqVEzkatOlvb42/OWefDGw+YWp6bNapDVfaEVySerwYaV2D/wn+s4wmBcDszWxmD2zrF7CvgWjO762cbpJ7A/sBVkp43syvipuS3UeHzAqCXmX1f5BgCjjCz2UXK83k/q/FRKc5VGgkKMtwTrtYLc2b2JfC5fprw4jfAS2U4xIbA0hiAdwc6xvKvgSaJeuOBUyWtDyCpraSWktoAy81sOHAdkOyBH5P4+1p8/iywJq8rqXDquvGEHK9ieY9YPgH4ncKtjhRJRzjnqkS2L8yl0SM7CbhT0rrAB8ApZdj3X8CTkt4BpgDvAcSJNF5VmJD5GTM7X9JWwGvxH+Ab4ARgC+A6ST8CK4HkLUUbSZpO6LEWDtw+G7gtltcHXgb6A1cS8sbTJRUAHxJyyPcS7iGfLmklcA9hjlLnXBWq4XE2pyrJCWdNTGtsb2bL0m5LIc8JVy/PCVevyswJr9P6F9bppFty1pn99751MifsnHNVTmQ7J+xBGDCzTmm3wTlXfh6EnXMuLcp2TtiDsHMu00S2J/DxIOycyzhlOh1RWybwcc7VYRUdJxznglkah7kWll0uaUGck2aapP0T2y6UNFfSbEn7Jsr7xrK5kgbk03YPws65TCu8Yy7XIw9Dgb7FlN9oZt3jY2w4n7oSVtzYOu5ze5y8qx5wG7Af0BXoF+vm5OkI51zmVTQlbGYvxzlv8nEI8FBc8PNDSXMJk4UBzDWzD0Kb9FCsOyvXwbwn7JzLvCq8bfmsOC3tEP007W5b4ONEnfmxrKTynDwIO+eyLb90RHNJUxKPM/I48h3A5kB3YBFwQ1U039MRzrlMC0PUSq22rKy3LZvZkjXnkO4BnoovFwDtE1XbxTJylJfIe8LOuYyrmlnUJCWXJTkMKBw5MQY4VlIjSZsSFpB4A5gMdJa0qaSGhIt3Y0o7j/eEnXOZV9FxwpJGAH0IaYv5hBV/+sTpaw2YB/wOwMxmKqwWNIuwsMOZZrY6HucswlS39YAhZjaztHN7EHbOZVsl3LZsZv2KKb4vR/2rgauLKR8LjC3LuT0IO+cyLcyilt3Mqgdh51zmZXjqCA/Czrns8wl8nHMuJVK2J/DxIOycy7wMd4RLDsKSbmHtZeDXYmZnV0mLnHOujOrV0p7wlGprhXPOlZNUS3PCZjYs+VrSuma2vOqb5JxzZZPhjnDpty1L2lnSLOC9+HpbSbdXecuccy5PlTCfcGryGeF8E7Av8CmAmb0N7FaVjXLOuXwJUCn/q8nyGh1hZh8XybmsrprmOOdcGUm19sJcoY8l7QKYpAbAOcC7Vdss55zLX4avy+UVhPsDNxNmiF9ImCHozKpslHPO5UtAQYajcKlB2MyWAcdXQ1ucc65cavrFt1zyGR2xmaQnJX0Sl4QeLWmz6micc86VRir9UZPlMzriQeARoDXQBhgJjKjKRjnnXFkUSDkfpYkLeS6VNCNRdp2k9+JCn49LahrLO0n6TtK0+Lgzsc92kt6RNFfSYOVxF0k+QXhdM3vAzFbFx3BgnTz2c865alHRIAwMBfoWKZsAdDOzbYD/Ahcmtr1vZt3jo3+i/A7gdMKSR52LOebP217SBknNJDUDnpE0IEb/jpL+QhlnjnfOuaoSLszlfpTGzF4GPitS9qyZrYovJxEW7iy5HWFNug3MbJKZGXA/cGhp5851YW4qYQKfwrfwu2T7WPtbwTnn0pHfVJbNJSXnw7nbzO4uw1lOBR5OvN5U0lvAV8AlZjaRMIJsfqLO/FiWU665IzYtQwOdcy41eaRey7zkfeLYFxMW9PxXLFoEdDCzTyVtBzwhaevyHBvyvGNOUjegK4lcsJndX96TOudcZSlMR1TJsaWTgQOBPWOKATNbAayIz6dKeh/4BbCAtVMW7WJZTqUGYUmXEZaC7krIBe8HvELIdzjnXOqq4mYNSX2BvwC/Ts4gKakF8JmZrY7DdTsDH5jZZ5K+ktQLeB04Ebil1Lbn0ZYjgT2BxWZ2CrAtsGGZ35FzzlUBqVKGqI0AXgO6SJov6TTgVqAJMKHIULTdgOmSpgGPAv3NrPCi3u+Be4G5wPvAM6WdO590xHdm9qOkVZI2AJYC7fPYzznnqkVF75gzs37FFN9XQt1RwKgStk0BupXl3PkE4SlxkPI9hBET3xC+MZxzrkao6XfF5ZLP3BG/j0/vlDSOMA5uetU2yznn8iPyviGjRsq10GfPXNvM7M2qaZID6LJ5W4aMvCrtZtQZHy79Nu0m1CkrVv1YeQdTtifwydUTviHHNgP2qOS2OOdcueQzwqCmynWzxu7V2RDnnCsPUXuXvHfOuUzIcAz2IOycy7YwZ3B2o7AHYedc5tXLcFI4n5U1JOkESZfG1x0k7Vj1TXPOudIVrjFXwfmEU5PP98ftwM5A4R0lXwO3VVmLnHOujApKedRk+aQjdjKznnHuTMzsc0kNq7hdzjmXF0m1fnTESkn1CGODC2cQqsSR1s45VzE1POOQUz5BeDDwONBS0tWEWdUuqdJWOedcngTUr809YTP7l6SphOksBRxqZu9Wecuccy5PtbonLKkDsBx4MllmZv+ryoY551xe8lzMs6bK58Lh08BT8e/zwAfkMVGxc85VBwH1pJyPUo8hDZG0VNKMRFkzSRMkzYl/N4rlkjRY0lxJ05OTnUk6KdafI+mkfNpfahA2s1+a2Tbxb2dgR3w+YedcDVLRJe+BoUDfImUDgOdj3Hs+voawxFvn+DgDuANC0AYuA3YixMnLCgN3zrbn1byEOIXlTmXdzznnqkLhBD65HqUxs5eBz4oUHwIMi8+HAYcmyu+3YBLQVFJrYF9ggpl9ZmafAxP4eWD/mXxywn9KvCwAegILS9vPOeeqhfK6MNdc0pTE67vN7O5S9mllZovi88VAq/i8LfBxot78WFZSeU75DFFrkni+ipAbLnZ9JeecS0MetyYvM7Pty3t8MzNJVt79c8kZhONNGk3M7LyqOLlzzlVUSEdUyaGXSGptZotiumFpLF/A2osdt4tlC4A+RcpfLO0kJTZdUn0zWw30Llu7nXOuOomCUh7lNAYoHOFwEjA6UX5iHCXRC/gypi3GA/tI2ihekNsnluWUqyf8BiH/O03SGGAksGYhLjN7rIxvyDnnKp1U8Z6wpBGEXmxzSfMJoxwGAo9IOg34CDg6Vh8L7A/MJdxDcQqAmX0m6Upgcqx3hZkVvdj3M/nkhNcBPiWsKWeE3r8BHoSdczVCRaerNLN+JWzas5i6BpxZwnGGAEPKcu5cQbhlHBkxg5+C75pzleUkzjlXVUTtvW25HrA+FJtQ8SDsnKsxautUlovM7Ipqa4lzzpWDqPkTt+eSKwhn96vFOVd31OKFPn+WkHbOuZqmcAKfrCoxCOcztMI552qC7IZgX/LeOZd5oqCWXphzzrkarzZfmHPOuUyorRfmnHOu5lPF75hLkwdh51ymeTrCOedS5j1h55xLUYZjsAdh51y2hXREdqOwB2HnXMYp0+mILOeznXMOCOmIXI/S91cXSdMSj68knSvpckkLEuX7J/a5UNJcSbMl7VvetntP2DmXaVLF544ws9lA93A81SOsF/c4YdWMG83s+rXPqa7AscDWQBvgOUm/iEvClYkHYVcmX3/1JddedDYfzHkXIS4aeAuNGjXmukv/xA8rvqde/fqcd/n1dN12O8yMG68cwGsvTWCdxo255O+302XrbdN+C5my105dWW/99SkoqEf9+vUZ+cxErrvyYl6cMJYGDRvSvuOmXP2PO9lgw6b85+UX+Mc1l7Jy5Q80aNCQ8y65il679kn7LVSLSs5G7Am8b2Yf5bgJ5BDgITNbAXwoaS6wI/BaWU/m6QhXJjddNYBeu+3JQ+Pf4P4nJ9Jp8y7cNugyTv3DXxj25ER+e86F3DboMgBee2kC8z96n0eem8oFV97EdZf+OeXWZ9PQkWN5fMJrjHxmIgC77LYHo1+YzBPPvU6nzTpzz603ANC02cbcPnQko59/g2tvuosB55yeZrOrlUr5H2HtuCmJxxk5DncsMCLx+ixJ0yUNiQt4ArQFPk7UmR/LysyDsMvbN19/ybTJ/+Ggo34DQIOGDWmywYZI4ttvvo51vqJ5y00AmPjcWPoeeiyS6NZjB775+kuWLV2cWvtri96/3pP69cOP2G177sDiRQsA6NptW1pu0hqALbp05fvvv+eHFStSa2d1KZzKMtcDWGZm2ycedxd7LKkhcDBhYWOAO4DNCamKRcANld1+T0e4vC38+H80bdacqy84kznvzWDLbt0595JrOffia/jjqUdw68C/8qMZdz08DoBPliyiVeufOgctNmnDJ0sWrQnSrnSS+G2/Q5DE0SecytEnnLrW9sceeoC+Bx/xs/2effoJunbbloaNGlVXU1NViemI/YA3zWwJQOHfcA7dAzwVXy4A2if2axfLyqzG9oQldZI0owz1D43J8hpH0smSbk27HRW1evUq/jvzbQ477lSGjXmZdRqvywN33cRjDw7h7Iuu4YmJMznnoqu59qKz025qrTH88QmMGv8qdw1/jBFD72bKpFfWbLvz5kHUq1+Pgw4/Zq195syexT+uuZTL/z64upubmjzSEfnqRyIVIal1YtthhIWPAcYAx0pqJGlToDPwRnnaXmODcDkcCtTIIFxbtNykDS02acPW3bcHYPe+BzN75ts88/gI+ux7EAB77Hcos95+E4AWrVqzZNFPnYNPFi+kRavWPz+wK1Gr1m0A2Lh5S/bc7yCmT5sKwOMPD+el58Yx6NYha80gtnjhAs4+7TiuvfluOnTaLJU2VzeROxWR78gJSesBewOPJYoHSXpH0nRgd+CPAGY2E3gEmAWMA84sz8gIqPlBuJ6keyTNlPSspMaSTpc0WdLbkkZJWlfSLoQ8znVxLN/m8TFO0lRJEyVtCSDpKEkz4v4vx7KTJY2W9KKkOZIuK2yApBMkvRGPe1ccvoKkfSS9JulNSSMlrR/Ld5D0n3j8NyQ1iYdqE9szR9Kgav0UK8nGLVrRqnVbPvpgDgBTXnuZTbfoQvOWrXnrjVcBmPray7SP//Hvuud+jHviIcyMGW9NZr0mG3gqogyWL/92Ta59+fJv+c9LL9C5S1cm/nsC991xI7cNfZjGjdddU/+rL7/g/048gj9d9Dd67rBzWs2ufqWMEc43VWFm35rZxmb2ZaLsN2b2SzPbxswONrNFiW1Xm9nmZtbFzJ4pb/Nrek64M9DPzE6X9AhwBPCYmd0DIOkq4DQzu0XSGOApM3s0bnse6G9mcyTtBNwO7AFcCuxrZgskNU2ca0egG7AcmCzpaeBb4Bigt5mtlHQ7cLykscAlwF5m9q2kC4A/SRoIPAwcY2aTJW0AfBeP3x3oAawAZku6xcySV1cz4Y9/HcTf/nwGK1f+QJv2nbh44G38aq/9uemqC1m9ehUNG67DBVfdBMAuffbhtZcmcNSePVmncWMuHnhbyq3Plk8/WcrZp/UDYNXqVRxw6NH8ave92bf3NqxcsYLTjj0YCBfnLv/7YB785138b94H3H7jQG6/cSAA944YzcbNW6b2HqpDrV1jrob40MymxedTgU5Atxh8mwLrA+OL7hR7pbsAIxM/1QqvULwKDI1BPfmzY4KZfRr3fwzYFVgFbEcIygCNgaVAL0Lq49VY3pAwPrALsMjMJgOY2VfxeADPF37DSpoFdGTtIS7EYTNnALRq0y7vD6k6/aLrLxny+L/XKtt2+5355xMv/qyuJM67/Pqflbv8tO+4KY8/N+ln5eNfnV5s/f7nXkD/cy+o6mbVSNkNwTU/CCfH16wmBMGhwKFm9rakk4E+xexXAHxhZt2LbjCz/rFnfAAwVdJ2hZuKViX82w4zswuTGyQdRAja/YqU/7IM7+Vnn30cNnM3wFa/7FG0Pc65kmQ4Ctf0nHBxmgCLJDUAjk+Ufx23FfZAP5R0FICCbePzzc3sdTO7FPiEn4aZ7C2pmaTGhIt8rwLPA0dKahn3bSapIzAJ6C1pi1i+nqRfALOB1pJ2iOVNJNX0LzrnMq9AyvmoybIYhP8KvE4Iku8lyh8Czpf0lqTNCQH6NElvAzMJtxlCuHj3Thz+9h/g7Vj+BjAKmA6MMrMpZjaLkPt9Nl4dnQC0NrNPgJOBEbH8NWBLM/uBkEO+JZ53ArBOlXwKzrk1VMqjJquxvTQzm0e4UFb4OplcvKOY+q/y8yFqfYupd3jRsnNm0bAAABKRSURBVJiznW9mhxZT/2HCxbai5S8AOxRTPpmQM04aGh+FdQ4sup9zrnyEL/TpnHPpKcMwtJrIgzBgZkNJ9FSdc9mS4RjsQdg5l3XydIRzzqUpwzHYg7BzLtvChbm0W1F+HoSdc5lXxpnSahQPws65zPOesHPOpcWHqDnnXLqynI7I4m3Lzjm3hoAC5X7kdRxpXpzSYJqkKbGsmaQJcR7wCYoLfcb5aAZLmquwCGjP8rbfg7BzLvsqb/KI3c2su5ltH18PIExD25kwodeAWL4fYb7zzoTpZ382lUK+PAg75zKvEteYK+oQYFh8Powww2Jh+f0WTAKaFlmPLm8ehJ1zmZdHOqK5pCmJxxnFHMYIMyZOTWxvlVjSaDHQKj5vy9qLMsyPZWXmF+acc9lXemd3WSLFUJJd47JnLYEJkpJT5WJmJqnSF1vwnrBzLtNC2rfi6QgzWxD/LgUeJ6w7uaQwzRD/Lo3VF/DTghAA7WJZmXkQds5lWympiHxGR8TVcZoUPgf2AWYAY4CTYrWTgNHx+RjgxDhKohfwZXIl5rLwdIRzLvsqPky4FfB4nI2tPvCgmY2TNBl4RNJpwEfA0bH+WGB/YC5hhfZTyntiD8LOuYyr+DpyZvYBsG0x5Z8CexZTbsCZFTpp5EHYOZdpWVhHLhcPws657MtwFPYg7JzLvJq+rH0uHoSdc5mX3RDsQdg5l3XyJe+dcy41vryRc86lLMMx2IOwcy77/MKcc86lKbsx2IOwcy7bVIbVM2oiD8LOuczL8hpzHoSdc9mX3RjsQdg5l32ejnDOudRUeB25VHkQds5lWtZv1vCVNZxzmSflfpS+v9pL+rekWZJmSjonll8uaYGkafGxf2KfCyXNlTRb0r7lbbv3hJ1zmVcJ6YhVwJ/N7M24zNFUSRPithvN7Pq1zid1BY4FtgbaAM9J+oWZrS7rib0n7JzLtMJxwhVZY87MFpnZm/H518C75F7C/hDgITNbYWYfEpY52rE87fcg7JzLPpXygOaSpiQeZ5R4KKkT0AN4PRadJWm6pCGSNoplbYGPE7vNJ3fQLpEHYedc5uWx5P0yM9s+8bi72ONI6wOjgHPN7CvgDmBzoDuwCLihstvuOWHnXOZVxjhhSQ0IAfhfZvYYgJktSWy/B3gqvlwAtE/s3i6WlZn3hJ1z2Vd6OiL37mFW+PuAd83sH4ny1olqhwEz4vMxwLGSGknaFOgMvFGepntP2DmXaaJSprLsDfwGeEfStFh2EdBPUnfAgHnA7wDMbKakR4BZhJEVZ5ZnZASAzKyCbXdVQdInwEdpt6McmgPL0m5EHZLVz7ujmbWojANJGkf4HHJZZmZ9K+N8lc2DsKtUkqaY2fZpt6Ou8M87+zwn7JxzKfIg7JxzKfIg7CpbseMvXZXxzzvjPCfsnHMp8p6wc86lyIOwc86lyIOwc86lyIOwc86lyIOwq7Ek1Yt/N5HUOO321DaSCoq8zvAiQdnlQdjVOJI2ldTbzFZLOgiYCAyWdHXabasNJK0LYGY/StpO0hGS1jEfKpUKH6LmahxJ/YDbgDOAPYDRwBfAH4BPzeycFJuXaZKaApcBTwA/AMOAhcB3wF+BaWa2Kr0W1j3eE3Y1jpmNAM4CbgQam9l4YCpwFdBM0l1pti/j1iNMTn4MYZawQ8ysD/AWcDbQXZLPrliNPAi7GqMwJymps5k9CJwL7CGpT+yd/RcYCDSNCy26MpAkM1sADCesobYFsBOAmV0E/A8YAPRMrZF1kAdhV2OYmUk6GLhHUnczGwVcDtwr6ddm9iMheJxqZrPSbGvWxABskvYirALxEHAP0FvSfgBmdgnwPrAivZbWPZ4TdjVG7N0+AJxhZlMT5ScC1wH9zOyFtNqXdTHY3gicY2bjJbUnrBq8NTDWzJ5MtYF1lOd+XE2yIfC/wgAsqYGZrTSz+yWtIqxu4Mohjog4F/g/M/t37Bl/LOlJoBFwmKRJhMnP/XOuRh6EXWoSP5ELYqphIfC9pK2AOWa2UtJuQA8zuzm5T5rtzqh6QEPCZwwh8H4PfA78E9jAzD5JqW11mueEXSoSAfhA4GpJNxCGTC0FzgT6SzqEECBmFu7nATg/iYucHSU1MrOvgfHAQEkbmdn38QtuHICZzUuvtXWb94RdKmIA3h24AjgWeIaQbvgLcCqwObADcJaZPZdaQzMqfr77AxcDL0lqCQwGNgBelfRP4CTgIjP7LMWm1nl+Yc6lRtLlwCuE4HsVcJyZfZjY3tjMvkupeZkWL3I+CBxM+GXREzjCzL6SdAzhV8cyM5voKZ50eU/YpWkR4a641sAJZvahpFOADmb2N3yoVJklAuo6hCC8BdAHOD4G4O2Bx8xsZeE+HoDT5TlhVy0SOcpekvaUtB3wLLANcC/wUSz7E/A6hLkN0mpv1iQm3ynsWP0POI5wW3JfM5sbxwhfCGyUQhNdCTwd4aqNpH0J41SvA+4Dtgc6AKcRer2tgOvMbIz/RM5f4iLn3sDRwJvAXKAFIR3xIjCPcLfhZWY2OqWmumJ4OsJVudhLawacAxwKtCeMeFhsZm9K+jdhCFUTM/vIA3DZxAC8B3ATYSzwxYS5IK4nDEk7l9AzvsTMnvLPt2bxnrCrNpIuBb4BjgRONrP/SjoOeMfM3km3ddkV510+C3gDWAXcBRxsZvMlrWtmyxN1PQDXMN4TdlUi8RO5FfB1DATNCL20FvEiUU/gfOD0NNuadXHe5c8Jc0GsAPY3s8VxLua2ku4tnJ7SA3DN40HYVYnEjRiDgLckrTKzkyRtDgyTNI9w1f5yM5uSYlMzJ/EF1wPYlHAhczowGZgXA/COhBzwn31+4JrN0xGuSkjampCLHEEIEHcC65rZ/vFOuAJgkZlN8p/IZRcvwt1OmFXOgJcIY383A3oDK4FBZjYmtUa6vHgQdpVO0sbA28A7hBsElsfyp4CRZjYszfZlXZxb42bgAjN7K36pbQdMNrMnJXUEvjOzpf4FV/P5OGFXKRLjgDuZ2adAf6AzsHei2uvA+ik0L/MS44ABdidMP7kbQBxythw4Mb7+yMyWxucegGs4zwm7CkvkKA8G/izprDgUah3gJkk7AFMIcxWcmWpjMyjx+e4JfEqYcxlgR0lHxMnvXwJ2lrSBmX2VWmNdmXkQdhUWA8TOwN8I8z+8K2lDM3tU0iLgYcLY4IPiNv+JXAaJL7hrgfPNbJqkUYRc8F/jts2Bv3sAzh4Pwq6yNCf0dtvEO+P2l7SaMPzsDMKNBB0JF5JcGUhqDlwAHBbHVm8DbAw8RrjJpTfwsK+MkU0ehF25JH4iNyf8RP4vsIQwXeIgwhSVfYDOZjZWUjPgWkmvmNk3abU7o+oRJmDvK2kAIa++G3AeYW6IH4DdJc0xs3HpNdOVh4+OcOUWfwafAswnjFF9ClhpZl/HGzGGA6eb2auxfpM4ubjLIfEFty0h+H5CGP1wEPC0hfXhjgb2MLP+kjoAewLjzGxRei135eFB2JVLnBLxHmA/4A5AhFm7DNiWsCLGX+KQqQIz+9FzwflTWJRzEDCUMNH9zmb2Qdy2O3Ar4UaMcbGsnpmtTqm5rgI8HeHyUkwAbUWYgrIrYT7gfma2PPbKPgGOMrMZcb8fwYdL5SMORWtLuL37YMJMc4uAb+K21sAlhDHC4wr/XTwAZ5f3hF2p4lCz/c3ssfgTeQvgfcINAxvFbfMlHQYcCPwhOWmMy01SA6C+mX0XP+uGhBnnPiBMzHNSvCB3CGEO5sZm9pn/sqgdvCfs8rES6CBpdnx+MOFi3DvAl0BXSZ0IQ9Qu9gCcP0n1gT2Ab+OdbrsS0g/7EJYk2sjMfpC0EzAAmG1m74H/sqgtvCfs8hInixkNfGJm2yXKfkW4g2slMNx8QvYyi3MBXw1sApxnZqMkbUJYHfk1wsiT3xAmO/IJ2WsZD8KuRMlgGn8ytyPcjrwTIef7iaT2ZvZx4by1HoDzV+TzHUr4fG8E3jKzhZKaEJZ7Wga8a2Yv+Odb+3gQdsVKDJM6ANgZWG1ml0kqAP5BuGB0DeE25N+Z2fwUm5s5ic+3HbAAaERIRZwKjDWz4ZJaAA3MbGGabXVVyyfwccWKAWJ/QqAdBZwk6VFgQzM7lzBXwQXA7R6Ayy7xBTeS8BmfBbxMmBdiP0nXAe8Rbvd2tZj3hF2xJDUmjAO+HmgDXERYmqgR4fbZLyQ1jX/9J3IZSdqVMB/wYYSUQy9gIuGLrSvQA/jIzJ5PrZGuWngQdmsU3lSReL0h0JLQO9s9DqH6AniaMGzKV2wog+QNFXG42X+BTsBVwGWEOTb+B/zNzD5J7OdfcrWYD1Fzhb3eVWa2UlJvwg0BH5rZVElNCTcLtJe0HmHSmCEegPNXeLu2hbXgdicE3pmEz/V3wKlm9rakI4GmhC++NUHYA3Dt5kG4jlNYBeN8YEwMxsMIecp7JZ0Q5wWeC1xJmK3rVDN7xXtn+ZG0LvC0pMGE1UZuA2YRLsLNJFz0XCCpIbAVcJqZzUyrva76eTqijotDzwYRZuoqAB43s+fj3W/DgAPN7GVJXQlrxPminGUUP8sBwGfAgNjrPY7QI25DGGv9PjDCzEam1lCXCg/CdVhiYp0GhPkIdieMhLg75n8PBx4FDjVfMLJCFBbmfAS4xsyui3fKHQN0IcyUdqffilw3+RC1OiwG4AIzW0m4ODSBMC/EDpIamtljwNHAijTbWRuY2QTCtJ8nS+oXc+oPAbMJvz4+i/U8ANcx3hOuo4rcrVXfzFbFvOSlQBNgDDDRzH4oWt+VXxx7fSUw2HzVaYf3hOucOB0iJP7tYwBuEAPuFYSVGo4gsTKyB+DKYWZjCRMdXSCpTbwD0dVh3hOuQxK3yu5FmBDmA+B9MxsetzeIw9QaAp3M7L9ptrc2k9QiORbY1V3+LVyHxAD8a+AW4EXCnAVnSvpz3L4y5oh/8ABctTwAu0I+TrjuaQfcY2b/BJD0OnCdpHFmNjN5x5xzrup5T7iWS+SACzUGTki8nklYJdnzUs6lwINwLVeYgpD0e0ldzexe4HVJzyssQ789sA3QIN2WOlc3+YW5WipxEW4nYAjhVtnlwCvAvwh3yXUCNgau9ZsxnEuHB+FaTNKOhCFnfzGz6ZL6EaZMnG5m98XhUU39Ti3n0uPpiNqtKbAXsHd8PRJ4Fegl6RxAwOfg44CdS4uPjqjFzOzZOP/DtZIWmtmIuDpGPeDtwrltnXPp8SBcy1lY/XgVcGWcD2IYMCLtdjnnAs8J1xGSDgYGEtITi308sHM1gwfhOsRvlXWu5vEg7JxzKfLREc45lyIPws45lyIPws45lyIPws45lyIPwi4VklZLmiZphqSRcWn48h5rqKQj4/N748rQJdXtI2mXcpxjnqTm+ZYXqfNNGc91uaTzytpGl00ehF1avjOz7mbWjbCcUv/kxrgacZmZ2W/NbFaOKn2AMgdh56qKB2FXE0wEtoi91ImSxgCzJNWTdJ2kyZKmS/odhBniJN0qabak54CWhQeS9KKk7ePzvpLelPR2nLqzEyHY/zH2wn8lqYWkUfEckyX1jvtuLOlZSTMl3UuYZyMnSU9Imhr3OaPIthtj+fOSWsSyzSWNi/tMlLRlZXyYLlv8tmWXqtjj3Q8YF4t6At3M7MMYyL40sx0kNQJelfQs0APoAnQFWhGm6RxS5LgtgHuA3eKxmsXZ4u4EvjGz62O9B4EbzewVSR2A8cBWwGXAK2Z2haQDgNPyeDunxnM0BiZLGmVmnwLrAVPM7I+SLo3HPgu4G+hvZnPilKO3A3uU42N0GeZB2KWlsaRp8flE4D5CmuANM/swlu8DbFOY7wU2BDoDuwEj4gRECyW9UMzxewEvFx7LzD4roR17AV0TC5BsIGn9eI7D475PS/o8j/d0tqTD4vP2sa2fAj8CD8fy4cBj8Ry7ACMT526UxzlcLeNB2KXlOzPrniyIwejbZBHwBzMbX6Te/pXYjgKgl5l9X0xb8iapDyGg72xmyyW9CKxTQnWL5/2i6Gfg6h7PCbuabDzwf5IaAEj6haT1gJeBY2LOuDWwezH7TgJ2k7Rp3LdZLP8aaJKo9yzwh8IXkgqD4svAcbFsP2CjUtq6IfB5DMBbEnrihQqAwt78cYQ0x1fAh5KOiueQpG1LOYerhTwIu5rsXkK+901JM4C7CL/eHgfmxG33A68V3TFOVHQG4af/2/yUDngSOKzwwhxwNrB9vPA3i59GafyNEMRnEtIS/yulreOA+pLeJcxWNymx7Vtgx/ge9iCsdgJwPHBabN9M4JA8PhNXy/gEPs45lyLvCTvnXIo8CDvnXIo8CDvnXIo8CDvnXIo8CDvnXIo8CDvnXIo8CDvnXIr+H2+lF54nDalxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}