{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "34d15468-71bf-47f4-bf8a-8906885c6495"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4503f1-adfc-4515-aca8-80aa474049ef"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.865109086999837\n",
            "GPU (s):\n",
            "0.03686835300004532\n",
            "GPU speedup over CPU: 77x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfde6050-984f-4b50-9840-c3a8006dcfbe"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab360b4-148b-4e66-8280-c1d2e39ceaec"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(60, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1404 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1404.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1404.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42cb4162-2a17-428b-d89f-c6ee8b1f21be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 80)       258880      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 80)          0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 60)           4860        ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 60)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            61          ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 134,997,881\n",
            "Trainable params: 134,997,881\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "za4Z4HXhGQPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 9\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86fa4b26-19a2-41bf-b2db-c21421a37603"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1404.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e6cb8bc-7f15-457d-fe16-870c7a3be3c5"
      },
      "source": [
        "model1404.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs = 9,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9\n",
            "157/157 [==============================] - 124s 708ms/step - loss: 0.6201 - binary_accuracy: 0.6530 - metrics_recall: 0.2146 - metrics_precision: 0.3918 - metrics_f1: 0.2264\n",
            "Epoch 2/9\n",
            "157/157 [==============================] - 112s 714ms/step - loss: 0.5203 - binary_accuracy: 0.7387 - metrics_recall: 0.4960 - metrics_precision: 0.6707 - metrics_f1: 0.5318\n",
            "Epoch 3/9\n",
            "157/157 [==============================] - 112s 714ms/step - loss: 0.4275 - binary_accuracy: 0.8099 - metrics_recall: 0.6565 - metrics_precision: 0.7598 - metrics_f1: 0.6820\n",
            "Epoch 4/9\n",
            "157/157 [==============================] - 112s 713ms/step - loss: 0.3544 - binary_accuracy: 0.8539 - metrics_recall: 0.7531 - metrics_precision: 0.8094 - metrics_f1: 0.7651\n",
            "Epoch 5/9\n",
            "157/157 [==============================] - 111s 710ms/step - loss: 0.3021 - binary_accuracy: 0.8784 - metrics_recall: 0.8027 - metrics_precision: 0.8306 - metrics_f1: 0.8070\n",
            "Epoch 6/9\n",
            "157/157 [==============================] - 113s 717ms/step - loss: 0.2550 - binary_accuracy: 0.9070 - metrics_recall: 0.8487 - metrics_precision: 0.8846 - metrics_f1: 0.8549\n",
            "Epoch 7/9\n",
            "157/157 [==============================] - 112s 716ms/step - loss: 0.2206 - binary_accuracy: 0.9205 - metrics_recall: 0.8620 - metrics_precision: 0.8984 - metrics_f1: 0.8728\n",
            "Epoch 8/9\n",
            "157/157 [==============================] - 112s 711ms/step - loss: 0.1918 - binary_accuracy: 0.9353 - metrics_recall: 0.8941 - metrics_precision: 0.9127 - metrics_f1: 0.8984\n",
            "Epoch 9/9\n",
            "157/157 [==============================] - 111s 708ms/step - loss: 0.1695 - binary_accuracy: 0.9443 - metrics_recall: 0.9070 - metrics_precision: 0.9276 - metrics_f1: 0.9124\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f313b878250>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1404.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa32886-adcf-4895-dc20-aea3e79170c6"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f54796e-ef14-4a3f-a49f-1d5c30d35ca2"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02421961],\n",
              "       [0.47706294],\n",
              "       [0.9603392 ],\n",
              "       ...,\n",
              "       [0.9481454 ],\n",
              "       [0.05774587],\n",
              "       [0.05143072]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92a207f-e341-4b11-a3c3-d65cd9f6c10c"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7664212910532276"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "84065f3d-cf98-4adf-ca6d-1b40547c6e89"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2107  223]\n",
            " [ 602  600]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}