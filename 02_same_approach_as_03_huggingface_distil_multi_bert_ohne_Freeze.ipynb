{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "cbd51451-f198-499f-b652-5a5e387466ce"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c710fa-6820-4656-dc05-dac57a6b9b36"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.791398167999887\n",
            "GPU (s):\n",
            "0.03697592000003169\n",
            "GPU speedup over CPU: 75x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e29db0f-0e1b-4d85-df60-ade0f32cef7e"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62c07cf-a13a-4e33-a886-0e88d74ca54f"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(230, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "#X= tf.keras.layers.LSTM(80, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(250, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1502 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1502.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1502.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7b536e-bcb4-4237-a939-3d12feb766cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 460)      1838160     ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 460)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 250)          115250      ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 250)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            251         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 136,687,741\n",
            "Trainable params: 136,687,741\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 7\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8a25c7-5f7c-4c10-f771-27b8aa332633"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1502.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb30368-d4ac-4d99-8664-5643d897aa7e"
      },
      "source": [
        "model1502.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =7,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "157/157 [==============================] - 132s 761ms/step - loss: 0.5983 - binary_accuracy: 0.6824 - metrics_recall: 0.1763 - metrics_precision: 0.4682 - metrics_f1: 0.2293\n",
            "Epoch 2/7\n",
            "157/157 [==============================] - 120s 763ms/step - loss: 0.4821 - binary_accuracy: 0.7664 - metrics_recall: 0.5671 - metrics_precision: 0.6944 - metrics_f1: 0.5973\n",
            "Epoch 3/7\n",
            "157/157 [==============================] - 119s 760ms/step - loss: 0.3993 - binary_accuracy: 0.8183 - metrics_recall: 0.6982 - metrics_precision: 0.7503 - metrics_f1: 0.7074\n",
            "Epoch 4/7\n",
            "157/157 [==============================] - 120s 762ms/step - loss: 0.3305 - binary_accuracy: 0.8611 - metrics_recall: 0.7752 - metrics_precision: 0.8102 - metrics_f1: 0.7815\n",
            "Epoch 5/7\n",
            "157/157 [==============================] - 119s 760ms/step - loss: 0.2818 - binary_accuracy: 0.8868 - metrics_recall: 0.8243 - metrics_precision: 0.8429 - metrics_f1: 0.8234\n",
            "Epoch 6/7\n",
            "157/157 [==============================] - 118s 754ms/step - loss: 0.2401 - binary_accuracy: 0.9038 - metrics_recall: 0.8464 - metrics_precision: 0.8678 - metrics_f1: 0.8485\n",
            "Epoch 7/7\n",
            "157/157 [==============================] - 118s 752ms/step - loss: 0.2091 - binary_accuracy: 0.9213 - metrics_recall: 0.8760 - metrics_precision: 0.8930 - metrics_f1: 0.8771\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1320aee850>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1502.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e51f36-5cbb-4331-efd0-3f2046321ed8"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bfb717-122a-4a17-d5a2-6ff3094cd315"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.04298594],\n",
              "       [0.80663526],\n",
              "       [0.95433104],\n",
              "       ...,\n",
              "       [0.9470099 ],\n",
              "       [0.11465962],\n",
              "       [0.06083994]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1495bd22-2e99-4be9-b01f-fbd3ad098b84"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7627406568516422"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "0c99f6b8-779b-4b25-de7e-2ac00206c513"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2053  277]\n",
            " [ 561  641]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}