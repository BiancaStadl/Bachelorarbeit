{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "c069ff43-3c88-40c0-b611-b2d0df32cd1c"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579ec44a-6796-4e25-bce9-a9a61f92d379"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.8506540859998495\n",
            "GPU (s):\n",
            "0.036900858000080916\n",
            "GPU speedup over CPU: 77x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd732a9e-e241-41e3-8e16-3829d9f4f0e3"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fab0103-f858-4583-9887-f5fb0204543d"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(230, return_sequences=True))(embedding_layer)\n",
        "#X= tf.keras.layers.LSTM(150, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(260, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1417 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1417.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_projector', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1417.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331e52ef-b660-4429-f525-a4ec70906fe5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 460)      1838160     ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 460)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 260)          119860      ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 260)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            261         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 136,692,361\n",
            "Trainable params: 136,692,361\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 6\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53728640-4e7c-4d40-886c-462791e34767"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1417.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26252cd8-47c9-4400-d900-dd7c5f98cbf5"
      },
      "source": [
        "model1417.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =6,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "157/157 [==============================] - 53s 265ms/step - loss: 0.5971 - binary_accuracy: 0.6892 - metrics_recall: 0.2214 - metrics_precision: 0.4335 - metrics_f1: 0.2668\n",
            "Epoch 2/6\n",
            "157/157 [==============================] - 42s 267ms/step - loss: 0.4772 - binary_accuracy: 0.7764 - metrics_recall: 0.5961 - metrics_precision: 0.7093 - metrics_f1: 0.6208\n",
            "Epoch 3/6\n",
            "157/157 [==============================] - 42s 265ms/step - loss: 0.3944 - binary_accuracy: 0.8223 - metrics_recall: 0.7096 - metrics_precision: 0.7613 - metrics_f1: 0.7156\n",
            "Epoch 4/6\n",
            "157/157 [==============================] - 42s 266ms/step - loss: 0.3295 - binary_accuracy: 0.8591 - metrics_recall: 0.7682 - metrics_precision: 0.8080 - metrics_f1: 0.7739\n",
            "Epoch 5/6\n",
            "157/157 [==============================] - 42s 266ms/step - loss: 0.2714 - binary_accuracy: 0.8918 - metrics_recall: 0.8343 - metrics_precision: 0.8445 - metrics_f1: 0.8318\n",
            "Epoch 6/6\n",
            "157/157 [==============================] - 42s 267ms/step - loss: 0.2362 - binary_accuracy: 0.9036 - metrics_recall: 0.8481 - metrics_precision: 0.8648 - metrics_f1: 0.8503\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f930deb8790>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1417.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6a7d5a-f2e8-401e-93a0-fa6ff5087840"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1201960f-ca84-4eda-bc7c-b2160f6955d1"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.04935336],\n",
              "       [0.4627264 ],\n",
              "       [0.9418467 ],\n",
              "       ...,\n",
              "       [0.9598331 ],\n",
              "       [0.27397168],\n",
              "       [0.13846105]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda1b2e9-94e4-4d13-e48c-43c884a8bf30"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7686862967157417"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "37b53335-ddae-49a3-8d17-e21816d335aa"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2098  232]\n",
            " [ 585  617]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}