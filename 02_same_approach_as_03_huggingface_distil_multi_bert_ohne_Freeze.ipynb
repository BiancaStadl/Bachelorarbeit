{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "ec9ec6d8-faaa-41a4-df32-f6591a6c73e5"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12969896-34b1-4f09-c421-a165337da2de"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.9062406020000253\n",
            "GPU (s):\n",
            "0.036326244000065344\n",
            "GPU speedup over CPU: 80x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbebccc2-43c0-4d6a-972c-cd83a8b8116a"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816ad5ea-ba7e-49d6-a8da-a48c52c7bfcf"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(90, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1403 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1403.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_projector', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1403.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a559c4b-f487-4cd2-8480-88dee31edc15"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 80)       258880      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 80)          0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 90)           7290        ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 90)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            91          ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,000,341\n",
            "Trainable params: 135,000,341\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 7\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d01862-0a8d-4d5c-99e9-f1ad865e2cbe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1403.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e957341-69bd-4416-827f-e46a7ef9ba1d"
      },
      "source": [
        "model1403.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs = 7,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "157/157 [==============================] - 125s 716ms/step - loss: 0.6065 - binary_accuracy: 0.6758 - metrics_recall: 0.1479 - metrics_precision: 0.3806 - metrics_f1: 0.1813\n",
            "Epoch 2/7\n",
            "157/157 [==============================] - 112s 716ms/step - loss: 0.4893 - binary_accuracy: 0.7644 - metrics_recall: 0.5677 - metrics_precision: 0.6854 - metrics_f1: 0.6020\n",
            "Epoch 3/7\n",
            "157/157 [==============================] - 111s 709ms/step - loss: 0.4136 - binary_accuracy: 0.8173 - metrics_recall: 0.6961 - metrics_precision: 0.7563 - metrics_f1: 0.7080\n",
            "Epoch 4/7\n",
            "157/157 [==============================] - 113s 718ms/step - loss: 0.3537 - binary_accuracy: 0.8527 - metrics_recall: 0.7646 - metrics_precision: 0.7995 - metrics_f1: 0.7703\n",
            "Epoch 5/7\n",
            "157/157 [==============================] - 113s 717ms/step - loss: 0.2948 - binary_accuracy: 0.8856 - metrics_recall: 0.8301 - metrics_precision: 0.8347 - metrics_f1: 0.8227\n",
            "Epoch 6/7\n",
            "157/157 [==============================] - 112s 716ms/step - loss: 0.2629 - binary_accuracy: 0.8988 - metrics_recall: 0.8406 - metrics_precision: 0.8577 - metrics_f1: 0.8425\n",
            "Epoch 7/7\n",
            "157/157 [==============================] - 112s 711ms/step - loss: 0.2344 - binary_accuracy: 0.9158 - metrics_recall: 0.8771 - metrics_precision: 0.8753 - metrics_f1: 0.8719\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efbff738690>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1403.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7e5823-df44-467b-ad38-fa4bc2d5ac54"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6304e3c2-7e6a-4cb3-a609-35fc24294c6d"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.04594834],\n",
              "       [0.39070752],\n",
              "       [0.91720885],\n",
              "       ...,\n",
              "       [0.9039297 ],\n",
              "       [0.5236951 ],\n",
              "       [0.03422109]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a886570-deba-432c-940a-af9adb593549"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7661381653454133"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "e2960059-9e56-453b-fe53-ea3eef2916a8"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2120  210]\n",
            " [ 616  586]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}