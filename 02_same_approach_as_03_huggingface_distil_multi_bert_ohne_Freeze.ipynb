{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "0135e148-37eb-4d79-b502-730279ee1de3"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ddb078-3c14-4b1a-af23-09fdf5e01f65"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.8387912929997583\n",
            "GPU (s):\n",
            "0.03612902299983034\n",
            "GPU speedup over CPU: 78x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193626ab-2ddf-4f99-e3ac-b4c0b0590922"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4cbb310-6196-4ae5-8bf0-4de5f529b352"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "#X= tf.keras.layers.LSTM(80, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(350, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1503 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1503.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1503.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a454bf0-1b0c-4693-9144-a64e127a9f6d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 600)      2565600     ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 600)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 350)          210350      ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 350)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            351         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 137,510,381\n",
            "Trainable params: 137,510,381\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 7\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eca0afd-29a7-4232-9b53-153cedf5fb43"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1503.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14020760-e661-4e96-ea8a-d1e3219279ec"
      },
      "source": [
        "model1503.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =7,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "157/157 [==============================] - 134s 777ms/step - loss: 0.5999 - binary_accuracy: 0.6882 - metrics_recall: 0.1746 - metrics_precision: 0.3800 - metrics_f1: 0.2140\n",
            "Epoch 2/7\n",
            "157/157 [==============================] - 122s 780ms/step - loss: 0.4788 - binary_accuracy: 0.7706 - metrics_recall: 0.6016 - metrics_precision: 0.7016 - metrics_f1: 0.6185\n",
            "Epoch 3/7\n",
            "157/157 [==============================] - 122s 780ms/step - loss: 0.3865 - binary_accuracy: 0.8281 - metrics_recall: 0.7187 - metrics_precision: 0.7743 - metrics_f1: 0.7248\n",
            "Epoch 4/7\n",
            "157/157 [==============================] - 123s 781ms/step - loss: 0.3180 - binary_accuracy: 0.8676 - metrics_recall: 0.7902 - metrics_precision: 0.8300 - metrics_f1: 0.7962\n",
            "Epoch 5/7\n",
            "157/157 [==============================] - 124s 788ms/step - loss: 0.2617 - binary_accuracy: 0.8928 - metrics_recall: 0.8276 - metrics_precision: 0.8534 - metrics_f1: 0.8306\n",
            "Epoch 6/7\n",
            "157/157 [==============================] - 124s 790ms/step - loss: 0.2191 - binary_accuracy: 0.9142 - metrics_recall: 0.8666 - metrics_precision: 0.8742 - metrics_f1: 0.8643\n",
            "Epoch 7/7\n",
            "157/157 [==============================] - 123s 784ms/step - loss: 0.1960 - binary_accuracy: 0.9247 - metrics_recall: 0.8856 - metrics_precision: 0.8864 - metrics_f1: 0.8808\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5815348b90>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1503.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421b4ba0-6e46-412b-8032-3e91d8570c96"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce084df1-3f4b-42c2-c51c-ad2e96dbbebe"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01861538],\n",
              "       [0.32925606],\n",
              "       [0.9126    ],\n",
              "       ...,\n",
              "       [0.9652474 ],\n",
              "       [0.19909129],\n",
              "       [0.01570297]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdc2a1f-d2ff-45a3-80b4-b7a2152834c6"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.766704416761042"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "9366cd5e-c023-4133-975b-9c393adefd22"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2109  221]\n",
            " [ 603  599]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}