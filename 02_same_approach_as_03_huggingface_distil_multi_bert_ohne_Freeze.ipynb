{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "6cdcfd84-cf5e-4e90-9f07-5d94baabf679"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2dcdd2-bd24-440b-9fa4-bbf899b652a8"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.8432988910008135\n",
            "GPU (s):\n",
            "0.04036294999968959\n",
            "GPU speedup over CPU: 70x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a7e6b1-e6f8-4d57-f463-9e961a91be29"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2cbe44-5e8c-499f-d50f-ced679a5c63d"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "#X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X= tf.keras.layers.LSTM(150, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(160, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1414 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1414.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1414.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5897be23-5d70-4994-bbe3-57371e38e7fe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 60, 150)      551400      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 150)         0           ['lstm[0][0]']                   \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 160)          24160       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 160)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            161         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,309,801\n",
            "Trainable params: 135,309,801\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 8\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca53d87-d6e2-4150-9665-5b54c126d69c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1414.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07964cec-66a1-4f74-9ecf-b928d19d1a83"
      },
      "source": [
        "model1414.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =8,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "157/157 [==============================] - 49s 246ms/step - loss: 0.6046 - binary_accuracy: 0.6780 - metrics_recall: 0.2129 - metrics_precision: 0.4131 - metrics_f1: 0.2442\n",
            "Epoch 2/8\n",
            "157/157 [==============================] - 39s 251ms/step - loss: 0.4868 - binary_accuracy: 0.7622 - metrics_recall: 0.5701 - metrics_precision: 0.6976 - metrics_f1: 0.5970\n",
            "Epoch 3/8\n",
            "157/157 [==============================] - 41s 260ms/step - loss: 0.4040 - binary_accuracy: 0.8163 - metrics_recall: 0.6948 - metrics_precision: 0.7575 - metrics_f1: 0.7067\n",
            "Epoch 4/8\n",
            "157/157 [==============================] - 41s 260ms/step - loss: 0.3304 - binary_accuracy: 0.8589 - metrics_recall: 0.7686 - metrics_precision: 0.8056 - metrics_f1: 0.7729\n",
            "Epoch 5/8\n",
            "157/157 [==============================] - 41s 258ms/step - loss: 0.2719 - binary_accuracy: 0.8904 - metrics_recall: 0.8286 - metrics_precision: 0.8508 - metrics_f1: 0.8293\n",
            "Epoch 6/8\n",
            "157/157 [==============================] - 41s 259ms/step - loss: 0.2278 - binary_accuracy: 0.9134 - metrics_recall: 0.8632 - metrics_precision: 0.8834 - metrics_f1: 0.8675\n",
            "Epoch 7/8\n",
            "157/157 [==============================] - 41s 259ms/step - loss: 0.1986 - binary_accuracy: 0.9263 - metrics_recall: 0.8796 - metrics_precision: 0.9004 - metrics_f1: 0.8851\n",
            "Epoch 8/8\n",
            "157/157 [==============================] - 41s 258ms/step - loss: 0.1687 - binary_accuracy: 0.9429 - metrics_recall: 0.9084 - metrics_precision: 0.9275 - metrics_f1: 0.9136\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe278dc2d10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1414.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a139a936-5588-4cdd-ade9-67e092c23a0d"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454a1956-a8b8-47ba-ae19-1b1996a3a1d2"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02172681],\n",
              "       [0.20368032],\n",
              "       [0.9524294 ],\n",
              "       ...,\n",
              "       [0.97528017],\n",
              "       [0.1428274 ],\n",
              "       [0.01510761]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa07697-f52c-4a46-8bd3-b2c77f2ec712"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7647225368063421"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "69046f91-02c1-4bad-9e4a-8d1edaf7d057"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2099  231]\n",
            " [ 600  602]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}