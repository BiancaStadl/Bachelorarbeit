{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "d8167633-88cc-4742-e0ba-bcb38bf2e457"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271e9303-8518-4933-d51c-728d9423297a"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.8817562340000222\n",
            "GPU (s):\n",
            "0.03688953999972\n",
            "GPU speedup over CPU: 78x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eebe482c-98f7-4fe3-db4d-5208dfa2e3aa"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297bc7cf-6e2d-4423-ad73-dde54f7a014c"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "#X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X= tf.keras.layers.LSTM(90, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(100, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1411 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1411.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1411.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138d83df-b986-4352-eb37-88c9fda0f955"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 60, 90)       309240      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 90)          0           ['lstm[0][0]']                   \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100)          9100        ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 100)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            101         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,052,521\n",
            "Trainable params: 135,052,521\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 8\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f4eec8-4f08-41c4-a3ff-c84ccbdd8a99"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1411.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9182c4c5-ef86-4afc-e3f0-d9119c5722f1"
      },
      "source": [
        "model1411.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =8,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "157/157 [==============================] - 48s 243ms/step - loss: 0.6037 - binary_accuracy: 0.6740 - metrics_recall: 0.1919 - metrics_precision: 0.3774 - metrics_f1: 0.2256\n",
            "Epoch 2/8\n",
            "157/157 [==============================] - 39s 247ms/step - loss: 0.4956 - binary_accuracy: 0.7564 - metrics_recall: 0.5616 - metrics_precision: 0.6834 - metrics_f1: 0.5800\n",
            "Epoch 3/8\n",
            "157/157 [==============================] - 39s 250ms/step - loss: 0.4208 - binary_accuracy: 0.8087 - metrics_recall: 0.6696 - metrics_precision: 0.7581 - metrics_f1: 0.6860\n",
            "Epoch 4/8\n",
            "157/157 [==============================] - 40s 252ms/step - loss: 0.3394 - binary_accuracy: 0.8537 - metrics_recall: 0.7545 - metrics_precision: 0.8088 - metrics_f1: 0.7665\n",
            "Epoch 5/8\n",
            "157/157 [==============================] - 40s 253ms/step - loss: 0.2842 - binary_accuracy: 0.8866 - metrics_recall: 0.8217 - metrics_precision: 0.8421 - metrics_f1: 0.8218\n",
            "Epoch 6/8\n",
            "157/157 [==============================] - 40s 254ms/step - loss: 0.2447 - binary_accuracy: 0.9078 - metrics_recall: 0.8468 - metrics_precision: 0.8757 - metrics_f1: 0.8530\n",
            "Epoch 7/8\n",
            "157/157 [==============================] - 40s 255ms/step - loss: 0.2086 - binary_accuracy: 0.9257 - metrics_recall: 0.8733 - metrics_precision: 0.8981 - metrics_f1: 0.8810\n",
            "Epoch 8/8\n",
            "157/157 [==============================] - 40s 255ms/step - loss: 0.1901 - binary_accuracy: 0.9345 - metrics_recall: 0.8963 - metrics_precision: 0.9044 - metrics_f1: 0.8954\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa929cd4810>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1411.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "821307f8-c8be-485f-9835-9c5fa04b2307"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32b4b59a-1dda-4386-a064-f82bbb9f116a"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0265135 ],\n",
              "       [0.927402  ],\n",
              "       [0.9637916 ],\n",
              "       ...,\n",
              "       [0.96481407],\n",
              "       [0.32988235],\n",
              "       [0.01855315]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318dc483-1776-4301-dba7-99a86766f5e9"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7627406568516422"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "3d01b166-5a62-473f-b339-50fc166f59fe"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2066  264]\n",
            " [ 574  628]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}