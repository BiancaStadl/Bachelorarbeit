{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "70b018a6-cfdb-435c-c8ef-0ace42e0bcb1"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a639187-6fd0-40db-b5d1-d3a150845576"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.0759233830003723\n",
            "GPU (s):\n",
            "0.04066990800038184\n",
            "GPU speedup over CPU: 75x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e73406-7673-4487-f2af-de357822f66c"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac310f7-3faf-4ff8-e405-88657b2c4ccc"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "#X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X= tf.keras.layers.LSTM(90, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(100, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1408 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1408.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_projector', 'vocab_layer_norm', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1408.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5bc9396-9028-46f5-b7ff-728b43ff802a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 60, 90)       309240      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 90)          0           ['lstm[0][0]']                   \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100)          9100        ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 100)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            101         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,052,521\n",
            "Trainable params: 135,052,521\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 3\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "#init_lr=2e-5\n",
        "init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2cd742-dedc-481a-d725-4d5b4275084c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1408.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30349361-478b-4747-d8cc-8e0b2b2d6384"
      },
      "source": [
        "model1408.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =3,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "157/157 [==============================] - 48s 245ms/step - loss: 0.5911 - binary_accuracy: 0.6858 - metrics_recall: 0.3378 - metrics_precision: 0.5211 - metrics_f1: 0.3546\n",
            "Epoch 2/3\n",
            "157/157 [==============================] - 39s 249ms/step - loss: 0.4200 - binary_accuracy: 0.8101 - metrics_recall: 0.6639 - metrics_precision: 0.7622 - metrics_f1: 0.6870\n",
            "Epoch 3/3\n",
            "157/157 [==============================] - 40s 252ms/step - loss: 0.2637 - binary_accuracy: 0.8984 - metrics_recall: 0.8210 - metrics_precision: 0.8728 - metrics_f1: 0.8368\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f19c6217d10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1408.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487a916a-d286-4046-c44d-838e97540581"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbfaa623-7a3b-40c1-9208-13806470aa31"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.12412641],\n",
              "       [0.867878  ],\n",
              "       [0.62171656],\n",
              "       ...,\n",
              "       [0.9711239 ],\n",
              "       [0.04352767],\n",
              "       [0.03674977]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc0525a-c97b-4079-e449-e5727fd7ef21"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7655719139297849"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7e42529b-d07b-4986-cb99-35302095fc29"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2090  240]\n",
            " [ 588  614]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}