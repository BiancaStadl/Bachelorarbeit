{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "c71bda41-5dbd-4537-f840-34c9ad3f3382"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63fb2a3-fbe9-4c98-db01-c381468b1271"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.7905427249997956\n",
            "GPU (s):\n",
            "0.03716280599928723\n",
            "GPU speedup over CPU: 75x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6faede-7b3f-45ae-e87e-dc2d2ad94f93"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"üòú\", \" \",string)\n",
        "   string = re.sub(\"üç´\", \" \",string)\n",
        "   string = re.sub(\"üòÅ\", \" \",string)\n",
        "   string = re.sub(\"üêñ\", \" \",string)\n",
        "   string = re.sub(\"üò°\", \" \",string)\n",
        "   string = re.sub(\"üòá\", \" \",string)\n",
        "   string = re.sub(\"üò¨\", \" \",string)\n",
        "   string = re.sub(\"üòÉ\", \" \",string)\n",
        "   string = re.sub(\"üòÇ\", \" \",string)\n",
        "   string = re.sub(\"üíô\", \" \",string)  \n",
        "   string = re.sub(\"üòõ\", \" \",string)\n",
        "   string = re.sub(\"üôè\", \" \",string)\n",
        "   string = re.sub(\"üëç\", \" \",string)\n",
        "   string = re.sub(\"üñï\", \" \",string)\n",
        "   string = re.sub(\"üòâ\", \" \",string)\n",
        "   string = re.sub(\"üí©\", \" \",string)\n",
        "   string = re.sub(\"ü§¢\", \" \",string)\n",
        "   string = re.sub(\"üëè\", \" \",string)\n",
        "   string = re.sub(\"üò®\", \" \",string)\n",
        "   string = re.sub(\"ü§£\", \" \",string)\n",
        "   string = re.sub(\"ü§°\", \" \",string)\n",
        "   string = re.sub(\"üòà\", \" \",string)\n",
        "   string = re.sub(\"üíÉüèΩ\", \" \",string)\n",
        "   string = re.sub(\"üëπ\", \" \",string)\n",
        "   string = re.sub(\"ü§ò\", \" \",string)\n",
        "   string = re.sub(\"üò±\", \" \",string)\n",
        "   string = re.sub(\"ü§î\", \" \",string) \n",
        "   string = re.sub(\"üåà\", \" \",string) \n",
        "   string = re.sub(\"üíï\", \" \",string) \n",
        "   string = re.sub(\"üë©‚Äç‚ù§Ô∏è‚Äçüë©\", \" \",string) \n",
        "   string = re.sub(\"üòç\", \" \",string) \n",
        "   string = re.sub(\"üëÜ\", \" \",string) \n",
        "   string = re.sub(\"üòñ\", \" \",string) \n",
        "   string = re.sub(\"üëá\", \" \",string) \n",
        "   string = re.sub(\"üî•\", \" \",string) \n",
        "   string = re.sub(\"üòò\", \" \",string) \n",
        "   string = re.sub(\"üéâ\", \" \",string) \n",
        "   string = re.sub(\"ü§¨\", \" \",string) \n",
        "   string = re.sub(\"üëä\", \" \",string)\n",
        "   string = re.sub(\"üá©üá™\", \" \",string)  \n",
        "   string = re.sub(\"üíî\", \" \",string)\n",
        "   string = re.sub(\"üôà\", \" \",string)\n",
        "   string = re.sub(\"ü§Ø\", \" \",string)\n",
        "   string = re.sub(\"üêü\", \" \",string)\n",
        "   string = re.sub(\"üõ∂\", \" \",string)\n",
        "   string = re.sub(\"üòä\", \" \",string)\n",
        "   string = re.sub(\"üòì\", \" \",string)\n",
        "   string = re.sub(\"üò≥\", \" \",string)\n",
        "   string = re.sub(\"üöÄ\", \" \",string)\n",
        "   string = re.sub(\"üëé\", \" \",string)\n",
        "   string = re.sub(\"üòé\", \" \",string)\n",
        "   string = re.sub(\"üê∏\", \" \",string)\n",
        "   string = re.sub(\"üìà\", \" \",string)\n",
        "   string = re.sub(\"üôÇ\", \" \",string)\n",
        "   string = re.sub(\"üòÖ\", \" \",string)\n",
        "   string = re.sub(\"üòÜ\", \" \",string)\n",
        "   string = re.sub(\"üôéüèø\", \" \",string)\n",
        "   string = re.sub(\"üëéüèΩ\", \" \",string)\n",
        "   string = re.sub(\"ü§≠\", \" \",string)\n",
        "   string = re.sub(\"üò§\", \" \",string)\n",
        "   string = re.sub(\"üòö\", \" \",string)\n",
        "   string = re.sub(\"üòä\", \" \",string)\n",
        "   string = re.sub(\"üò≤\", \" \",string)\n",
        "   string = re.sub(\"ü§Æ\", \" \",string)\n",
        "   string = re.sub(\"üôÑ\", \" \",string)\n",
        "   string = re.sub(\"ü§ë\", \" \",string)\n",
        "   string = re.sub(\"üéÖ\", \" \",string)\n",
        "   string = re.sub(\"üëã\", \" \",string)\n",
        "   string = re.sub(\"üí™\", \" \",string)\n",
        "   string = re.sub(\"üòÑ\", \" \",string)\n",
        "   string = re.sub(\"üßê\", \" \",string)\n",
        "   string = re.sub(\"üò†\", \" \",string)\n",
        "   string = re.sub(\"üéà\", \" \",string)\n",
        "   string = re.sub(\"üöÇ\", \" \",string)\n",
        "   string = re.sub(\"üòä\", \" \",string)\n",
        "   string = re.sub(\"üöá\", \" \",string)\n",
        "   string = re.sub(\"üöä\", \" \",string)\n",
        "   string = re.sub(\"ü§∑\", \" \",string)\n",
        "   string = re.sub(\"üò•\", \" \",string)\n",
        "   string = re.sub(\"üôÉ\", \" \",string)\n",
        "   string = re.sub(\"üî©\", \" \",string)\n",
        "   string = re.sub(\"üîß\", \" \",string)\n",
        "   string = re.sub(\"üî®\", \" \",string)\n",
        "   string = re.sub(\"üõ†\", \" \",string)\n",
        "   string = re.sub(\"üíì\", \" \",string)\n",
        "   string = re.sub(\"üí°\", \" \",string)\n",
        "   string = re.sub(\"üç∏\", \" \",string)\n",
        "   string = re.sub(\"ü•É\", \" \",string)\n",
        "   string = re.sub(\"ü•Ç\", \" \",string)\n",
        "   string = re.sub(\"üò∑\", \" \",string)\n",
        "   string = re.sub(\"ü§ê\", \" \",string)\n",
        "   string = re.sub(\"üåé\", \" \",string)\n",
        "   string = re.sub(\"üëë\", \" \",string)\n",
        "   string = re.sub(\"ü§õ\", \" \",string)\n",
        "   string = re.sub(\"üòÄ\", \" \",string)\n",
        "   string = re.sub(\"üõ§\", \" \",string)\n",
        "   string = re.sub(\"üéÑ\", \" \",string)\n",
        "   string = re.sub(\"üì¥\", \" \",string)\n",
        "   string = re.sub(\"üå≠\", \" \",string)\n",
        "   string = re.sub(\"ü§ï\", \" \",string)\n",
        "   string = re.sub(\"üò≠\", \" \",string)\n",
        "   string = re.sub(\"üçæ\", \" \",string)\n",
        "   string = re.sub(\"üçû\", \" \",string)\n",
        "   string = re.sub(\"ü§¶\", \" \",string)\n",
        "   string = re.sub(\"ü§Ø\", \" \",string)\n",
        "   string = re.sub(\"üïØÔ∏è\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f32c585-2de2-4be6-e237-45a543c65045"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "#X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X= tf.keras.layers.LSTM(90, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(100, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1410 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1410.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1410.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092e300e-04a9-4919-800f-8d96773bb500"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 60, 90)       309240      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 90)          0           ['lstm[0][0]']                   \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100)          9100        ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 100)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            101         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,052,521\n",
            "Trainable params: 135,052,521\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 6\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fc8f50-bbac-41ee-d660-091b5a424cf7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1410.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218a9a97-52fe-49e4-9504-a6aff4085ed1"
      },
      "source": [
        "model1410.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =6,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "157/157 [==============================] - 48s 245ms/step - loss: 0.6001 - binary_accuracy: 0.6820 - metrics_recall: 0.1352 - metrics_precision: 0.3445 - metrics_f1: 0.1729\n",
            "Epoch 2/6\n",
            "157/157 [==============================] - 39s 249ms/step - loss: 0.5001 - binary_accuracy: 0.7546 - metrics_recall: 0.5309 - metrics_precision: 0.6931 - metrics_f1: 0.5693\n",
            "Epoch 3/6\n",
            "157/157 [==============================] - 39s 251ms/step - loss: 0.4215 - binary_accuracy: 0.8099 - metrics_recall: 0.6881 - metrics_precision: 0.7419 - metrics_f1: 0.6969\n",
            "Epoch 4/6\n",
            "157/157 [==============================] - 40s 253ms/step - loss: 0.3553 - binary_accuracy: 0.8465 - metrics_recall: 0.7640 - metrics_precision: 0.7810 - metrics_f1: 0.7583\n",
            "Epoch 5/6\n",
            "157/157 [==============================] - 40s 254ms/step - loss: 0.3130 - binary_accuracy: 0.8728 - metrics_recall: 0.8079 - metrics_precision: 0.8174 - metrics_f1: 0.8030\n",
            "Epoch 6/6\n",
            "157/157 [==============================] - 40s 255ms/step - loss: 0.2757 - binary_accuracy: 0.8956 - metrics_recall: 0.8398 - metrics_precision: 0.8518 - metrics_f1: 0.8387\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa8142d92d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1410.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3422ca-1b3e-4d67-a0ce-651467ee53f7"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97743836-0d85-49f8-a450-eae0b1f511fe"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03908628],\n",
              "       [0.6471417 ],\n",
              "       [0.8214789 ],\n",
              "       ...,\n",
              "       [0.85470533],\n",
              "       [0.36266723],\n",
              "       [0.02570954]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eccebd2a-fa15-4b7e-8a61-42719c04e85b"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7638731596828993"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "16f65040-8a3d-4da6-b44a-5d8c3cb39cf0"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2081  249]\n",
            " [ 585  617]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}