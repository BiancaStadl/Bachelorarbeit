{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow"
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "f79ad4dc-68d8-45a4-e0ed-009a4a9c4679"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "949fce82-95ee-4fc8-f58d-0c86223f1f81"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.9120082920001096\n",
            "GPU (s):\n",
            "0.04156263699996998\n",
            "GPU speedup over CPU: 70x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46f7f14-5ebe-4bde-d6f2-7cd51ac7c4f9"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7853b7-fbe2-4bdf-f751-11802bb50be6"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(90, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1402 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1402.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1402.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59acf8e9-7fb6-447f-b0d9-f7ebab0ca1ec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 320)      1189120     ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 320)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 90)           28890       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 90)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            91          ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,952,181\n",
            "Trainable params: 135,952,181\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 4\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "#init_lr=2e-5\n",
        "init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d926d16-751c-4611-db94-9afda016b8d8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1402.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1c9e55-073a-4fb1-ccc5-2c5e940a7618"
      },
      "source": [
        "model1402.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs = 4,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "157/157 [==============================] - 128s 741ms/step - loss: 0.5717 - binary_accuracy: 0.6904 - metrics_recall: 0.4024 - metrics_precision: 0.5619 - metrics_f1: 0.4120\n",
            "Epoch 2/4\n",
            "157/157 [==============================] - 117s 746ms/step - loss: 0.3897 - binary_accuracy: 0.8287 - metrics_recall: 0.7106 - metrics_precision: 0.7819 - metrics_f1: 0.7238\n",
            "Epoch 3/4\n",
            "157/157 [==============================] - 116s 742ms/step - loss: 0.2348 - binary_accuracy: 0.9042 - metrics_recall: 0.8512 - metrics_precision: 0.8674 - metrics_f1: 0.8465\n",
            "Epoch 4/4\n",
            "157/157 [==============================] - 116s 738ms/step - loss: 0.1233 - binary_accuracy: 0.9583 - metrics_recall: 0.9350 - metrics_precision: 0.9404 - metrics_f1: 0.9349\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1de1713350>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1402.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3cde71-7bfe-491d-e88c-4679ba0a5685"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4f12a9-7640-4119-f9e7-bf06d0577cee"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00638642],\n",
              "       [0.53153944],\n",
              "       [0.94117457],\n",
              "       ...,\n",
              "       [0.9947049 ],\n",
              "       [0.00709038],\n",
              "       [0.01860476]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248e7dea-26e4-4ef1-a02c-ec760b8b2047"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7692525481313703"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "4dcbf5b6-d518-4439-b4ee-0e0dc5e01408"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2098  232]\n",
            " [ 583  619]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}