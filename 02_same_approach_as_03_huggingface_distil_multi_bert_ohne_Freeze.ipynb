{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "b09f8013-3caf-479a-ce67-1e71c94a931a"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194d5427-d1b1-4e06-a491-103563abefff"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.787946952000084\n",
            "GPU (s):\n",
            "0.04754584700003761\n",
            "GPU speedup over CPU: 79x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61058ea-41ec-4b84-9c5d-b7844d10aec8"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"ğŸ˜œ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ«\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ğŸ–\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜‡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¬\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜ƒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’™\", \" \",string)  \n",
        "   string = re.sub(\"ğŸ˜›\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ–•\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜‰\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’©\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¢\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¨\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤£\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜ˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’ƒğŸ½\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘¹\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤˜\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜±\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤”\", \" \",string) \n",
        "   string = re.sub(\"ğŸŒˆ\", \" \",string) \n",
        "   string = re.sub(\"ğŸ’•\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘©â€â¤ï¸â€ğŸ‘©\", \" \",string) \n",
        "   string = re.sub(\"ğŸ˜\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘†\", \" \",string) \n",
        "   string = re.sub(\"ğŸ˜–\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘‡\", \" \",string) \n",
        "   string = re.sub(\"ğŸ”¥\", \" \",string) \n",
        "   string = re.sub(\"ğŸ˜˜\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‰\", \" \",string) \n",
        "   string = re.sub(\"ğŸ¤¬\", \" \",string) \n",
        "   string = re.sub(\"ğŸ‘Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‡©ğŸ‡ª\", \" \",string)  \n",
        "   string = re.sub(\"ğŸ’”\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™ˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¯\", \" \",string)\n",
        "   string = re.sub(\"ğŸŸ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ›¶\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜“\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜³\", \" \",string)\n",
        "   string = re.sub(\"ğŸš€\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¸\", \" \",string)\n",
        "   string = re.sub(\"ğŸ“ˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜…\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜†\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™ğŸ¿\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘ğŸ½\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤­\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¤\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜²\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤®\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™„\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ…\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘‹\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’ª\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜„\", \" \",string)\n",
        "   string = re.sub(\"ğŸ§\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜ \", \" \",string)\n",
        "   string = re.sub(\"ğŸˆ\", \" \",string)\n",
        "   string = re.sub(\"ğŸš‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜Š\", \" \",string)\n",
        "   string = re.sub(\"ğŸš‡\", \" \",string)\n",
        "   string = re.sub(\"ğŸšŠ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤·\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜¥\", \" \",string)\n",
        "   string = re.sub(\"ğŸ™ƒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ”©\", \" \",string)\n",
        "   string = re.sub(\"ğŸ”§\", \" \",string)\n",
        "   string = re.sub(\"ğŸ”¨\", \" \",string)\n",
        "   string = re.sub(\"ğŸ› \", \" \",string)\n",
        "   string = re.sub(\"ğŸ’“\", \" \",string)\n",
        "   string = re.sub(\"ğŸ’¡\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¸\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¥ƒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¥‚\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜·\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤\", \" \",string)\n",
        "   string = re.sub(\"ğŸŒ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ‘‘\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤›\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜€\", \" \",string)\n",
        "   string = re.sub(\"ğŸ›¤\", \" \",string)\n",
        "   string = re.sub(\"ğŸ„\", \" \",string)\n",
        "   string = re.sub(\"ğŸ“´\", \" \",string)\n",
        "   string = re.sub(\"ğŸŒ­\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤•\", \" \",string)\n",
        "   string = re.sub(\"ğŸ˜­\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¾\", \" \",string)\n",
        "   string = re.sub(\"ğŸ\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¦\", \" \",string)\n",
        "   string = re.sub(\"ğŸ¤¯\", \" \",string)\n",
        "   string = re.sub(\"ğŸ•¯ï¸\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51b1d04-c2fc-4e0b-8b1e-70404ccc5338"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(80, return_sequences=True))(embedding_layer)\n",
        "#X= tf.keras.layers.LSTM(150, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(90, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1419 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1419.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1419.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b5acd3-3e95-4368-aea2-eff4a3f955b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 160)      543360      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 160)         0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 90)           14490       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 90)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            91          ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,292,021\n",
            "Trainable params: 135,292,021\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 7\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a467341-dd65-4196-c9c5-86478f4ee3c0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1419.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70a3e4a-4c22-4cfa-ee3d-6983aa18e9a3"
      },
      "source": [
        "model1419.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =7,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "157/157 [==============================] - 93s 501ms/step - loss: 0.5988 - binary_accuracy: 0.6824 - metrics_recall: 0.1458 - metrics_precision: 0.3101 - metrics_f1: 0.1781\n",
            "Epoch 2/7\n",
            "157/157 [==============================] - 79s 501ms/step - loss: 0.4915 - binary_accuracy: 0.7588 - metrics_recall: 0.5563 - metrics_precision: 0.6880 - metrics_f1: 0.5836\n",
            "Epoch 3/7\n",
            "157/157 [==============================] - 79s 501ms/step - loss: 0.4075 - binary_accuracy: 0.8151 - metrics_recall: 0.6783 - metrics_precision: 0.7634 - metrics_f1: 0.6996\n",
            "Epoch 4/7\n",
            "157/157 [==============================] - 79s 500ms/step - loss: 0.3361 - binary_accuracy: 0.8583 - metrics_recall: 0.7601 - metrics_precision: 0.8129 - metrics_f1: 0.7731\n",
            "Epoch 5/7\n",
            "157/157 [==============================] - 78s 499ms/step - loss: 0.2742 - binary_accuracy: 0.8898 - metrics_recall: 0.8173 - metrics_precision: 0.8507 - metrics_f1: 0.8248\n",
            "Epoch 6/7\n",
            "157/157 [==============================] - 79s 500ms/step - loss: 0.2388 - binary_accuracy: 0.9054 - metrics_recall: 0.8517 - metrics_precision: 0.8707 - metrics_f1: 0.8540\n",
            "Epoch 7/7\n",
            "157/157 [==============================] - 79s 501ms/step - loss: 0.2096 - binary_accuracy: 0.9239 - metrics_recall: 0.8861 - metrics_precision: 0.8907 - metrics_f1: 0.8810\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbd25474990>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1419.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d470f6db-29cf-4cbd-f2e5-b6fe1ec39700"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f931e723-c3ed-4b28-d346-af651ac7a53e"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02444363],\n",
              "       [0.72895825],\n",
              "       [0.89507025],\n",
              "       ...,\n",
              "       [0.93430966],\n",
              "       [0.03891427],\n",
              "       [0.03565075]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f16d3f-2967-4b32-d6d6-4d64e2b20d8e"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7641562853907135"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "01d33639-0cb1-48a0-bb06-7db93492ea86"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2123  207]\n",
            " [ 626  576]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}