{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "4240cd69-e52d-4cab-e3ec-9ea395c802f8"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f104187d-1081-427c-d573-596c3ddadaad"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.930005201000313\n",
            "GPU (s):\n",
            "0.03690592699967965\n",
            "GPU speedup over CPU: 79x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ffe3a04-35ad-4c43-ee77-fd530861db73"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057837c6-3446-466b-93c7-0ba49d8a0216"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "#X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "X= tf.keras.layers.LSTM(160, return_sequences=True)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(200, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1406 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1406.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1406.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d976318-7f5a-45c5-9d12-b40cbc8d2a7f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 60, 160)      594560      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 160)         0           ['lstm[0][0]']                   \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 200)          32200       ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 200)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            201         ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 135,361,041\n",
            "Trainable params: 135,361,041\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 5\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "#init_lr=2e-5\n",
        "init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06fab6a4-13b4-4e24-eb32-01652af8b8aa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1406.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3464484-5789-4272-cabb-1263aa45377a"
      },
      "source": [
        "model1406.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =5,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 48s 244ms/step - loss: 0.5718 - binary_accuracy: 0.7063 - metrics_recall: 0.3342 - metrics_precision: 0.5480 - metrics_f1: 0.3670\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 39s 249ms/step - loss: 0.4097 - binary_accuracy: 0.8119 - metrics_recall: 0.6929 - metrics_precision: 0.7480 - metrics_f1: 0.6959\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 39s 252ms/step - loss: 0.2475 - binary_accuracy: 0.9062 - metrics_recall: 0.8628 - metrics_precision: 0.8678 - metrics_f1: 0.8549\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 40s 254ms/step - loss: 0.1375 - binary_accuracy: 0.9517 - metrics_recall: 0.9275 - metrics_precision: 0.9315 - metrics_f1: 0.9265\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 40s 255ms/step - loss: 0.0680 - binary_accuracy: 0.9784 - metrics_recall: 0.9722 - metrics_precision: 0.9656 - metrics_f1: 0.9673\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efefc62da10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1406.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1313419-0630-4c9d-a024-b245c80e4bfa"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d9219e-1688-41fe-ac1e-d2eab876a392"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00404278],\n",
              "       [0.8259268 ],\n",
              "       [0.9491729 ],\n",
              "       ...,\n",
              "       [0.9703197 ],\n",
              "       [0.00317202],\n",
              "       [0.00398343]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276e2a13-4663-43f6-c8ac-15f736c595d1"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7627406568516422"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "b4980143-6216-4baa-da31-ca8bcb0899ed"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2104  226]\n",
            " [ 612  590]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}