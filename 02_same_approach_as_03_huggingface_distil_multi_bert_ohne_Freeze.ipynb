{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 same approach as 03 huggingface distil_multi_bert ohne Freeze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BiancaStadl/Bachelorarbeit/blob/main/02_same_approach_as_03_huggingface_distil_multi_bert_ohne_Freeze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJ5aNUwcn0x"
      },
      "source": [
        "Siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a  \n",
        "\n",
        "Punkt 2.2.3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "huggingface\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "\n",
        "look at that! https://huggingface.co/transformers/model_doc/distilbert.html\n",
        "\n",
        "https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "mit:\n",
        "\n",
        "hier sehr viel von https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb (batchencode und model building)\n",
        "\n",
        "\n",
        "freeze unfreeze siehe:\n",
        "* https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow\n",
        "* https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/task_summary#sequence-classification\n",
        "\n",
        "https://huggingface.co/distilbert-base-multilingual-cased\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/transfer_learning?hl=en#the_typical_transfer-learning_workflow "
      ],
      "metadata": {
        "id": "0BWlSLlw3KRw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNWyze032Y0j"
      },
      "source": [
        "general tutorial: https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=en\n",
        "\n",
        "German pre-trained embeddings:\n",
        "* Distilbert -> cite!! https://huggingface.co/distilbert-base-multilingual-cased "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsspqybXTK-"
      },
      "source": [
        "https://github.com/huggingface/transformers\n",
        "\n",
        "https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuL5ZPrUk4y_"
      },
      "source": [
        "\"As we will see, the Hugging Face Transformers library makes transfer learning very approachable, as our general workflow can be divided into four main stages:\n",
        "\n",
        "    Tokenizing Text\n",
        "    Defining a Model Architecture\n",
        "    Training Classification Layer Weights\n",
        "    Fine-tuning DistilBERT and Training All Weights\"\n",
        "\n",
        "    https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsqsKVDWJwl"
      },
      "source": [
        "Citation: Using GPU in colab and Tensorflow: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JVk2IxqVIEY",
        "outputId": "4b360d43-a32a-4d0d-bd2e-af70163e8eb3"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7x8SWtVVJ9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81da8b83-6f57-4f04-e05e-cfecd15eed8d"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "2.8011206020000827\n",
            "GPU (s):\n",
            "0.03716946300028212\n",
            "GPU speedup over CPU: 75x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3xfitNdliBI"
      },
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import losses\n",
        "from tensorflow import keras \n",
        "from keras import optimizers \n",
        "from keras import metrics \n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUmO-Vhq1v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90907968-07a6-4126-d149-66ed5a45f6ea"
      },
      "source": [
        "!pip install transformers \n",
        "from transformers import DistilBertTokenizerFast\n",
        "#distilbert-base-german-cased,distilbert-base-multilingual-cased\n",
        "\n",
        "# Instantiate DistilBERT tokenizer...Fast version to optimize runtime\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "##Achtung: but the distilbert-base-multilingual-cased model throws an exception during training -> siehe https://towardsdatascience.com/text-classification-with-hugging-face-transformers-in-tensorflow-2-without-tears-ee50e4f3e7ed\n",
        "#direkt von https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRwhH-dt7P9"
      },
      "source": [
        "documentation\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.__call__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkqEytBwu-Rv"
      },
      "source": [
        "#von direkt https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
        "# Define the maximum number of words to tokenize (DistilBERT can tokenize up to 512)\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "\n",
        "# Define function to encode text data in batches\n",
        "def batch_encode(tokenizer, texts, batch_size=32, max_length=60):\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    A function that encodes a batch of texts and returns the texts'\n",
        "    corresponding encodings and attention masks that are ready to be fed \n",
        "    into a pre-trained transformer model.\n",
        "    \n",
        "    Input:\n",
        "        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n",
        "        - texts:       List of strings where each string represents a text\n",
        "        - batch_size:  Integer controlling number of texts in a batch\n",
        "        - max_length:  Integer controlling max number of words to tokenize in a given text\n",
        "    Output:\n",
        "        - input_ids:       sequence of texts encoded as a tf.Tensor object\n",
        "        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n",
        "    \"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer.batch_encode_plus(batch,\n",
        "                                             max_length=max_length,\n",
        "                                             padding='max_length',\n",
        "                                             truncation=True,\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_token_type_ids=False\n",
        "                                             )\n",
        "        input_ids.extend(inputs['input_ids'])\n",
        "        attention_mask.extend(inputs['attention_mask'])\n",
        "    \n",
        "    \n",
        "    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq08Me5la_Cc"
      },
      "source": [
        "max_length = 60"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVYBMvYSotTH"
      },
      "source": [
        "url = \"https://github.com/uds-lsv/GermEval-2018-Data/archive/master.zip\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"GermEval-2018-Data-master.zip\", url, \n",
        "                                   extract=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'GermEval-2018-Data-master')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS14OUtfo34V"
      },
      "source": [
        "#os.listdir(dataset_dir)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X429K6hpOVm"
      },
      "source": [
        "training_file = os.path.join(dataset_dir, 'germeval2018.training.txt')\n",
        "#with open(training_file) as f:\n",
        " # print(f.read())\n",
        "\n",
        "#print()\n",
        "\n",
        "testing_file = os.path.join(dataset_dir, 'germeval2018.test.txt')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRqhP_Fx0cK3"
      },
      "source": [
        "def remove_clutter(string):\n",
        "   string = re.sub(\"@[^\\s]+\",\" \",string)\n",
        "   string = re.sub(\"#[^\\s]+\",\" \", string)\n",
        "   string = re.sub(\"\\u00a9\",\" \", string)\n",
        "   string = re.sub(\"\\u00ae\",\" \", string)\n",
        "   string = re.sub(\"[\\u2000-\\u3300]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83c[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83d[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"\\ud83e[\\ud000-\\udfff]\",\" \", string)\n",
        "   string = re.sub(\"😜\", \" \",string)\n",
        "   string = re.sub(\"🍫\", \" \",string)\n",
        "   string = re.sub(\"😁\", \" \",string)\n",
        "   string = re.sub(\"🐖\", \" \",string)\n",
        "   string = re.sub(\"😡\", \" \",string)\n",
        "   string = re.sub(\"😇\", \" \",string)\n",
        "   string = re.sub(\"😬\", \" \",string)\n",
        "   string = re.sub(\"😃\", \" \",string)\n",
        "   string = re.sub(\"😂\", \" \",string)\n",
        "   string = re.sub(\"💙\", \" \",string)  \n",
        "   string = re.sub(\"😛\", \" \",string)\n",
        "   string = re.sub(\"🙏\", \" \",string)\n",
        "   string = re.sub(\"👍\", \" \",string)\n",
        "   string = re.sub(\"🖕\", \" \",string)\n",
        "   string = re.sub(\"😉\", \" \",string)\n",
        "   string = re.sub(\"💩\", \" \",string)\n",
        "   string = re.sub(\"🤢\", \" \",string)\n",
        "   string = re.sub(\"👏\", \" \",string)\n",
        "   string = re.sub(\"😨\", \" \",string)\n",
        "   string = re.sub(\"🤣\", \" \",string)\n",
        "   string = re.sub(\"🤡\", \" \",string)\n",
        "   string = re.sub(\"😈\", \" \",string)\n",
        "   string = re.sub(\"💃🏽\", \" \",string)\n",
        "   string = re.sub(\"👹\", \" \",string)\n",
        "   string = re.sub(\"🤘\", \" \",string)\n",
        "   string = re.sub(\"😱\", \" \",string)\n",
        "   string = re.sub(\"🤔\", \" \",string) \n",
        "   string = re.sub(\"🌈\", \" \",string) \n",
        "   string = re.sub(\"💕\", \" \",string) \n",
        "   string = re.sub(\"👩‍❤️‍👩\", \" \",string) \n",
        "   string = re.sub(\"😍\", \" \",string) \n",
        "   string = re.sub(\"👆\", \" \",string) \n",
        "   string = re.sub(\"😖\", \" \",string) \n",
        "   string = re.sub(\"👇\", \" \",string) \n",
        "   string = re.sub(\"🔥\", \" \",string) \n",
        "   string = re.sub(\"😘\", \" \",string) \n",
        "   string = re.sub(\"🎉\", \" \",string) \n",
        "   string = re.sub(\"🤬\", \" \",string) \n",
        "   string = re.sub(\"👊\", \" \",string)\n",
        "   string = re.sub(\"🇩🇪\", \" \",string)  \n",
        "   string = re.sub(\"💔\", \" \",string)\n",
        "   string = re.sub(\"🙈\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🐟\", \" \",string)\n",
        "   string = re.sub(\"🛶\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😓\", \" \",string)\n",
        "   string = re.sub(\"😳\", \" \",string)\n",
        "   string = re.sub(\"🚀\", \" \",string)\n",
        "   string = re.sub(\"👎\", \" \",string)\n",
        "   string = re.sub(\"😎\", \" \",string)\n",
        "   string = re.sub(\"🐸\", \" \",string)\n",
        "   string = re.sub(\"📈\", \" \",string)\n",
        "   string = re.sub(\"🙂\", \" \",string)\n",
        "   string = re.sub(\"😅\", \" \",string)\n",
        "   string = re.sub(\"😆\", \" \",string)\n",
        "   string = re.sub(\"🙎🏿\", \" \",string)\n",
        "   string = re.sub(\"👎🏽\", \" \",string)\n",
        "   string = re.sub(\"🤭\", \" \",string)\n",
        "   string = re.sub(\"😤\", \" \",string)\n",
        "   string = re.sub(\"😚\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"😲\", \" \",string)\n",
        "   string = re.sub(\"🤮\", \" \",string)\n",
        "   string = re.sub(\"🙄\", \" \",string)\n",
        "   string = re.sub(\"🤑\", \" \",string)\n",
        "   string = re.sub(\"🎅\", \" \",string)\n",
        "   string = re.sub(\"👋\", \" \",string)\n",
        "   string = re.sub(\"💪\", \" \",string)\n",
        "   string = re.sub(\"😄\", \" \",string)\n",
        "   string = re.sub(\"🧐\", \" \",string)\n",
        "   string = re.sub(\"😠\", \" \",string)\n",
        "   string = re.sub(\"🎈\", \" \",string)\n",
        "   string = re.sub(\"🚂\", \" \",string)\n",
        "   string = re.sub(\"😊\", \" \",string)\n",
        "   string = re.sub(\"🚇\", \" \",string)\n",
        "   string = re.sub(\"🚊\", \" \",string)\n",
        "   string = re.sub(\"🤷\", \" \",string)\n",
        "   string = re.sub(\"😥\", \" \",string)\n",
        "   string = re.sub(\"🙃\", \" \",string)\n",
        "   string = re.sub(\"🔩\", \" \",string)\n",
        "   string = re.sub(\"🔧\", \" \",string)\n",
        "   string = re.sub(\"🔨\", \" \",string)\n",
        "   string = re.sub(\"🛠\", \" \",string)\n",
        "   string = re.sub(\"💓\", \" \",string)\n",
        "   string = re.sub(\"💡\", \" \",string)\n",
        "   string = re.sub(\"🍸\", \" \",string)\n",
        "   string = re.sub(\"🥃\", \" \",string)\n",
        "   string = re.sub(\"🥂\", \" \",string)\n",
        "   string = re.sub(\"😷\", \" \",string)\n",
        "   string = re.sub(\"🤐\", \" \",string)\n",
        "   string = re.sub(\"🌎\", \" \",string)\n",
        "   string = re.sub(\"👑\", \" \",string)\n",
        "   string = re.sub(\"🤛\", \" \",string)\n",
        "   string = re.sub(\"😀\", \" \",string)\n",
        "   string = re.sub(\"🛤\", \" \",string)\n",
        "   string = re.sub(\"🎄\", \" \",string)\n",
        "   string = re.sub(\"📴\", \" \",string)\n",
        "   string = re.sub(\"🌭\", \" \",string)\n",
        "   string = re.sub(\"🤕\", \" \",string)\n",
        "   string = re.sub(\"😭\", \" \",string)\n",
        "   string = re.sub(\"🍾\", \" \",string)\n",
        "   string = re.sub(\"🍞\", \" \",string)\n",
        "   string = re.sub(\"🤦\", \" \",string)\n",
        "   string = re.sub(\"🤯\", \" \",string)\n",
        "   string = re.sub(\"🕯️\", \" \",string)\n",
        "\n",
        "   string = re.sub(\"OTHER|OFFENSE|ABUSE|INSULT\",\" \",string)\n",
        "   return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5asMgo4LtnRg"
      },
      "source": [
        "statementsForTraining = []\n",
        "sentimentsForTraining = []\n",
        "\n",
        "fileToRead = open(training_file, 'r')\n",
        "\n",
        "while True:\n",
        "  #next line in file\n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  findSentiment = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "      \n",
        "\n",
        "  statementsForTraining.append(line)\n",
        "\n",
        "  if findSentiment.group(0) == \"OTHER\":  \n",
        "    sentimentsForTraining.append(0)\n",
        "  else:\n",
        "    sentimentsForTraining.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        " #print(\"{}: {}\".format(count,line.strip()))\n",
        "  \n",
        " # print(sentiment.group(0))\n",
        " \n",
        "fileToRead.close()\n",
        "\n",
        "training_sentences = statementsForTraining\n",
        "training_labels = sentimentsForTraining\n",
        "\n",
        "#print(training_sentences[0:100])\n",
        "#print(training_labels[9])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqZPENb98gD"
      },
      "source": [
        "\n",
        "statementsForTesting = []\n",
        "sentimentsForTesting = []\n",
        "\n",
        "fileToRead = open(testing_file, 'r')\n",
        "\n",
        "while True:\n",
        " \n",
        "  line = fileToRead.readline()\n",
        "\n",
        "  if line == \"\":\n",
        "   break\n",
        "\n",
        "  sent = re.search(\"OTHER|OFFENSE\",line)\n",
        "\n",
        "  line = remove_clutter(line)\n",
        "\n",
        "\n",
        "  statementsForTesting.append(line)\n",
        "  #print(len(line))\n",
        "\n",
        "  if sent.group(0) == \"OTHER\": \n",
        "    sentimentsForTesting.append(0)\n",
        "  else:\n",
        "    sentimentsForTesting.append(1)\n",
        "\n",
        "  if not line:\n",
        "    break\n",
        "\n",
        "\n",
        "fileToRead.close()\n",
        "\n",
        "\n",
        "testing_sentences = statementsForTesting\n",
        "testing_labels = sentimentsForTesting\n",
        "#print(len(testing_sentences))\n",
        "#print(testing_sentences)   \n",
        "#print(statementsForTesting)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9iDxdwbvIVO"
      },
      "source": [
        "# Encode training set X\n",
        "X_train_ids, X_train_attention = batch_encode(tokenizer, training_sentences)\n",
        "\n",
        "# Encode test set\n",
        "Y_test_ids, Y_test_attention = batch_encode(tokenizer, testing_sentences)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrMqxkExYKX"
      },
      "source": [
        "see also here for the code https://github.com/RayWilliam46/FineTune-DistilBERT/blob/main/notebooks/train_balanced.ipynb "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5qYC_xx_aTK"
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def metrics_recall(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(data_true,0,1)))\n",
        "\n",
        "    recall = true_positives / (possible_positives+K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def metrics_precision(data_true, data_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(data_true*data_pred,0,1)))\n",
        "\n",
        "    positives_predicted = K.sum(K.round(K.clip(data_pred,0,1)))\n",
        "    precision = true_positives / (positives_predicted+K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def metrics_f1(data_true, data_pred):\n",
        "    precision_data = metrics_precision(data_true, data_pred)\n",
        "    recall_data = metrics_recall(data_true, data_pred)\n",
        "    return 2*(precision_data*recall_data)/(precision_data+recall_data+K.epsilon())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivdTjRlyvzl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95205ac4-8933-4709-b6d9-3dfaf1564d12"
      },
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "#siehe https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
        "# config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "# config.output_hidden_states = False\n",
        "\n",
        "\n",
        "input_ids_in = tf.keras.layers.Input(shape=(60,), name='input_token', dtype='int32')\n",
        "input_masks_in = tf.keras.layers.Input(shape=(60,), name='masked_token', dtype='int32') \n",
        "distilBERT= TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', output_hidden_states=False, dropout=0.2, attention_dropout=0.2)\n",
        "\n",
        "\n",
        "embedding_layer = distilBERT(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n",
        "#X= tf.keras.layers.LSTM(80, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(embedding_layer)\n",
        "X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
        "X = tf.keras.layers.Dense(70, activation='relu')(X)\n",
        "X = tf.keras.layers.Dropout(0.2)(X)\n",
        "X = tf.keras.layers.Dense(1, activation='sigmoid')(X)\n",
        "model1504 = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n",
        "\n",
        "for layer in model1504.layers[:3]:\n",
        "  layer.trainable = True\n",
        "\n",
        "\n",
        "#siehe\n",
        "\n",
        "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a und 03"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-multilingual-cased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1504.summary()"
      ],
      "metadata": {
        "id": "Ng_9yV0WrNYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07884ddd-948a-4771-9c42-397f002b8ea4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  134734080  ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 60, 768),                                                    \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 60, 80)       258880      ['tf_distil_bert_model[0][0]']   \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 80)          0           ['bidirectional[0][0]']          \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 70)           5670        ['global_max_pooling1d[0][0]']   \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 70)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            71          ['dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 134,998,701\n",
            "Trainable params: 134,998,701\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40qt-vG0HjcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization"
      ],
      "metadata": {
        "id": "7mjrpjTlpbIu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_epochs = 7\n",
        "\n",
        "#das ist dann schon wieder von 01 (tf tutorial classify text)\n",
        "\n",
        "steps_per_epoch = 157\n",
        "num_train_steps = steps_per_epoch * training_epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#num_warmup_steps = 10_000 int(0.1*num_train_steps)\n",
        "\n",
        "#init_lr = 3e-5\n",
        "init_lr=2e-5\n",
        "#init_lr =1e-4 \n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "print(num_warmup_steps)"
      ],
      "metadata": {
        "id": "RmdBBEPApaoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3fbc381-4640-4937-c9b8-8dac73e299ee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:using Adamw optimizer\n",
            "INFO:absl:gradient_clip_norm=1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2xQjSNyUCu"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model1504.compile(loss=loss, optimizer=optimizer,metrics=[metrics,metrics_recall,metrics_precision,metrics_f1])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfBDQO4y7vV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f890212-7d82-42c8-9ac7-cdee78a7d69f"
      },
      "source": [
        "model1504.fit(\n",
        "     x = [X_train_ids, X_train_attention],\n",
        "     y = np.array(training_labels),\n",
        "     epochs =7,\n",
        "     batch_size = 32\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "157/157 [==============================] - 128s 737ms/step - loss: 0.6229 - binary_accuracy: 0.6478 - metrics_recall: 0.2246 - metrics_precision: 0.4270 - metrics_f1: 0.2361\n",
            "Epoch 2/7\n",
            "157/157 [==============================] - 114s 727ms/step - loss: 0.5000 - binary_accuracy: 0.7632 - metrics_recall: 0.5357 - metrics_precision: 0.7065 - metrics_f1: 0.5783\n",
            "Epoch 3/7\n",
            "157/157 [==============================] - 115s 729ms/step - loss: 0.4227 - binary_accuracy: 0.8040 - metrics_recall: 0.6533 - metrics_precision: 0.7423 - metrics_f1: 0.6735\n",
            "Epoch 4/7\n",
            "157/157 [==============================] - 113s 720ms/step - loss: 0.3579 - binary_accuracy: 0.8471 - metrics_recall: 0.7634 - metrics_precision: 0.7925 - metrics_f1: 0.7635\n",
            "Epoch 5/7\n",
            "157/157 [==============================] - 113s 720ms/step - loss: 0.3034 - binary_accuracy: 0.8754 - metrics_recall: 0.8042 - metrics_precision: 0.8275 - metrics_f1: 0.8030\n",
            "Epoch 6/7\n",
            "157/157 [==============================] - 113s 719ms/step - loss: 0.2646 - binary_accuracy: 0.9024 - metrics_recall: 0.8359 - metrics_precision: 0.8671 - metrics_f1: 0.8446\n",
            "Epoch 7/7\n",
            "157/157 [==============================] - 113s 719ms/step - loss: 0.2424 - binary_accuracy: 0.9098 - metrics_recall: 0.8719 - metrics_precision: 0.8636 - metrics_f1: 0.8599\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff6c3d16f10>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bn10sQaTAYS6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjc-rMEuL16"
      },
      "source": [
        "BERTDistilledCasedPredict = model1504.predict([Y_test_ids, Y_test_attention])\n",
        "BERT_pred_thresh = np.where(BERTDistilledCasedPredict >= 0.5, 1, 0)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity checks.."
      ],
      "metadata": {
        "id": "6SzAL7oiDzEg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrZlbvV7Rs8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f607cf-15a5-4ea8-ebd6-7e7fe10d5c54"
      },
      "source": [
        "BERT_pred_thresh"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlAzqD1kIDI6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hwokE3RxuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497a0afe-b3ed-4e67-b34c-68ddd4335a32"
      },
      "source": [
        "BERTDistilledCasedPredict"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03935354],\n",
              "       [0.82794803],\n",
              "       [0.8513762 ],\n",
              "       ...,\n",
              "       [0.90920526],\n",
              "       [0.7442961 ],\n",
              "       [0.03639021]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NEPZr5p1sp9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU1E97B1tMV"
      },
      "source": [
        "accuracy = accuracy_score(testing_labels, BERT_pred_thresh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsewHKIR2nY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed127c98-9bd8-483e-b3f7-9e017a706c46"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7599093997734995"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaakc1HMuHOI"
      },
      "source": [
        "#not sure if that and the matrix still work like that\n",
        "# (loss,accuracy, metrics_recall, metrics_precision,\n",
        "# metrics_f1) = model.evaluate(testing_sentences, testing_labels, verbose=1)\n",
        "#but maybe here \n",
        "#https://www.yuyongze.me/blog/BERT-text-classification-movie/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd_WGzTuMuYX"
      },
      "source": [
        "#for p in LSTM_predict80AE:\n",
        " # print(p)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PluuAMv2MxlW"
      },
      "source": [
        "#prediction_rounded80AE = np.round(LSTM_predict80AE)\n",
        "\n",
        "#for p in prediction_rounded80AE:\n",
        " # print(p)\n",
        "\n",
        "\n",
        "#print(nptesting_labels[200:210])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfW_WcDlWsZv"
      },
      "source": [
        "https://deeplizard.com/learn/video/km7pxKy4UHU\n",
        "\n",
        "Quelle der def plot_confusion_matrix: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZjt-y0-WrPZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5RUaFEcXmYc"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mu7wle3Wr5S"
      },
      "source": [
        "cm = confusion_matrix(y_true=testing_labels, y_pred=BERT_pred_thresh)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcIt6FU7Wr_q"
      },
      "source": [
        "plot_labels = ['no hatespeech','hatespeech']"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-K7cFJfWsGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "0fce8283-8420-4b1e-d3d4-0f99faab162b"
      },
      "source": [
        "plot_confusion_matrix(cm=cm, classes=plot_labels, title='disilbert multi')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[2078  252]\n",
            " [ 596  606]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEmCAYAAACzoiEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5yU1fXH8c93l64gIEWKioWg2CgKKkrALvYuamzYEluKhRgTezSiPw0qGgtBRLFhQcVCMEY0ooACAoqiQgRBQFBQUFk4vz/uHRxwmZ1ddvfZZ/e885oXM3eecmf8/c7cPc99zpWZ4ZxzLhkFSXfAOedqMg/CzjmXIA/CzjmXIA/CzjmXIA/CzjmXIA/CzjmXIA/CrtJJGiLp+vh8b0kz8tjnCkn3x+ftJJmkWvH1a5LOqtheV5yS+i/pHkl/rsw+ucpTK+kOuJrNzMYCHfLY7q+V0B0gBEVgmJndX1nnzDr36cBZZrZXps3MzqvsfrjK40HYuUiSACXdD1ezeDrCVThJnSW9K2mZpMeAelnv9ZI0J+v15ZLmxm1nSNo3tl8taViO02wj6R1JSyU9K6lp1jF3l/RfSV9LmiypV9Z7r0m6QdKbwHLgIWBv4E5J30q6s5jPk0mHnCHpc0lLJJ0naTdJU+J57szafq2+r5tOyWrfHrgH2COe++vYviZ946ofD8KuQkmqAzxDCG5NgSeAY9azbQfgAmA3M2sIHAjMyvNUpwJnAq2AImBgPGYb4AXg+nj+S4ARkppn7fsr4BygIXA6MBa4wMw2NrMLcpyzO9AeOAG4HfgTsB+wA3C8pF/m2XcAzOwD4DzgrXjuxqXZ36WTB2FX0XYHagO3m9lKM3sSGL+ebVcBdYGOkmqb2Swz+yTP8zxkZlPN7Dvgz4QgWAicAowys1FmttrMRgMTgD5Z+w4xs2lmVmRmK0vx2a4zs+/N7BXgO2C4mS0ws7mEQN65FMdyNZQHYVfRWgNzbe1KUbOL29DMZgK/Ba4GFkh6VFLrPM/z+TrHrw00A7YEjospgq/jn/h7EUbMxe1bGl9mPV9RzOuNy3hcV4N4EHYVbR7QJl70ythifRub2SNxZsCWgAF/y/M8m69z/JXAIkKAfcjMGmc9NjKzm7JPu2438jxnvr4DGmS93izHtl7WsIbxIOwq2luEHO1FkmpLOhroVtyGkjpI2kdSXeB7wmhydZ7nOUVSR0kNgGuBJ81sFTAMOEzSgZIKJdWLFwPb5jjWl8DWeZ43H5OAnpK2kLQJ8McSzt025tJdDeBB2FUoM/sROJpwwWsx4SLWU+vZvC5wE2EEOx9oQe6Ale0hYEjcrx5wUTz/58ARwBXAQsLI+FJy/9/+34Fj46yHgXmef71iHvoxYAowEXg+x+avAtOA+ZIWbei5XdUnL+runHPJ8ZGwc84lyIOwc84lyIOwc84lyIOwc84lyAv4VFGqVd9Up2HS3agxOm+/3qnLrgLMnj2LRYsWlUuxpMJGW5oVrci5ja1Y+LKZHVQe5ytvHoSrKNVpSN0OxyfdjRrjzbd/VqfHVaAe3Xctt2NZ0YoS/3/l+0l3NSu3E5YzD8LOuXSToKAw6V6UmQdh51z6Kb2XtzwIO+dSLt0j4fT+fDjnXIaU+1Hi7tpc0r8lTZc0TdLFsb2ppNGSPo7/NontkjRQ0sxYyL9L1rFOi9t/LOm0ks7tQdg5l24ipCNyPUpWBPzBzDoSamCfL6kj0B8YY2btgTHxNcDBhIL+7QkLAtwNIWgDVxEK/ncDrsoE7vXxIOycS7mYjsj1KIGZzTOzd+PzZcAHQBtC8acH42YPAkfG50cAQy0YBzSW1IqwGsxoM1tsZkuA0UDOqXGeE3bOpV/JKYdmkiZkvb7XzO4t/lBqR1gV5W2gpZnNi2/NB1rG521YezGAObFtfe3r5UHYOZdyyiflsMjMSpycLGljYATwWzNbmr0WgZmZpHIvO+npCOdcuokNTkcASKpNCMAPm1mm5vWXMc1A/HdBbJ/L2qu5tI1t62tfLw/CzrmU0wZfmIvLbz0AfGBm/5f11kggM8PhNODZrPZT4yyJ3YFvYtriZeAASU3iBbkDYtt6eTrCOZduAgo3eJ5wD+BXwPuSJsW2KwgrvTwuqR9hAdnM/dGjCCt2zwSWA2cAmNliSdfx04ri15rZ4lwn9iDsnEu/POYC52JmbxDCeXH2LWZ7A85fz7EGA4PzPbcHYedcyuV1Ya7K8iDsnEu/FN+27EHYOZdued6aXFV5EHbOpZ+PhJ1zLimeE3bOuWR5OsI55xIiQUF6Q1l6e+6ccxk+EnbOuQT5hTnnnEuI/MKcc84ly9MRzjmXDAEFBT4Sds65ZIj1l95JAQ/CzrmUE/J0hHPOJcfTEc45l6A0j4TT+/PhnHOEAKyC3I88jjFY0gJJU7PaHpM0KT5mZVbckNRO0oqs9+7J2qerpPclzZQ0UHn8OvhI2DmXeuUwEh4C3AkMzTSY2QlZx78V+CZr+0/MrFMxx7kbOBt4m7AE0kHAi7lO7CNh51zqScr5KImZvQ4UuxZcHM0eDwwvoQ+tgEZmNi4ufzQUOLKkc3sQds6lm8gnHdFM0oSsxzmlOMPewJdm9nFW21aS3pP0H0l7x7Y2wJysbebEtpw8HeGcS708RruLzGzXMh6+L2uPgucBW5jZV5K6As9I2qGMx/Yg7JxLN6EKm6ImqRZwNNA102ZmPwA/xOcTJX0C/AKYC7TN2r1tbMvJ0xHOufRTCY+y2w/40MzWpBkkNZdUGJ9vDbQHPjWzecBSSbvHPPKpwLMlncCDsHMu3bThF+YkDQfeAjpImiOpX3zrRH5+Qa4nMCVOWXsSOM/MMhf1fgPcD8wEPqGEmRHg6QjnXDWwoekIM+u7nvbTi2kbAYxYz/YTgB1Lc24Pwi6nti0bc/91p9Ji04aYweARb3LX8Ndo0qgBD/3tTLZs3ZTZXyzmlMse4OtlK/jdqftyQp/dAKhVWMB2W23G5vv0Z8nS5Vx4cm9OP2pPzIxpM7/gnKuG8cOPRQl/wqrr888/56wzTmXBgi+RxJn9zuGCiy7m+muvZvAD99G8WXMArrn+rxx0cB/G/Gs0f76iPz/++CN16tThr38bQK/e+yT8KSqevHaEq86KVq2m//89xaQP57Bxg7r895HLGfP2h/zqsO689s4MbvnnaC45Y38uOeMArhz4LLcNHcNtQ8cA0Kfnjlx4cm+WLF1O6+ab8Ju+v6TzMTfw/Q8rGfa3MznuwK4Me+7thD9h1VWrVi1uuvlWOnfpwrJly9ize1f23W9/AC68+Hf87veXrLX9pps248lnnqN169ZMmzqVww45kE9nl3hdKP3iFLW08pywy2n+oqVM+jBck/h2+Q98+Nl8WjdvzKG9dl4TQIc99zaH9d75Z/sef9CuPP7SxDWvaxUWUr9ubQoLC6hfrw7zFn7zs33cT1q1akXnLl0AaNiwIdtttz1ffLH+oNqpc2dat24NQMcdduD7FSv44YcfKqWvSdvQnHCSPAi7vG3RqimdOrRl/NRZtNi0IfMXLQVCoG6xacO1tq1frzb777k9z4yZBMAXC7/h9qFj+OjF6/hs9A0s/XYFY8Z9WOmfIa1mz5rFpEnvsVu37gDcM+hOduu8M+eedSZLliz52fZPPzWCTp27ULdu3cruaiI8CJczSUMkHVuK7RtL+k1F9mlDxOIfzZLux4bYqH4dht9yFpfeMoJl333/s/fN1n59SM+deGvSpyxZuhyAxg3rc2ivndj+0KvY+oA/sVH9OpwYc8cut2+//Za+xx/DgFtvp1GjRpx97q+ZPuMT3p44ic1ataL/pX9Ya/vp06Zx5RWXc+egfyTU48q3oQV8klQlg3AZNCZMDXEVoFatAobfcjaPvTiBZ1+dDMCCr5axWbNGAGzWrBELFy9ba5/jDuzKE1mpiH26b8esL75i0ZJvKSpazTOvTmb3XbaqvA+RUitXrqTv8cdwQt+TOfKoowFo2bIlhYWFFBQUcGa/s5kw4Z0128+ZM4cTjjuK+wcPZetttkmq25WqpFFwjRwJx1JvH0i6T9I0Sa9Iqh/f6yRpnKQpkp6W1GQ9h+kp6b+SPs2MiiVtLGmMpHdjubgj4rY3AdvEsnID4raXShofz3NNbNtI0guSJkuaKumE2D5L0s3xmO9I2ja2N5c0Ih5nvKQeWccZHLd9L9MPSYWSbonHniLpwqzPc2FWv7cr32+8Yt1z1cnM+Gw+A4e9uqbthf+8zymHhT+NTzmsO8+/NmXNe402rsdeXbfluay2z+cvpttOW1G/Xm0AenfrwIzPvqykT5BOZsZ5Z/ejw3bbc/Hvfr+mfd68eWueP/vM03TcIcyI+vrrrzn68EO47oab2LNHj0rvb5LSHIQrcnZEe6CvmZ0t6XHgGGAYobLQhWb2H0nXAlcBvy1m/1bAXsB2wEjCpOjvgaPMbGn8836cpJFAf2DHTGk5SQfE83cj3C8zUlJPoDnwhZkdErfbJOt835jZTpJOBW4HDgX+DtxmZm9I2gJ4Gdge+BPwqpmdKakx8I6kfxHukGkHdDKzIklNs46/yMy6xLTJJcBZ635ghaIiobBI7Y3z+Y4r3J6dtubkQ7vz/kdzGfdofwCuunMkt/xzNMP+dianHbkH/5u3mFMuG7xmn8N778KYcR+y/Psf17SNnzqbp//1Hm89cjlFq1Yz+cM5PDDizUr/PGny3zff5JGHH2LHHXeie9dQNfGa6//K448OZ8rkSUhiy3btuCOmHe4ZdCeffDKTG6+/lhuvvxaA5158hRYtWiT2GSpLVU855CJbN5lXHgeV2gGjzax9fH05UBu4A3jfzLaI7dsAT5hZl3X2HxL3fzi+XmZmDSXVBm4j3LGyGugAbAXUA543sx3j9rcAxwJfx0NuDNwIjAVeAR6L24+N288C9jGzT+M55pvZppIWAF9kda15POdr8ZyZSa5NgQOB64F7zGz0Op9nFtDDzOZK6g7cYGb75foOCxq0sLodjs+1iStHS8bfmXQXapQe3Xdl4sQJ5RI567Zsb21O/nvObT677ZCJG1DAp0JV5Eg4e27MKqD+Buyf+Y91MiEQdjWzlTG41StmXwE3mtnPrkxI6gL0Aa6XNMbMro1vZf8aZZ4XALub2ffrHEPAMWY2Y532fD7PKnx+tnPlRoKCFI+EK/XCnJl9AyzRT/U3fwX8pxSH2ARYEANwb2DL2L4MyJ4j9TJwpqSNASS1kdRCUmtguZkNAwYA2SPwE7L+fSs+fwVYk9eVlKmk/zIhx6vY3jm2jwbOVai8xDrpCOdchUj3hbkkRmSnAfdIagB8CpxRin0fBp6T9D4wAfgQINb1fFNhfagXzexSSdsDb8X/AN8CpwDbAgMkrQZWAr/OOnYTSVMII9bMfeQXAXfF9lrA68B5wHWEvPEUSQXAZ4Qc8v2EknZTJK0E7iMsmeKcq0BVPM7mVCE54bSJaY1dzWxR0n3J8Jxw5fKccOUqz5xwvVa/sHan3ZFzmxl/O6hG5oSdc67CiXTnhD0IA2bWLuk+OOfKzoOwc84lRenOCXsQds6lmshroc8qq7rUjnDO1ViioCD3o8QjhDIEC+IMq0zb1ZLmxnIIkyT1yXrvj5JmSpoh6cCs9oNi20xJ/fPpvQdh51zqlcM84SHAQcW032ZmneJjVDxXR8LaczvEfQbFujGFwF3AwUBHoG/cNidPRzjnUq087pgzs9djuYV8HAE8amY/AJ9JmkmoUwMw08w+Df3So3Hb6bkO5iNh51zqSbkfQDNJE7Ie5+R56AtiRcTB+qniYxvg86xt5sS29bXn5EHYOZd6eaQjFpnZrlmPe/M47N3ANkAnYB5wa0X03dMRzrl0q6ACPma2puC1pPuA5+PLucDmWZu2jW3kaF8vHwk751ItTFErMR1R+uNKrbJeHgVkZk6MBE6UVFfSVoTa5e8A44H2kraSVIdw8W5kSefxkbBzLuU2vFKapOFAL0LueA5hsYlesXKiAbOAcwHMbFpcqGI6oab4+Wa2Kh7nAkKVxUJgsJlNK+ncHoSdc6lXDrMj+hbT/ECO7W8AbiimfRQwqjTn9iDsnEs3v23ZOeeSE6qopffylgdh51zq+UjYOecSlOYCPh6EnXOpJuVXpKeq8iDsnEu9FA+E1x+EJd3B2svAr8XMLqqQHjnnXCkVVtOR8IRK64VzzpVRuCuuGgZhM3sw+7WkBma2vOK75JxzpZPigXDJtSMk7SFpOvBhfL2LpEEV3jPnnMvThq6skaR8ZjjfDhwIfAVgZpOBnhXZKeecy5cAlfC/qiyv2RFm9vk6OZdVFdMd55wrJanaXpjL+FzSnoBJqg1cDHxQsd1yzrn8pfi6XF5B+Dzg74RlOr4glGk7vyI75Zxz+RJQkOIoXGIQNrNFwMmV0BfnnCuTqn7xLZd8ZkdsLek5SQslLZD0rKStK6NzzjlXkpJW1ajqg+R8Zkc8AjwOtAJaA08AwyuyU845VxoFUs5HSeJqygskTc1qGyDpw7ja8tOSGsf2dpJWSJoUH/dk7dNV0vuSZkoaqDzuIsknCDcws4fMrCg+hgH18tjPOecqxYYGYWAIcNA6baOBHc1sZ+Aj4I9Z731iZp3i47ys9ruBswnrzrUv5pg/7/v63pDUVFJT4EVJ/WP031LSZZRy+Q7nnKso4cJc7kdJzOx1YPE6ba+YWVF8OY6wevL6+xEWBm1kZuPMzIChwJElnTvXhbmJhAI+mY9wbnb/WPtXwTnnkpFfKctmkrLr4dxrZveW4ixnAo9lvd5K0nvAUuBKMxtLmEE2J2ubObEtp1y1I7YqRQedcy4xeaReF5nZrmU89p8Iqyo/HJvmAVuY2VeSugLPSNqhLMeGPO+Yk7Qj0JGsXLCZDS3rSZ1zrrxk0hEVcmzpdOBQYN+YYsDMfgB+iM8nSvoE+AUwl7VTFm1jW04lBmFJVwG9CEF4FHAw8AYh3+Gcc4mriJs1JB0EXAb8MruCpKTmwGIzWxWn67YHPjWzxZKWStodeBs4FbijxL7n0ZdjgX2B+WZ2BrALsEmpP5FzzlUAqVymqA0H3gI6SJojqR9wJ9AQGL3OVLSewBRJk4AngfPMLHNR7zfA/cBM4BPgxZLOnU86YoWZrZZUJKkRsADYPI/9nHOuUmzoHXNm1reY5gfWs+0IYMR63psA7Fiac+cThCfEScr3EWZMfEv4xXDOuSqhqt8Vl0s+tSN+E5/eI+klwjy4KRXbLeecy4/I+4aMKinXQp9dcr1nZu9WTJccwPbbtmX4yBuT7kaN8dG8ZUl3oUb5fuXq8juY0l3AJ9dI+NYc7xmwTzn3xTnnyiSfGQZVVa6bNXpXZkecc64sRPVd8t4551IhxTHYg7BzLt1CzeD0RmEPws651CtMcVI4n5U1JOkUSX+Jr7eQ1K3iu+accyXLrDG3gfWEE5PP78cgYA8gc0fJMuCuCuuRc86VUkEJj6osn3REdzPrEmtnYmZLJNWp4H4551xeJFX72RErJRUS5gZnKgiV40xr55zbMFU845BTPkF4IPA00ELSDYSqaldWaK+ccy5PAmpV55GwmT0saSKhnKWAI83sgwrvmXPO5alaj4QlbQEsB57LbjOz/1Vkx5xzLi95LuZZVeWTjniBnxb8rAdsBcwAyrymknPOlRcBhSkeCpc4e8PMdjKzneO/7YFueD1h51wVsqFL3ksaLGmBpKlZbU0ljZb0cfy3SWyXpIGSZkqakl1xUtJpcfuPJZ2WV99L+2FjCcvupd3POecqQqaAT65HHoYAB63T1h8YEwefY+JrCOtsto+Pc4C7IQRt4CpCfOwGXJUJ3LnkkxP+fdbLAqAL8EVJ+znnXKXQhl+YM7PXJbVbp/kIwiLHAA8CrwGXx/ahcfXlcZIaS2oVtx2dWW9O0mhCYB+e69z55IQbZj0vIuSIi11fyTnnkpDHrcnNJE3Ien2vmd1bwj4tzWxefD4faBmftwE+z9puTmxbX3tOOYNwvEmjoZldUtKBnHMuCSEdUeJmi8xs17Kew8xMkpV1/1zW23VJtcxsFdCjIk7snHPlQxSU8CijL2Oagfjvgtg+l7VXnG8b29bXnlOu34934r+TJI2U9CtJR2ceeX4I55yrUFIYCed6lNFIIDPD4TTg2az2U+Msid2Bb2La4mXgAElN4gW5A2JbTvnkhOsBXxHWlMvMFzbgqVJ8GOecqzAbWq5S0nDChbVmkuYQZjncBDwuqR8wGzg+bj4K6APMJNzIdgaAmS2WdB0wPm53beYiXS65gnCLODNiKj8F34wKyY0451xpiXKZHdF3PW/tW8y2Bpy/nuMMBgaX5ty5gnAhsDEUm1DxIOycqzKqaynLeWZ2baX1xDnnykBU/cLtueQKwun9aXHO1RzVeKHPn+VCnHOuqkl7AZ/1BuF8ruo551xVkN4Q7EveO+dSTxRU0wtzzjlX5VXnC3POOZcK1fXCnHPOVX3a8DvmkuRB2DmXap6OcM65hPlI2DnnEpTiGOxB2DmXbiEdkd4o7EHYOZdy8nSEc84lKcUx2IOwcy7dpGpaO8K54hy854402GhjCgsLKSysxfAX/sOM6e9z/RW/Zfl339G67RbcOPB+Nm7YCICPPpjKdX+8mG+XLaOgoIBHnnuNuvXqJfwp0mPpN19zzWUXMvOj6UjimgF30W6b9lz2mzP4Ys5sWrfdkgGDhtCocRMAxr81lgHX9GflypU0abopg594MeFPUDk2NAZL6gA8ltW0NfAXoDFwNrAwtl9hZqPiPn8E+gGrgIvMrMSljIrjQdiV2v2PvUCTppuueX3NZRfw+ytvYNfd9+Lpxx5iyD/+zgWX/JmioiKuuPhsbrj9Xjp03Imvl3xFrdq1E+x5+tx89eX06LUft/7jIVb++CMrVizngTtvpVuPX9Lv/N/zwF3/xwODbuN3V1zL0m++5q9/+j2DHnqKVm0256tFC0s+QTWhDbwwZ2YzgE6wZpX5ucDThKWLbjOzW9Y6n9QROBHYAWgN/EvSL+LiyKWS5jnOroqY/dkndO0eFuXeY+/ejBk1EoC3Xh9D++13oEPHnQBo3GRTCgsLE+tn2ixb+g0T3/kvR514KgC169Sh0SaN+ffoFzj82JMAOPzYk/j3K88D8OKzT7DvwYfRqk1Y8HfTZs2T6Xgly5SyzPUopX2BT8xsdo5tjgAeNbMfzOwzwnpz3crSfw/CrnQkzjvlSE7s05MnH/4nANv8Yjv+/coLALzywjPMnxdW+Z796UxE2P6EPnvzz7tvT6zbaTT389k0abopf/nDrzn+4L24+rILWL78OxYvWkjzlpsB0KxFSxbHEe/sT2ey9Juv6Xd8H07s05Pnnnwkye5XKin3g7CA54Ssxzk5DnciMDzr9QWSpkgaHFdRBmgDfJ61zZzYVmpVNghLaidpaim2PzL+iVDlSDpd0p1J96M8DBnxMo+NGstdQ0fw2ND7mPj2m1wzYBCPDb2PE/v0ZPm3y6gdUw6rVq3ivQnjuHHgAwwZ8TKvvvwcb7/xWrIfIEVWFRXx4dTJHPerfjz+4hvUr9+AwYP+b61tQuGaEGWKVhUx/f1J3DHkCe4e9jT3DryZWZ9+nEDPK59K+B+wyMx2zXrcW+xxpDrA4cATseluYBtCqmIecGt5973KBuEyOBKokkG4Omm5WWsg/Km7z4GHMnXSRLba9hf84+FneXTU6xx0xLG03XIrAFq0ak3XbnvSpOmm1K/fgL16H8AHUycn2f1UadmqDS1btWHnzrsBsH+fI/lw6mSaNmvOwi/nA7Dwy/k0bdYsbL9ZG/bsuS8NGmxEk6ab0qV7Dz6anvc4JrVE7lREKdMRBwPvmtmXAGb2pZmtMrPVwH38lHKYC2yetV/b2FZqVT0IF0q6T9I0Sa9Iqi/pbEnjJU2WNEJSA0l7En69BkiaJGmb+HhJ0kRJYyVtByDpOElT4/6vx7bTJT0r6TVJH0u6KtMBSadIeice9x8xaY+kAyS9JeldSU9I2ji27ybpv/H470hqGA/VOvbnY0k3V+q3WE6WL/+O775dtub5W2NfZdsO26+5ALR69WruGziA407pB0CPnvvy8YzprFixnKKiIiaOe5Ot23dIrP9p06xFS1q2asOsT8Jo9u03X2Pr9tvRa/8+jIyphpFPPkLv/Q8BoPcBh/De+HEUFRWxYsVy3n9vAlvVhO+7hFREKVPCfclKRUhqlfXeUUDmV20kcKKkupK2AtoD75Sl+1V9dkR7oK+ZnS3pceAY4Ckzuw9A0vVAPzO7Q9JI4HkzezK+NwY4z8w+ltQdGATsQ5h2cqCZzZXUOOtc3YAdgeXAeEkvAN8BJwA9zGylpEHAyZJGAVcC+5nZd5IuB34v6SbCNJcTzGy8pEbAinj8TkBn4AdghqQ7zCw7p1TlLV64gN+dczIARUVF9DnyOHr02p+HHxjEo0PvA2Dfgw7nyONPAaBR4yb86qzzOenQXkhi794H0HPfgxLrfxr1v3YAf7zoLFau/JG2W7Tj2lsGsdpWc+mvT+eZx4bSqs0WDLh7CABbt+9Aj177cdwBe6CCAo4+8VTad6j+fxyW1xpzkjYC9gfOzWq+WVInwIBZmffMbFqMSdOBIuD8ssyMAJCZbUi/K4ykdsBoM2sfX18O1AbGAtcT5u9tDLxsZudJGkIMwnFUuhCYkXXIuma2vaR7CDmexwkB/StJpwP7mNmp8VzXAosJX+4VwIJ4jPqEX8kJwBBCMh6gDvAWcDtwj5n1WOeznE4I5GfH1y8CN5jZG+tsdw5wDkCrNpt3femtaaX+3pxLg76H/JJpU94tlzsstt+ps/3z6X/n3GaP9k0mmtmu5XG+8lbVR8I/ZD1fRQiCQ4AjzWxyDG69itmvAPjazDqt+0YM2N2BQ4CJkrpm3lp3U8KP7INm9sfsNyQdRviB6LtO+06l+Cw/++7jxYJ7AXbYuUvV/HV0ripK7w1zVT4nXJyGwDxJtYGTs9qXxfcws6XAZ5KOA1CwS3y+jZm9bWZ/IYyWM8n1/SU1lVSfcJHvTWAMcKykFnHfppK2BMYBPSRtG9s3kvQLwsi7laTdYntDSVX9h8651CuQcj6qsjQG4T8DbxOC5IdZ7Y8Cl0p6T7eJUcYAABHSSURBVNI2hADdT9JkYBphcjWEi3fvx+lv/wUyl+vfAUYAU4ARZjbBzKYTcr+vSJoCjAZamdlC4HRgeGx/C9jOzH4k5JDviOcdDfg9us5VMJXwqMqq7CjNzGYRLpRlXmffNnh3Mdu/yc+nqP3sKpCZHb1uW1wkcI6ZHVnM9o+x9j3lmfZXgd2KaR8P7L5O85D4yGxz6Lr7OefKRvhCn845l5zST0OrUjwIA2Y2hKyRqnMuXVIcgz0IO+fSTp6OcM65JKU4BnsQds6lW7gwl3Qvys6DsHMu9Ta0qHuSPAg751LPR8LOOZcUn6LmnHPJ8nSEc84lREBBemOwB2HnXDXgQdg555KT5nREGquoOefcWgqU+5EPSbNihcVJkibEtqaSRsdlyUZnVluO5XEHSpqpsBJzlzL3vaw7OudclVF+tSx7m1mnrFU4+gNj4go/Y+JrCAuCto+PcyimsmO+PAg751ItxNkSl7wvqyOAB+PzBwkLPmTah1owDmi8zqKgefMg7JxLtxJSEaWYOWGEBRwmxvUeAVqa2bz4fD7QMj5vA2Qv1DsntpWaX5hzzqVfyYG2WSbPG90b13TMtldchb0FMFpS9so9mJlJKve1Hz0IO+dSLq915BaVtNqymc2N/y6Q9DTQDfhSUiszmxfTDZmV1+fy0/qUAG1jW6l5OsI5l2olXZPLJxsRF+ttmHkOHABMBUYCp8XNTgOejc9HAqfGWRK7A99kpS1KxUfCzrn02/Bpwi2Bp2Nx+FrAI2b2kqTxwOOS+gGzgePj9qOAPsBMYDlwRllP7EHYOZd6G7qsvZl9CuxSTPtXwL7FtBtw/gadNPIg7JxLvfTeL+dB2DmXdvIl751zLjG+vJFzziUsxTHYg7BzLv029MJckjwIO+fSL70x2IOwcy7dVLr6EFWOB2HnXOqluai7B2HnXPqlNwZ7EHbOpZ+nI5xzLjEbXLg9UR6EnXOp5jdrOOdcwjwIO+dcgjwd4ZxzCfF5ws45lzQPws45lxxPRzjnXII8HeGcc0nyIOycc8kQ6S5lqbBenatqJC0krO6aNs2ARUl3ogZJ6/e9pZk1L48DSXqJ8D3kssjMDiqP85U3D8KuXEmaYGa7Jt2PmsK/7/QrSLoDzjlXk3kQds65BHkQduXt3qQ7UMP4951ynhN2zrkE+UjYOecS5EHYOecS5EHYOecS5EHYOecS5EHYVVmSCuO/m0mqn3R/qhtJBeu8Tu+9vynmQdhVOZK2ktTDzFZJOgwYCwyUdEPSfasOJDUAMLPVkrpKOkZSPfOpUonwKWquypHUF7gLOAfYB3gW+Bq4EPjKzC5OsHupJqkxcBXwDPAj8CDwBbAC+DMwycyKkuthzeMjYVflmNlw4ALgNqC+mb0MTASuB5pK+keS/Uu5jYB5wAnAFcARZtYLeA+4COgkyasrViIPwq7KyOQkJbU3s0eA3wL7SOoVR2cfATcBjSV1TLCrqSRJZjYXGAZ8AGwLdAcwsyuA/wH9gS6JdbIG8iDsqgwzM0mHA/dJ6mRmI4Crgfsl/dLMVhOCx5lmNj3JvqZNDMAmaT+gLfAocB/QQ9LBAGZ2JfAJ8ENyPa15PCfsqow4un0IOMfMJma1nwoMAPqa2atJ9S/tYrC9DbjYzF6WtDlwBLADMMrMnku0gzWU535cVbIJ8L9MAJZU28xWmtlQSUWAjxjKKM6I+C3wazP7dxwZfy7pOaAucJSkcYTi5/49VyIPwi4xWX8iF8RUwxfA95K2Bz42s5WSegKdzezv2fsk2e+UKgTqEL5jCIH3e2AJ8E+gkZktTKhvNZrnhF0isgLwocANkm4lTJlaAJwPnCfpCEKAmJbZzwNwfrIucm4pqa6ZLQNeBm6S1MTMvo8/cC8BmNms5Hpbs/lI2CUiBuDewLXAicCLhHTDZcCZwDbAbsAFZvavxDqaUvH77QP8CfiPpBbAQKAR8KakfwKnAVeY2eIEu1rj+YU5lxhJVwNvEILv9cBJZvZZ1vv1zWxFQt1LtXiR8xHgcMJfFl2AY8xsqaQTCH91LDKzsZ7iSZaPhF2S5hHuimsFnGJmn0k6A9jCzK7Bp0qVWlZArUcIwtsCvYCTYwDeFXjKzFZm9vEAnCzPCbtKkZWj3F3SvpK6Aq8AOwP3A7Nj2++BtyHUNkiqv2mTVXwnM7D6H3AS4bbkg8xsZpwj/EegSQJddOvh6QhXaSQdSJinOgB4ANgV2ALoRxj1tgQGmNlI/xM5f1kXOfcHjgfeBWYCzQnpiNeAWYS7Da8ys2cT6qorhqcjXIWLo7SmwMXAkcDmhBkP883sXUn/Jkyhamhmsz0Al04MwPsAtxPmAv+JUAviFsKUtN8SRsZXmtnz/v1WLT4SdpVG0l+Ab4FjgdPN7CNJJwHvm9n7yfYuvWLd5QuAd4Ai4B/A4WY2R1IDM1ueta0H4CrGR8KuQmT9idwSWBYDQVPCKK15vEjUBbgUODvJvqZdrLu8hFAL4gegj5nNj7WY20i6P1Oe0gNw1eNB2FWIrBsxbgbek1RkZqdJ2gZ4UNIswlX7q81sQoJdTZ2sH7jOwFaEC5lTgPHArBiAuxFywH/w+sBVm6cjXIWQtAMhFzmcECDuARqYWZ94J1wBMM/MxvmfyKUXL8INIlSVM+A/hLm/WwM9gJXAzWY2MrFOurx4EHblTtKmwGTgfcINAstj+/PAE2b2YJL9S7tYW+PvwOVm9l78UesKjDez5yRtCawwswX+A1f1+TxhVy6y5gG3M7OvgPOA9sD+WZu9DWycQPdSL2seMEBvQvnJngBxytly4NT4eraZLYjPPQBXcZ4TdhssK0d5OPAHSRfEqVD1gNsl7QZMINQqOD/RzqZQ1ve7L/AVoeYyQDdJx8Ti9/8B9pDUyMyWJtZZV2oehN0GiwFiD+AaQv2HDyRtYmZPSpoHPEaYG3xYfM//RC6FrB+4G4FLzWySpBGEXPCf43vbAH/zAJw+HoRdeWlGGO22jnfG9ZG0ijD97BzCjQRbEi4kuVKQ1Ay4HDgqzq3eGdgUeIpwk0sP4DFfGSOdPAi7Msn6E7kZ4U/kj4AvCeUSbyaUqOwFtDezUZKaAjdKesPMvk2q3ylVSCjAfpCk/oS8ek/gEkJtiB+B3pI+NrOXkuumKwufHeHKLP4ZfAYwhzBH9XlgpZktizdiDAPONrM34/YNY3Fxl0PWD9wuhOC7kDD74TDgBQvrwx0P7GNm50naAtgXeMnM5iXXc1cWHoRdmcSSiPcBBwN3AyJU7TJgF8KKGJfFKVMFZrbac8H5U1iU82ZgCKHQ/R5m9ml8rzdwJ+FGjJdiW6GZrUqou24DeDrC5aWYANqSUIKyI6EecF8zWx5HZQuB48xsatxvNfh0qXzEqWhtCLd3H06oNDcP+Da+1wq4kjBH+KXMfxcPwOnlI2FXojjVrI+ZPRX/RN4W+IRww0CT+N4cSUcBhwIXZheNcblJqg3UMrMV8buuQ6g49ymhMM9p8YLcEYQazPXNbLH/ZVE9+EjY5WMlsIWkGfH54YSLce8D3wAdJbUjTFH7kwfg/EmqBewDfBfvdNuLkH44gLAkURMz+1FSd6A/MMPMPgT/y6K68JGwy0ssFvMssNDMuma17U24g2slMMy8IHupxVrANwCbAZeY2QhJmxFWR36LMPPkV4RiR16QvZrxIOzWKzuYxj+Z2xJuR+5OyPkulLS5mX2eqVvrATh/63y/Qwjf723Ae2b2haSGhOWeFgEfmNmr/v1WPx6EXbGypkkdAuwBrDKzqyQVAP9HuGD0V8JtyOea2ZwEu5s6Wd9vW2AuUJeQijgTGGVmwyQ1B2qb2RdJ9tVVLC/g44oVA0QfQqAdAZwm6UlgEzP7LaFWweXAIA/ApZf1A/cE4Tu+AHidUBfiYEkDgA8Jt3u7asxHwq5YkuoT5gHfArQGriAsTVSXcPvs15Iax3/9T+RSkrQXoR7wUYSUw+7AWMIPW0egMzDbzMYk1klXKTwIuzUyN1Vkvd4EaEEYnfWOU6i+Bl4gTJvyFRtKIfuGijjd7COgHXA9cBWhxsb/gGvMbGHWfv4jV435FDWXGfUWmdlKST0INwR8ZmYTJTUm3CywuaSNCEVjBnsAzl/mdm0La8H1JgTeaYTv9VzgTDObLOlYoDHhh29NEPYAXL15EK7hFFbBuBQYGYPxg4Q85f2STol1gWcC1xGqdZ1pZm/46Cw/khoAL0gaSFht5C5gOuEi3DTCRc+5kuoA2wP9zGxaUv11lc/TETVcnHp2M6FSVwHwtJmNiXe/PQgcamavS+pIWCPOF+Uspfhd9gcWA/3jqPckwoi4NWGu9SfAcDN7IrGOukR4EK7Bsgrr1CbUI+hNmAlxb8z/Hg08CRxpvmDkBlFYmPNx4K9mNiDeKXcC0IFQKe0evxW5ZvIpajVYDMAFZraScHFoNKEuxG6S6pjZU8DxwA9J9rM6MLPRhLKfp0vqG3PqjwIzCH99LI7beQCuYXwkXEOtc7dWLTMrinnJvwANgZHAWDP7cd3tXdnFudfXAQPNV512+Ei4xonlECHrv30MwLVjwL2WsFLDMWStjOwBuHyY2ShCoaPLJbWOdyC6GsxHwjVI1q2y+xEKwnwKfGJmw+L7teM0tTpAOzP7KMn+VmeSmmfPBXY1l/8K1yAxAP8SuAN4jVCz4HxJf4jvr4w54h89AFcsD8Auw+cJ1zxtgfvM7J8Akt4GBkh6ycymZd8x55yreD4SruaycsAZ9YFTsl5PI6yS7Hkp5xLgQbiay6QgJP1GUkczux94W9IYhWXodwV2Bmon21Pnaia/MFdNZV2E6w4MJtwquxx4A3iYcJdcO2BT4Ea/GcO5ZHgQrsYkdSNMObvMzKZI6ksomTjFzB6I06Ma+51aziXH0xHVW2NgP2D/+PoJ4E1gd0kXAwKWgM8Ddi4pPjuiGjOzV2L9hxslfWFmw+PqGIXA5ExtW+dccjwIV3MWVj8uAq6L9SAeBIYn3S/nXOA54RpC0uHATYT0xHyfD+xc1eBBuAbxW2Wdq3o8CDvnXIJ8doRzziXIg7BzziXIg7BzziXIg7BzziXIg7BLhKRVkiZJmirpibg0fFmPNUTSsfH5/XFl6PVt20vSnmU4xyxJzfJtX2ebb0t5rqslXVLaPrp08iDskrLCzDqZ2Y6E5ZTOy34zrkZcamZ2lplNz7FJL6DUQdi5iuJB2FUFY4Ft4yh1rKSRwHRJhZIGSBovaYqkcyFUiJN0p6QZkv4FtMgcSNJrknaNzw+S9K6kybF0ZztCsP9dHIXvLam5pBHxHOMl9Yj7birpFUnTJN1PqLORk6RnJE2M+5yzznu3xfYxkprHtm0kvRT3GStpu/L4Ml26+G3LLlFxxHsw8FJs6gLsaGafxUD2jZntJqku8KakV4DOQAegI9CSUKZz8DrHbQ7cB/SMx2oaq8XdA3xrZrfE7R4BbjOzNyRtAbwMbA9cBbxhZtdKOgTol8fHOTOeoz4wXtIIM/sK2AiYYGa/k/SXeOwLgHuB88zs41hydBCwTxm+RpdiHoRdUupLmhSfjwUeIKQJ3jGzz2L7AcDOmXwvsAnQHugJDI8FiL6Q9Goxx98deD1zLDNbvJ5+7Ad0zFqApJGkjeM5jo77viBpSR6f6SJJR8Xnm8e+fgWsBh6L7cOAp+I59gSeyDp33TzO4aoZD8IuKSvMrFN2QwxG32U3ARea2cvrbNenHPtRAOxuZt8X05e8SepFCOh7mNlySa8B9dazucXzfr3ud+BqHs8Ju6rsZeDXkmoDSPqFpI2A14ETYs64FdC7mH3HAT0lbRX3bRrblwENs7Z7Bbgw80JSJii+DpwU2w4GmpTQ102AJTEAb0cYiWcUAJnR/EmENMdS4DNJx8VzSNIuJZzDVUMehF1Vdj8h3/uupKnAPwh/vT0NfBzfGwq8te6OsVDROYQ//SfzUzrgOeCozIU54CJg13jhbzo/zdK4hhDEpxHSEv8roa8vAbUkfUCoVjcu673vgG7xM+xDWO0E4GSgX+zfNOCIPL4TV814AR/nnEuQj4Sdcy5BHoSdcy5BHoSdcy5BHoSdcy5BHoSdcy5BHoSdcy5BHoSdcy5B/w+vYtU/TdjrggAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}